<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>shuang&#39;s blog</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2018-10-09T13:24:14.634Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>cccshuang</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>公共经济学之二</title>
    <link href="http://yoursite.com/2018/10/09/%E5%85%AC%E5%85%B1%E7%BB%8F%E6%B5%8E%E5%AD%A6%E4%B9%8B%E4%BA%8C/"/>
    <id>http://yoursite.com/2018/10/09/公共经济学之二/</id>
    <published>2018-10-09T12:26:48.000Z</published>
    <updated>2018-10-09T13:24:14.634Z</updated>
    
    <content type="html"><![CDATA[<p>这节课里有一句话说，产权明晰是效率提升的关键。这让我感受颇深，我认为效率提高地根本原因在于产权明晰之后更能带动人们的积极性，从而为自己谋利益的同时也在为企业谋利益，实现个人与集体的共赢。凡是一个有效率的企业甚至国家政策，都应该做到产权明晰。像我们国家的农业以前实行人民公社，实行一大二公、大锅饭，人们劳动的积极性不能够被充分地调动起来，后来实行家庭联产承包责任制，包产到户，这也是将土地的产权明晰，结果是人们的生产劳动积极性大大提高，效率也极大地提高。对于企业，单一自然人产权的个人独资企业企业效率最高，而单一国有产权的国有企业效率最低，这也是因为产权明晰后人们的生产积极性大大提高。<br>那么为什么产权明晰可以提高人们的积极性呢？我们可以想一想产权不明晰时候的情况，不清晰的产权，人们说不清这是你的还是我的，必然由一个团体共享产权，当有利益分配或者进行决策时，必然会发生争议，从而加大成本，降低人们参与的积极性；另一方面，人们对于不能明确是自己的利益的东西一般不会过多地将自己的时间、劳动等资源进行投入，因为回报是不可预期的，没有明确的目标收益，很有可能事不关己而高高挂起。相反，如果产权明晰，人们之间的利益划分明确，对自己投入的回报能有所预期，出于自利，将会更加愿意投入更多的资源，更加积极的参与进来。人们的积极性提高了，自然效率也就提高了。<br>上了两节课，我对“天下熙熙，皆为利来；天下攘攘，皆为利往”这句话有了更深的感触，突然有一种感觉：好像社会生活中的现象都可以用一个“利”字来解释。也许是这样，也许并不这么简单，生活还是需要多观察，多思考啊。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;这节课里有一句话说，产权明晰是效率提升的关键。这让我感受颇深，我认为效率提高地根本原因在于产权明晰之后更能带动人们的积极性，从而为自己谋利益的同时也在为企业谋利益，实现个人与集体的共赢。凡是一个有效率的企业甚至国家政策，都应该做到产权明晰。像我们国家的农业以前实行人民公社，
      
    
    </summary>
    
      <category term="公共经济学课程感悟" scheme="http://yoursite.com/categories/%E5%85%AC%E5%85%B1%E7%BB%8F%E6%B5%8E%E5%AD%A6%E8%AF%BE%E7%A8%8B%E6%84%9F%E6%82%9F/"/>
    
    
      <category term="公共经济学" scheme="http://yoursite.com/tags/%E5%85%AC%E5%85%B1%E7%BB%8F%E6%B5%8E%E5%AD%A6/"/>
    
      <category term="感悟" scheme="http://yoursite.com/tags/%E6%84%9F%E6%82%9F/"/>
    
  </entry>
  
  <entry>
    <title>数据挖掘-认识数据</title>
    <link href="http://yoursite.com/2018/09/28/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98-%E8%AE%A4%E8%AF%86%E6%95%B0%E6%8D%AE/"/>
    <id>http://yoursite.com/2018/09/28/数据挖掘-认识数据/</id>
    <published>2018-09-28T11:55:29.000Z</published>
    <updated>2018-09-30T13:06:40.696Z</updated>
    
    <content type="html"><![CDATA[<h2 id="数据对象与属性类型"><a href="#数据对象与属性类型" class="headerlink" title="数据对象与属性类型"></a>数据对象与属性类型</h2><p>数据集由数据对象组成。一个数据对象代表一个实体。例如销售数据库中，对象可以是顾客、商品。属性是一个数据字段，表示数据对象的一个特征。</p><h3 id="属性类型"><a href="#属性类型" class="headerlink" title="属性类型"></a>属性类型</h3><ul><li><p>标称属性（nominal attribute）：一些事物的名称，每个值代表某种类别、编码或者状态。不具有有意义的序，不是定量的，其均值和中位数无意义，总数有意义。例如，颜色这个对象的属性可能有黑色、红色、白色等，职业可能值有教师、医生等。</p></li><li><p>二元属性（binary attribute）：一种标称属性，只有两个类别或状态：0或1。有对称和非对称两种情况，对称比如性别男女两种状态；非对称比如HIV检测中的阳性和阴性，为了方便，通常用1表示最重要的结果（通常是稀有的，另一个用0编码。</p></li><li><p>序数属性（ordinal attribute）：值之间具有有意义的序，但是相继值之间的差未知。其中心趋势可以用众数和中位数来表示，但不能定义均值。比如成绩有A+、A、A-等。</p></li></ul><p>上面三个都是定性的属性，即它们描述对象的特征而不给出实际大小或数量，其值只代表编码，而不是可测量的量。</p><ul><li>数值属性（numeric attribute）是定量的，可度量，用整数或实数值表示。<ul><li>区间标度属性（interval-scaled）：允许比较和定量评估值之间的差，但是没有真正的零点，没有比率或者倍数关系，可以计算中位数，众数和均值。例如，摄氏温度，我们不能说10摄氏度比5摄氏度温暖2倍。</li><li>比率标度属性（ratio-scaled）：具有固定零点，可以计算均值、中位数和众数。例如，工作年限、文章字数等计数属性。</li></ul></li></ul><h2 id="数据的基本统计描述"><a href="#数据的基本统计描述" class="headerlink" title="数据的基本统计描述"></a>数据的基本统计描述</h2><p>我们为了把握数据的全貌，关注数据的中心趋势度量、数据的散布和图形显示。</p><h3 id="中心趋势度量"><a href="#中心趋势度量" class="headerlink" title="中心趋势度量"></a>中心趋势度量</h3><p>中心趋势度量度量数据分布的中部或中心位置，或者说，给定一个属性，它的值大部分落在何处？</p><h4 id="均值-mean"><a href="#均值-mean" class="headerlink" title="均值 (mean)"></a>均值 (mean)</h4><p>最常用最有效的是的算术均值：<br>$$  \overline{x} = \frac{\sum_{i=1}^N x_i}{N} $$<br>或者使用加权平均，反映对应值的意义、重要性或者出现频率。<br>$$  \overline{x} = \frac{\sum_{i=1}^N w_ix_i}{\sum_{i=1}^N w_i} $$<br>但是均值对极端值很敏感，对于非对称数据，数据中心更好的度量是中位数。</p><h4 id="中位数-median"><a href="#中位数-median" class="headerlink" title="中位数 (median)"></a>中位数 (median)</h4><p>中位数是有序数据的中间值，将数据分成两半。<br>中位数在观测数量很大时，计算开销很大。下面给出近似计算公式。假定数据根据值划分成了区间，并且已知每个区间的频率（数据值的个数）。令包含中位数频率的区间为中位数区间。<br><img src="/2018/09/28/数据挖掘-认识数据/median.PNG" alt=""><br>其中，$L_1$是中位数区间下界，$N$是整个数据集中值的个数，$\sum freq$是低于中位数区间的所有区间的频率和，$freq_{median}$是中位数区间的频率，$width$是中位数区间的宽度。</p><h4 id="众数-mode"><a href="#众数-mode" class="headerlink" title="众数 (mode)"></a>众数 (mode)</h4><p>出现最频繁的值。具有有一个、两个、三个众数的数据集合分别成为单峰的（unimodal）、双峰的（bimodal）、三峰的（trimodal）。<br>当数据对称时，众数 = 中位数 = 均值。<br>当次数分布右偏时,即正倾斜时，均值受偏高数值影响较大，其位置必然在众数之右，中位数在众数与算术平均数之间，众数 &lt; 中位数 &lt; 均值。<br>反之，当次数分布左偏时,即负倾斜时，均值受偏小数值的影响较大,其位置在众数之左,中位数仍在两者之间，均值 &lt; 中位数 &lt; 众数。<br><img src="/2018/09/28/数据挖掘-认识数据/data_shape.jpg" alt=""></p><h4 id="中列数（midrange）"><a href="#中列数（midrange）" class="headerlink" title="中列数（midrange）"></a>中列数（midrange）</h4><p>最大和最小值的平均值。</p><h3 id="数据的散布"><a href="#数据的散布" class="headerlink" title="数据的散布"></a>数据的散布</h3><h4 id="极差（range）"><a href="#极差（range）" class="headerlink" title="极差（range）"></a>极差（range）</h4><p>最大值和最小值之差。</p><h4 id="四分位数（quartile）"><a href="#四分位数（quartile）" class="headerlink" title="四分位数（quartile）"></a>四分位数（quartile）</h4><p>把数据划分成四个基本上大小相等的连贯集合。<br>$ Q_1 $：有25%的数据在此之下；<br>$ Q_2 $：有50%的数据在此之下,即中位数；<br>$ Q_3 $：有75%的数据在此之下。<br>四分位数极差IQR：给出被数据中间一半所覆盖的范围。<br>$$ IQR = Q_3 - Q_1 $$<br>对于倾斜分布，单个散步数值度量如IQR都不是很有用，识别可以离群点的通常规则是挑选落在  $Q_3$ 之上和 $Q_1$ 之下至少 $1.5 * IQR$ 处的值。<br>五数概括：最小值、$Q_1$、中位数、$Q_3$、最大值。盒图体现了五数概括。<br><img src="/2018/09/28/数据挖掘-认识数据/boxplot.jpg" alt=""></p><h4 id="方差（Variance）和标准差（Standard-deviation）"><a href="#方差（Variance）和标准差（Standard-deviation）" class="headerlink" title="方差（Variance）和标准差（Standard deviation）"></a>方差（Variance）和标准差（Standard deviation）</h4><p>$$ \sigma ^2 = \frac{1}{N} \sum_{i=1}^{N}(x_i - \overline{x})^2 $$<br>$\sigma ^2$ 是方差，$\sigma$是标准差。</p><h3 id="图形显示"><a href="#图形显示" class="headerlink" title="图形显示"></a>图形显示</h3><h4 id="分位数图"><a href="#分位数图" class="headerlink" title="分位数图"></a>分位数图</h4><p>设 $x_i$ 是按递增顺序的数据，使得 $x_1$ 是最小的观测值，而 $x_N$ 是最大的，每个观测值$x_i$ 与一个百分数$f_i$对应，指出大约$f_i * 100%$的数据小于值$x_i$。<br><img src="/2018/09/28/数据挖掘-认识数据/quantileplot.jpg" alt=""></p><h4 id="分位数-分位数图"><a href="#分位数-分位数图" class="headerlink" title="分位数-分位数图"></a>分位数-分位数图</h4><p>可以使得用户观察从一个分布到另一个分布是否有漂移。<br>例如两个部门销售商品的单价数据的分位数-分位数图。<br><img src="/2018/09/28/数据挖掘-认识数据/qqplot.jpg" alt=""><br>比如在$Q2$,部门1销售的商品50%低于或等于78美元，而部门21销售的商品50%低于或等于85美元。中间那条45度实线代表没有偏移。从总体来看也可以看出部门1销售的商品单价趋向于比部门2低。</p><h4 id="（频率）直方图"><a href="#（频率）直方图" class="headerlink" title="（频率）直方图"></a>（频率）直方图</h4><p>如果数据是标称的，一般称作条形图，数据是数值的，多使用术语直方图。</p><h4 id="散点图"><a href="#散点图" class="headerlink" title="散点图"></a>散点图</h4><p>散点图是一种观察双变量数据的有用方法。可以通过其看出两个变量是正相关、负相关还是不相关的。</p><h2 id="度量数据的相似性和相异性"><a href="#度量数据的相似性和相异性" class="headerlink" title="度量数据的相似性和相异性"></a>度量数据的相似性和相异性</h2><p>相似性和相异行都称为邻近性，用于评估对象之间相互比较的相似或不相似的程度。</p><h3 id="数据矩阵和相异性矩阵"><a href="#数据矩阵和相异性矩阵" class="headerlink" title="数据矩阵和相异性矩阵"></a>数据矩阵和相异性矩阵</h3><h4 id="数据矩阵"><a href="#数据矩阵" class="headerlink" title="数据矩阵"></a>数据矩阵</h4><p>n个对象被p个属性刻画。<br>对象-属性结构，行代表对象，列代表属性，因此数据矩阵通常被称为二模矩阵。<br>$$<br> \left[<br>\begin{matrix}<br>   x_{11} &amp; \cdots &amp; x_{1f} &amp; \cdots &amp; x_{1p} \\<br>   \cdots &amp; \cdots &amp; \cdots &amp; \cdots &amp; \cdots \\<br>   x_{i1} &amp; \cdots &amp; x_{if} &amp; \cdots &amp; x_{ip} \\<br>   \cdots &amp; \cdots &amp; \cdots &amp; \cdots &amp; \cdots \\<br>   x_{n1} &amp; \cdots &amp; x_{nf} &amp; \cdots &amp; x_{np}<br>  \end{matrix}<br> \right]<br>$$</p><h4 id="相异性矩阵"><a href="#相异性矩阵" class="headerlink" title="相异性矩阵"></a>相异性矩阵</h4><p>存放n个对象两两之间的邻近度。<br>对象-对象结构，只包含一类实体，因此被称为单模矩阵。<br>$$<br> \left[<br>\begin{matrix}<br>   0 &amp;  &amp; &amp;  &amp;  \\<br>   d(2,1) &amp; 0 &amp;  &amp;  &amp;  \\<br>   d(3,1) &amp; d(3,2) &amp; 0 &amp;  &amp;  \\<br>   \vdots &amp; \vdots &amp; \vdots &amp;  &amp;  \\<br>   d(n,1) &amp; d(n,2) &amp; \cdots &amp; \cdots &amp; 0<br>  \end{matrix}<br> \right]<br>$$<br>该矩阵是对称的，$d(i,j)$是对象i和对象j之间的相异性的度量，其中$d(i,i) = 0$,即一个对象与自己的差别为0。</p><p>下面我们讨论对于不同类型数据的邻近性度量方法。</p><h4 id="标称数据的邻近性度量"><a href="#标称数据的邻近性度量" class="headerlink" title="标称数据的邻近性度量"></a>标称数据的邻近性度量</h4><p>标称属性可以取不同的状态，如颜色有红、黄等状态，这些状态可以用字母、符号或者一组整数来表示。<br>两个对象之间的相异性可以根据不匹配率来计算。<br>$$d(i,j) = \frac{p-m}{p} $$<br>其中，m是匹配的数目，即i和j取值相同状态的属性数，p是属性总数。<br>相似性可以根据下式计算:<br>$$ sim(i,j) = 1 - d(i, j) = \frac{m}{p} $$</p><p>标称属性可以使用非对称的二元属性编码，如对颜色，可以对所有的颜色状态分别创建一个二元变量，如果一个对象为黄色，则黄色属性设置为1，其他设置为0。</p><h4 id="二元属性的邻近性度量"><a href="#二元属性的邻近性度量" class="headerlink" title="二元属性的邻近性度量"></a>二元属性的邻近性度量</h4><table><thead><tr><th></th><th>1</th><th>0</th></tr></thead><tbody><tr><td>1</td><td>q</td><td>r</td></tr><tr><td>0</td><td>s</td><td>t</td></tr></tbody></table><p>在上表中，q是对象i和j都取1的属性数，其他类似。<br>对于对称的二元属性：<br>$$ d(i,j) = \frac{r+s}{q+r+s+t} $$</p><p>对于非对称的二元树型，一般两个值都取1被认为比两个都取0的情况更有意义，负匹配数t通常忽略。<br>$$ d(i,j) = \frac{r+s}{q+r+s} $$<br>非对称的二元相似性被称为<strong>Jaccard系数</strong>，在文献中被广泛使用。<br>$$ sim(i,j) = \frac{q}{q+r+s} = 1 - d(i,j) $$</p><h4 id="数值属性的相异性：闵科夫斯基距离"><a href="#数值属性的相异性：闵科夫斯基距离" class="headerlink" title="数值属性的相异性：闵科夫斯基距离"></a>数值属性的相异性：闵科夫斯基距离</h4><p>$$<br>d(i,j) = \sqrt[h]{|x_{i1} - x_{j1}|^h + \cdots + |x_{ip} - x_{jp}|^h}<br>$$<br>当h=1时，为曼哈顿距离，$ d(i,j) = |x_{i1} - x_{j1}| + \cdots + |x_{ip} - x_{jp}|$<br>当h=2时，为欧几里得距离，$ d(i,j) = \sqrt[2]{|x_{i1} - x_{j1}|^2 + \cdots + |x_{ip} - x_{jp}|^2} $<br>当h趋近于无穷时，为上确界距离，即两个对象的最大属性值差，$ d(i,j) = \max_{f}^{p}|x_{if}-x_{jf}| $</p><h4 id="序数属性的邻近性度量"><a href="#序数属性的邻近性度量" class="headerlink" title="序数属性的邻近性度量"></a>序数属性的邻近性度量</h4><ol><li>假设$f$为一个序数属性，值为$x_{if}$，其有$M_f$个有序的状态，表示排位，用对应的排位$r_{if} \in {1,\dots,M_F}$取代$x_{if}$</li><li>由于每个序数属性都可能有不同的状态数，所以通常将每个属性的值域映射到$[0.0,1.0]$上，用$z_{if}$代替$r_{if}$来实现数据规格化。<br>$$ z_{if} = \frac{r_{if}-1}{M_{f}-1} $$</li><li>这时候序数属性的邻近性度量就可以转换为数值属性的邻近性度量来求了。</li></ol><h4 id="混合属性的相异性"><a href="#混合属性的相异性" class="headerlink" title="混合属性的相异性"></a>混合属性的相异性</h4><p>一个对象可能包含很多不同类型的数据，可能有标称的、对称或者非对称二元的、数值的或者序数的，假设数据集包含$ p $个混合类型的属性，则：<br>$$<br>d(i,j) = \frac{ \sum_{f=1}^{p} \delta_{ij}^{[f]} d_{ij}^{[f]} } { \sum_{f=1}^{p} \delta_{ij}^{[f]} }<br>$$<br>其中，如果对象i和对象j没有属性f的度量值，或者两个对象的f的度量值都为0且f是非对称的二元属性，则$\delta_{ij}^{[f]}=0$，否则取1。<br>至于$d_{ij}^{[f]}$,</p><ul><li>f是数值的：$ d_{ij}^{[f]} = \frac{|x_{if}-x_{jf}|} {\max_{h} x_{hf}-\min_{h} x_{hf}} $ ，其中，h遍取属性f的所有非缺失对象。</li><li>f是标称或二元的：如果$x_{if} = x{jf}$，则$d_{ij}^{[f]}=0$，否则取1。</li><li>f是序数的：计算排位$r_{if}$和$z_{if} = \frac{r_{if}-1}{M_f-1}$，然后作为数值属性处理。</li></ul><h4 id="余弦相似性"><a href="#余弦相似性" class="headerlink" title="余弦相似性"></a>余弦相似性</h4><p>用来比较文档，每个文档都被一个所谓的词频向量表示，通常很长，而且稀疏，传统的距离度量效果并不好。<br>$$ sim(i,j) = \frac{x  \cdot y}{||x|| ||y||} $$<br>当属性是二值属性时，$x  \cdot y$是$x$ 和$y$共有的属性数，而$|x||y|$是$x$具有的属性数 和$y$具有的属性数的几何均值，于是$sim(x,y)$是公共属性相对拥有的一种度量，余弦相似性一个简单变种如下:<br>$$<br>sim(x,y) = \frac{x \cdot y}{x \cdot x + y \cdot y - x \cdot y}<br>$$<br>称为Tanimoto距离，常用在信息检索和生物学分类中。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;数据对象与属性类型&quot;&gt;&lt;a href=&quot;#数据对象与属性类型&quot; class=&quot;headerlink&quot; title=&quot;数据对象与属性类型&quot;&gt;&lt;/a&gt;数据对象与属性类型&lt;/h2&gt;&lt;p&gt;数据集由数据对象组成。一个数据对象代表一个实体。例如销售数据库中，对象可以是顾客、商
      
    
    </summary>
    
      <category term="数据挖掘" scheme="http://yoursite.com/categories/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/"/>
    
    
      <category term="数据挖掘" scheme="http://yoursite.com/tags/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/"/>
    
  </entry>
  
  <entry>
    <title>使用多台电脑写Hexo博客</title>
    <link href="http://yoursite.com/2018/09/28/%E4%BD%BF%E7%94%A8%E5%A4%9A%E5%8F%B0%E7%94%B5%E8%84%91%E5%86%99Hexo%E5%8D%9A%E5%AE%A2/"/>
    <id>http://yoursite.com/2018/09/28/使用多台电脑写Hexo博客/</id>
    <published>2018-09-28T06:36:53.000Z</published>
    <updated>2018-09-28T07:14:27.244Z</updated>
    
    <content type="html"><![CDATA[<p>最近觉得写Hexo博客都必须要打开自己的笔记本电脑很麻烦，想既可以在笔记本也可以在实验室的电脑上更新自己的博客，解决这个问题的主要方法是利用Git的分支，master分支用于发布和展示博客内容，并新建一个分支，这里起名比如叫”hexo”,用于保存博客配置和博客markdown文件，供自己维护更新。<br>下面进行配置。</p><h3 id="笔记本上配置方法"><a href="#笔记本上配置方法" class="headerlink" title="笔记本上配置方法"></a>笔记本上配置方法</h3><p>首先，我们已经在自己的笔记本上搭建好了Hexo博客，接下来登录Github，在yourusername.github.io仓库上新建一个分支,比如取名”hexo”，并切换到该分支，并在该仓库-&gt;Settings-&gt;Branches-&gt;Default branch中将默认分支设为”hexo”。<br>然后使用命令<code>git clone git@github.com:yourgithubname/yourname.github.io.git</code>将该仓库克隆到本地，在Git Bash中进入本地yourusername.github.io文件目录，执行<code>git branch</code>命令查看当前所在分支，应为新建的分支”hexo”。<br>将本地博客的部署文件（Hexo目录下的全部文件）全部拷贝进username.github.io文件目录中去，然后将themes目录以内中的主题的.git目录删除（如果有），因为一个git仓库中不能包含另一个git仓库，提交主题文件夹会失败，执行<code>git add .</code>、<code>git commit -m &#39;back up hexo files&#39;</code>、<code>git push</code>即可将博客的hexo部署环境提交到GitHub个人仓库的xxx分支。<br>之后再在笔记本上写博客，即在username.github.io文件目录中进行了，这时需要在<code>npm install</code>一下。</p><h3 id="实验室电脑配置方法"><a href="#实验室电脑配置方法" class="headerlink" title="实验室电脑配置方法"></a>实验室电脑配置方法</h3><p>将新电脑的生成的ssh key添加到GitHub账户上<br>在新电脑上克隆yourname.github.io仓库的xxx分支到本地，此时本地git仓库处于xxx分支<br>切换到yourname.github.io目录，执行npm install(由于仓库有一个.gitignore文件，里面默认是忽略掉 node_modules文件夹的，也就是说仓库的hexo分支并没有存储该目录[也不需要]，所以需要install下)</p><p>这里，如果<code>npm install</code>出错，如”npm ERR! Unexpected end of JSON input while parsing near”,可尝试：</p><ul><li>删除package-lock.json文件</li><li>清除cache: <code>npm cache clean --force</code></li><li>不要用淘宝镜像：<code>npm set registry https://registry.npmjs.org/</code></li></ul><h3 id="发布更新博客"><a href="#发布更新博客" class="headerlink" title="发布更新博客"></a>发布更新博客</h3><p>一切都没有问题后，我们在写完博客要发布的时候：<br>首先执行<code>git add .</code>、<code>git commit -m &#39;back up hexo files&#39;</code>、<code>git push</code>指令，保证’hexo’分支版本最新。<br>执行<code>hexo d -g</code>指令，将博客更新到master分支。</p><p>注意：每次换电脑进行博客更新时，不管上次在其他电脑有没有更新，最好先<code>git pull</code>一下，按照上述步骤进行更新。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;最近觉得写Hexo博客都必须要打开自己的笔记本电脑很麻烦，想既可以在笔记本也可以在实验室的电脑上更新自己的博客，解决这个问题的主要方法是利用Git的分支，master分支用于发布和展示博客内容，并新建一个分支，这里起名比如叫”hexo”,用于保存博客配置和博客markdow
      
    
    </summary>
    
      <category term="Hexo" scheme="http://yoursite.com/categories/Hexo/"/>
    
    
      <category term="Hexo" scheme="http://yoursite.com/tags/Hexo/"/>
    
  </entry>
  
  <entry>
    <title>2-5 TensorFlow Tutorial</title>
    <link href="http://yoursite.com/2018/09/27/2-5-TensorFlow-Tutorial/"/>
    <id>http://yoursite.com/2018/09/27/2-5-TensorFlow-Tutorial/</id>
    <published>2018-09-27T07:00:17.000Z</published>
    <updated>2018-09-28T02:50:55.576Z</updated>
    
    <content type="html"><![CDATA[<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">#eye的用法</span><br><span class="line">print(&quot;eye的用法&quot;)</span><br><span class="line">C=6</span><br><span class="line">print (np.eye(C))</span><br><span class="line"></span><br><span class="line">#构建一个onehot矩阵</span><br><span class="line">print(&quot;onehot编码：&quot;)</span><br><span class="line">Y1=np.array([[3,1,2,5,4,2],[2,1,2,3,5,4]])</span><br><span class="line">print(Y1.reshape(-1)) #在都不指定维度，只有一个-1的时候，直接拉成一维的</span><br><span class="line">Y = np.eye(C)[Y1.reshape(-1)]#后面跟的数组说明1偏移的位置,所以这些数字不能够超出矩阵的范围</span><br><span class="line">#所以上述的表达式其实是在行方向延展了，在列方向置为1的位置由Y1.reshape(-1)这个list所提供，返回的Y值在行数上与Y1.reshape(-1)是一致的。</span><br><span class="line">print (Y.shape)</span><br><span class="line">print(Y)</span><br><span class="line">#对比上述</span><br><span class="line">Y = np.eye(C)[Y1.reshape(-1)].T</span><br><span class="line">print (Y.shape)</span><br><span class="line">print(Y)</span><br></pre></td></tr></table></figure><p>运行结果<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">eye的用法</span><br><span class="line">[[1. 0. 0. 0. 0. 0.]</span><br><span class="line"> [0. 1. 0. 0. 0. 0.]</span><br><span class="line"> [0. 0. 1. 0. 0. 0.]</span><br><span class="line"> [0. 0. 0. 1. 0. 0.]</span><br><span class="line"> [0. 0. 0. 0. 1. 0.]</span><br><span class="line"> [0. 0. 0. 0. 0. 1.]]</span><br><span class="line">onehot编码：</span><br><span class="line">[3 1 2 5 4 2 2 1 2 3 5 4]</span><br><span class="line">(12, 6)</span><br><span class="line">[[0. 0. 0. 1. 0. 0.]</span><br><span class="line"> [0. 1. 0. 0. 0. 0.]</span><br><span class="line"> [0. 0. 1. 0. 0. 0.]</span><br><span class="line"> [0. 0. 0. 0. 0. 1.]</span><br><span class="line"> [0. 0. 0. 0. 1. 0.]</span><br><span class="line"> [0. 0. 1. 0. 0. 0.]</span><br><span class="line"> [0. 0. 1. 0. 0. 0.]</span><br><span class="line"> [0. 1. 0. 0. 0. 0.]</span><br><span class="line"> [0. 0. 1. 0. 0. 0.]</span><br><span class="line"> [0. 0. 0. 1. 0. 0.]</span><br><span class="line"> [0. 0. 0. 0. 0. 1.]</span><br><span class="line"> [0. 0. 0. 0. 1. 0.]]</span><br><span class="line">(6, 12)</span><br><span class="line">[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]</span><br><span class="line"> [0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]</span><br><span class="line"> [0. 0. 1. 0. 0. 1. 1. 0. 1. 0. 0. 0.]</span><br><span class="line"> [1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]</span><br><span class="line"> [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1.]</span><br><span class="line"> [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0.]]</span><br></pre></td></tr></table></figure></p><p>“Xavier”初始化方法,为了使得网络中信息更好的流动，每一层输出的方差应该尽量相等。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>公共经济学之一</title>
    <link href="http://yoursite.com/2018/09/26/%E5%85%AC%E5%85%B1%E7%BB%8F%E6%B5%8E%E5%AD%A6/"/>
    <id>http://yoursite.com/2018/09/26/公共经济学/</id>
    <published>2018-09-26T10:32:22.000Z</published>
    <updated>2018-09-28T02:50:55.576Z</updated>
    
    <content type="html"><![CDATA[<h3 id="人生的自利"><a href="#人生的自利" class="headerlink" title="人生的自利"></a>人生的自利</h3><p>听了朱老师的公共经济学后，我就在想，经济学中的一些思想是否对我们的人生也能够有所启发，有所指导呢？在这里，我想论证一下人生的自利。</p><h4 id="自利的合理性"><a href="#自利的合理性" class="headerlink" title="自利的合理性"></a>自利的合理性</h4><p>经济学有两个基本假设：稀缺和自利。其中，自利也就是说，人总是在一定制度约束下，追求自身利益最大化。我觉得这种说法是非常有道理的。除了在经济学领域中，我们把视野放大到整个生物界，这在某种程度上也是与达尔文的进化论的相呼应的。生物学中讲，应激性是生物具有的普遍特性，它能够使生物“趋利避害”，增强生物适应周围环境的能力。生物界中这种自我保存的愿望，不就是一种最原始的自利吗？所以，自利作为一种经济学现象，其追踪溯源还是来自于人作为一种生物的本性，是一种合理的现象。卢梭在《论人类不平等的起源》中认为，在人类还没形成社会，仍然是自然人时，人类生来就有的怜悯缓和了个体出于自利的行为，从而促进了人类整体的相互保存。在人类形成了社会之后，便有了法律、道德、美德、制度，作为人追求利益的约束。因此每个人无论从感性还是理性，在追求自身利益最大化的过程中，都必须遵守一定的制度，只顾追求自身利益，而枉顾他人的利益，这无疑也是不利于人类整体的生存和发展的。这也是为什么说，自利是指人要在一定制度约束下追求自身利益最大化，若是没有约束，便成了自私。自私的人，无疑在社会上是可耻的，是要被摒弃的。</p><h4 id="如何自利"><a href="#如何自利" class="headerlink" title="如何自利"></a>如何自利</h4><p>经济学中有一个词叫机会成本，即把一定资源用于生产某种产品时所放弃的另一些产品生产上最大的收益。放在生活中也是一样的，我们做决策之前几乎都会考虑，如果我做这件事，能够得到多少利益，如果把金钱、时间和精力等资源放在令一件事情上，又能够得到多少，但是由于我们每个人的价值取向和自身眼光的局限性，在选择之中怎么评估不同选择能获得的最大利益呢，这在数学中是一个最优化问题，不过这个问题放在现实生活中的决策上就很复杂，约束条件很多，因变量也很多，不是那么容易量化。<br>那么我们可以定性的进行分析。我们每个人，每时每刻，每做出一项选择，无不是在与利益在打交道，这种利益又来源于我们的欲望，不仅包括生存的欲望，金钱上的欲望，或者是声誉、地位、理想，因此各种形式的利益上都可以归结于人类欲望。因此，人类追求利益的过程，也就是在满足自我欲望的过程。我们最求利益最大化，也可以看成如何尽可能满足自己的欲望。<br>这个最优化问题必须要有约束条件，因为人的欲望如果不加以节制，必定是贪婪的无穷无尽的，这个约束条件在个人身上的体现就是遵守法律，恪守道德，克制贪婪的欲望。人类由于这些约束而丧失的，乃是天然的自由以及对于他所企图的和所能得到的一切东西的那种无限的权利；而所获得的，乃是社会的自由以及对于他所享有的一切东西的所有权。从这个角度来看，有了约束，人们遵守规则，牺牲自己部分欲望，反而能够得到社会对自己享有东西所有权的认可，这何尝也不是一种更大的利益呢？因此我们在谋求利益的过程中，一定要审视是否遵守了法律，是否道德，这对于人类的社会的发展是有益的，同时也会反过来使自己受益。<br>既然人的欲望是无穷的，那么尽可能最大地满足自己的欲望便会导致一个恶劣的结果，永远也满足不了。因此，我们要追求利益最大化，并不是放纵自己的欲望。<br>举个例子，为什么现在很多人物资条件提高了，反而不管挣多少钱都越来越烦躁不安，感到不幸福？不是因为缺钱，而是被各种无休止的欲望所折磨。我们在满足欲望的同时，克制贪婪的欲望，才能体会到幸福，而幸福，我认为，才是人类追求利益的根本目的，正所谓知足常乐也是这个道理，我们不能舍本逐末。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;人生的自利&quot;&gt;&lt;a href=&quot;#人生的自利&quot; class=&quot;headerlink&quot; title=&quot;人生的自利&quot;&gt;&lt;/a&gt;人生的自利&lt;/h3&gt;&lt;p&gt;听了朱老师的公共经济学后，我就在想，经济学中的一些思想是否对我们的人生也能够有所启发，有所指导呢？在这里，我想论证一
      
    
    </summary>
    
      <category term="公共经济学课程感悟" scheme="http://yoursite.com/categories/%E5%85%AC%E5%85%B1%E7%BB%8F%E6%B5%8E%E5%AD%A6%E8%AF%BE%E7%A8%8B%E6%84%9F%E6%82%9F/"/>
    
    
      <category term="公共经济学" scheme="http://yoursite.com/tags/%E5%85%AC%E5%85%B1%E7%BB%8F%E6%B5%8E%E5%AD%A6/"/>
    
      <category term="感悟" scheme="http://yoursite.com/tags/%E6%84%9F%E6%82%9F/"/>
    
  </entry>
  
  <entry>
    <title>2-4 Optimization Methods</title>
    <link href="http://yoursite.com/2018/09/26/2-4-Optimization-Methods/"/>
    <id>http://yoursite.com/2018/09/26/2-4-Optimization-Methods/</id>
    <published>2018-09-26T01:42:28.000Z</published>
    <updated>2018-09-28T02:50:55.576Z</updated>
    
    <content type="html"><![CDATA[<p>np.sqrt(x) 计算数组各元素的平方根<br>np.square(x) 计算数组各元素的平方</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">&quot;&quot;&quot;</span><br><span class="line">打印结果:</span><br><span class="line">加法运算: 5+8=13</span><br><span class="line">减法运算: 5-8=-3</span><br><span class="line">乘法运算: 5*8=40</span><br><span class="line">除法运算: 5/8=0.625</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">#函数或lambda表达式作为参数传参</span><br><span class="line">def calculate(x, y, func):</span><br><span class="line">    return func(x, y)</span><br><span class="line">#加法</span><br><span class="line">def add(x, y):</span><br><span class="line">    return x + y</span><br><span class="line">#减法</span><br><span class="line">def sub(x, y):</span><br><span class="line">    return x - y</span><br><span class="line">a,b = 5,8</span><br><span class="line">add_ret = calculate(a, b, add)  #加法</span><br><span class="line">sub_ret = calculate(a, b, sub)  #减法</span><br><span class="line">mul_ret = calculate(a, b, lambda a,b : a*b)  #乘法</span><br><span class="line">dev_ret = calculate(a, b, lambda a,b : a/b)  #除法</span><br><span class="line"> </span><br><span class="line">print(&apos;加法运算: &#123;&#125;+&#123;&#125;=&#123;&#125;&apos;.format(a, b, add_ret))</span><br><span class="line">print(&apos;减法运算: &#123;&#125;-&#123;&#125;=&#123;&#125;&apos;.format(a, b, sub_ret))</span><br><span class="line">print(&apos;乘法运算: &#123;&#125;*&#123;&#125;=&#123;&#125;&apos;.format(a, b, mul_ret))</span><br><span class="line">print(&apos;除法运算: &#123;&#125;/&#123;&#125;=&#123;&#125;&apos;.format(a, b, dev_ret))</span><br></pre></td></tr></table></figure><p>本文介绍几种优化算法，可以加速学习，甚至提升学习效果。</p><h4 id="梯度下降法"><a href="#梯度下降法" class="headerlink" title="梯度下降法"></a>梯度下降法</h4><p>梯度下降法是（Gradient Descent (GD)）机器学习中最简单的优化算法。<br>将所有 $m$ 个算例用于一步梯度下降时，称为 Batch Gradient Descent。当训练集较大时，收敛速度比较慢。<br>将一个算例用于一步梯度下降时，称为 Stochastic（随机的） Gradient Descent (SGD)。当训练集较大时，收敛速度快，但是会在聚集点处震荡。<br>将$n$个算例用于一步梯度下降时（$1&lt;n&lt;m$），称为 Mini-Batch Gradient descent。</p><h4 id="Mini-Batch-梯度下降法"><a href="#Mini-Batch-梯度下降法" class="headerlink" title="Mini-Batch 梯度下降法"></a>Mini-Batch 梯度下降法</h4><p>首先建立训练集(X, Y)的 mini-batches。分为两步，第一步是将(X, Y)随机打乱，这里要保证 X 和 Y 打乱后也要一一对应；第二步是进行划分，划分的大小一般取2的指数，如 16, 32, 64, 128，注意，最后一个 mini-batch 也许不够划分大小，不过这对训练没有影响。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">def random_mini_batches(X, Y, mini_batch_size = 64:</span><br><span class="line">    </span><br><span class="line">    m = X.shape[1]                  # number of training examples</span><br><span class="line">    mini_batches = []</span><br><span class="line">        </span><br><span class="line">    # Step 1: Shuffle (X, Y)</span><br><span class="line">    permutation = list(np.random.permutation(m))</span><br><span class="line">    shuffled_X = X[:, permutation]</span><br><span class="line">    shuffled_Y = Y[:, permutation].reshape((1,m))</span><br><span class="line"></span><br><span class="line">    # Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.</span><br><span class="line">    num_complete_minibatches = math.floor(m/mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning</span><br><span class="line">    for k in range(0, num_complete_minibatches):</span><br><span class="line">        mini_batch_X = shuffled_X[:, k * mini_batch_size : (k + 1) * mini_batch_size]</span><br><span class="line">        mini_batch_Y = shuffled_Y[:, k * mini_batch_size : (k + 1) * mini_batch_size]</span><br><span class="line">        mini_batch = (mini_batch_X, mini_batch_Y)</span><br><span class="line">        mini_batches.append(mini_batch)</span><br><span class="line"></span><br><span class="line">    # Handling the end case (last mini-batch &lt; mini_batch_size)</span><br><span class="line">    if m % mini_batch_size != 0:</span><br><span class="line">        mini_batch_X = shuffled_X[:, num_complete_minibatches * mini_batch_size : m]</span><br><span class="line">        mini_batch_Y = shuffled_Y[:, num_complete_minibatches * mini_batch_size : m]</span><br><span class="line">        mini_batch = (mini_batch_X, mini_batch_Y)</span><br><span class="line">        mini_batches.append(mini_batch)</span><br><span class="line">    </span><br><span class="line">    return mini_batches</span><br></pre></td></tr></table></figure><h4 id="Momentum-梯度下降法"><a href="#Momentum-梯度下降法" class="headerlink" title="Momentum 梯度下降法"></a>Momentum 梯度下降法</h4><p>mini-batch梯度下降法仅使用部分训练集来更新参数，因此会产生一些偏差，在收敛的过程中震荡，我们使用 Momentum 通过对梯度求指数加权平均来平滑梯度，从而避免震荡。不仅mini-batch梯度下降法，也可以把 Momentum 加到batch gradient descent 或者 stochastic gradient descent中去。<br>首先我们需要使用参数 $v$ 来代表过去的梯度，由此来求指数加权平均。对其进行初始化为0，$v$ 与 参数 $dW$ 和 $db$ 大小相同。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">def initialize_velocity(parameters):</span><br><span class="line"></span><br><span class="line">    L = len(parameters) // 2 # number of layers in the neural networks</span><br><span class="line">    v = &#123;&#125;</span><br><span class="line">    </span><br><span class="line">    # Initialize velocity</span><br><span class="line">    for l in range(L):</span><br><span class="line">        v[&apos;dW&apos; + str(l + 1)] = np.zeros((parameters[&apos;W&apos; + str(l+1)].shape[0], parameters[&apos;W&apos; + str(l+1)].shape[1]))</span><br><span class="line">        v[&apos;db&apos; + str(l+1)] = np.zeros((parameters[&apos;b&apos; + str(l+1)].shape[0], parameters[&apos;b&apos; + str(l+1)].shape[1]))</span><br><span class="line">        </span><br><span class="line">    return v</span><br></pre></td></tr></table></figure></p><p>然后更新参数。</p><p>$$ \begin{cases}<br>v_{dW^{[l]}} = \beta v_{dW^{[l]}} + (1 - \beta) dW^{[l]} \\<br>W^{[l]} = W^{[l]} - \alpha v_{dW^{[l]}}<br>\end{cases}$$</p><p>$$\begin{cases}<br>v_{db^{[l]}} = \beta v_{db^{[l]}} + (1 - \beta) db^{[l]} \\<br>b^{[l]} = b^{[l]} - \alpha v_{db^{[l]}}<br>\end{cases}$$</p><p>实现如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">def update_parameters_with_momentum(parameters, grads, v, beta, learning_rate):</span><br><span class="line">    L = len(parameters) // 2 # number of layers in the neural networks</span><br><span class="line">    </span><br><span class="line">    # Momentum update for each parameter</span><br><span class="line">    for l in range(L):</span><br><span class="line">        # compute velocities</span><br><span class="line">        v[&apos;dW&apos; + str(l + 1)] = beta * v[&apos;dW&apos; + str(l + 1)] + (1 - beta) * grads[&apos;dW&apos; + str(l + 1)]</span><br><span class="line">        v[&apos;db&apos; + str(l + 1)] = beta * v[&apos;db&apos; + str(l + 1)] + (1 - beta) * grads[&apos;db&apos; + str(l + 1)]</span><br><span class="line">        # update parameters</span><br><span class="line">        parameters[&apos;W&apos; + str(l + 1)] = parameters[&apos;W&apos; + str(l + 1)] - learning_rate * v[&apos;dW&apos; + str(l + 1)]</span><br><span class="line">        parameters[&apos;b&apos; + str(l + 1)] = parameters[&apos;b&apos; + str(l + 1)] - learning_rate * v[&apos;db&apos; + str(l + 1)]</span><br><span class="line">        </span><br><span class="line">    return parameters, v</span><br></pre></td></tr></table></figure></p><p>如果 $\beta = 0$，那么就成了普通的梯度下降法。$\beta$ 越大，越平滑，因为把过去的梯度更多地考虑进去了，但是过大也会使梯度过度平滑。<br>常用的 $\beta$ 值取 0.8 到 0.999。如果感觉没有必要调这个参数值, 一般取 $\beta = 0.9$。</p><h4 id="Adam-梯度下降法"><a href="#Adam-梯度下降法" class="headerlink" title="Adam 梯度下降法"></a>Adam 梯度下降法</h4><p>Adam 是训练神经网络最有效的优化算法之一。它同时结合了RMSProp 和 Momentum。<br>首先计算过去梯度的指数加权平均值，存在参数 $v$ 中，并进行偏差修正，得到 $v^{corrected}$。<br>然后计算过去梯度平方的指数加权平均值，存在参数 $s$ 中，并进行偏差修正，得到 $s^{corrected}$。<br>最后结合前两步更新参数。</p><p>$$\begin{cases}<br>v_{dW^{[l]}} = \beta_1 v_{dW^{[l]}} + (1 - \beta_1) \frac{\partial \mathcal{J} }{ \partial W^{[l]} } \\<br>v_{dW^{[l]}}^{corrected} = \frac{v_{dW^{[l]}}}{1 - (\beta_1)^t} \\<br>s_{dW^{[l]}} = \beta_2 s_{dW^{[l]}} + (1 - \beta_2) (\frac{\partial \mathcal{J} }{\partial W^{[l]} })^2 \\<br>s_{dW^{[l]}}^{corrected} = \frac{s_{dW^{[l]}}}{1 - (\beta_1)^t} \\<br>W^{[l]} = W^{[l]} - \alpha \frac{v_{dW^{[l]}}^{corrected}}{\sqrt{s_{dW^{[l]}}^{corrected}} + \varepsilon}<br>\end{cases}$$</p><p>下面进行实现，首先对 $v,s$ 进行初始化。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">def initialize_adam(parameters) :</span><br><span class="line">    </span><br><span class="line">    L = len(parameters) // 2 # number of layers in the neural networks</span><br><span class="line">    v = &#123;&#125;</span><br><span class="line">    s = &#123;&#125;</span><br><span class="line">    </span><br><span class="line">    # Initialize v, s. Input: &quot;parameters&quot;. Outputs: &quot;v, s&quot;.</span><br><span class="line">    for l in range(L): </span><br><span class="line">        v[&apos;dW&apos; + str(l + 1)] = np.zeros((parameters[&apos;W&apos; + str(l+1)].shape[0], parameters[&apos;W&apos; + str(l+1)].shape[1]))</span><br><span class="line">        v[&apos;db&apos; + str(l + 1)] = np.zeros((parameters[&apos;b&apos; + str(l+1)].shape[0], parameters[&apos;b&apos; + str(l+1)].shape[1]))</span><br><span class="line">        s[&apos;dW&apos; + str(l + 1)] = np.zeros((parameters[&apos;W&apos; + str(l+1)].shape[0], parameters[&apos;W&apos; + str(l+1)].shape[1]))</span><br><span class="line">        s[&apos;db&apos; + str(l + 1)] = np.zeros((parameters[&apos;b&apos; + str(l+1)].shape[0], parameters[&apos;b&apos; + str(l+1)].shape[1]))</span><br><span class="line"> </span><br><span class="line">    return v, s</span><br></pre></td></tr></table></figure></p><p>对参数进行更新。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">def update_parameters_with_adam(parameters, grads, v, s, t, learning_rate = 0.01,</span><br><span class="line">                                beta1 = 0.9, beta2 = 0.999,  epsilon = 1e-8):</span><br><span class="line">    </span><br><span class="line">    L = len(parameters) // 2                 # number of layers in the neural networks</span><br><span class="line">    v_corrected = &#123;&#125;                         # Initializing first moment estimate, python dictionary</span><br><span class="line">    s_corrected = &#123;&#125;                         # Initializing second moment estimate, python dictionary</span><br><span class="line">    </span><br><span class="line">    # Perform Adam update on all parameters</span><br><span class="line">    for l in range(L):</span><br><span class="line">        # Moving average of the gradients. Inputs: &quot;v, grads, beta1&quot;. Output: &quot;v&quot;.</span><br><span class="line">        v[&apos;dW&apos; + str(l + 1)] = beta1 * v[&apos;dW&apos; + str(l + 1)] + (1 - beta1) * grads[&apos;dW&apos; + str(l + 1)]</span><br><span class="line">        v[&apos;db&apos; + str(l + 1)] = beta1 * v[&apos;db&apos; + str(l + 1)] + (1 - beta1) * grads[&apos;db&apos; + str(l + 1)]</span><br><span class="line"></span><br><span class="line">        # Compute bias-corrected first moment estimate. Inputs: &quot;v, beta1, t&quot;. Output: &quot;v_corrected&quot;.</span><br><span class="line">        v_corrected[&apos;dW&apos; + str(l + 1)] = v[&apos;dW&apos; + str(l + 1)] / (1 - beta1 ** t)</span><br><span class="line">        v_corrected[&apos;db&apos; + str(l + 1)] = v[&apos;db&apos; + str(l + 1)] / (1 - beta1 ** t)</span><br><span class="line"></span><br><span class="line">        # Moving average of the squared gradients. Inputs: &quot;s, grads, beta2&quot;. Output: &quot;s&quot;.</span><br><span class="line">        s[&apos;dW&apos; + str(l + 1)] = beta2 * s[&apos;dW&apos; + str(l + 1)] + (1 - beta2) * (grads[&apos;dW&apos; + str(l + 1)] ** 2)</span><br><span class="line">        s[&apos;db&apos; + str(l + 1)] = beta2 * s[&apos;db&apos; + str(l + 1)] + (1 - beta2) * (grads[&apos;db&apos; + str(l + 1)] ** 2)</span><br><span class="line"></span><br><span class="line">        # Compute bias-corrected second raw moment estimate. Inputs: &quot;s, beta2, t&quot;. Output: &quot;s_corrected&quot;.</span><br><span class="line"></span><br><span class="line">        s_corrected[&apos;dW&apos; + str(l + 1)] = s[&apos;dW&apos; + str(l + 1)] / (1 - beta2 ** t)</span><br><span class="line">        s_corrected[&apos;db&apos; + str(l + 1)] = s[&apos;db&apos; + str(l + 1)] / (1 - beta2 ** t)</span><br><span class="line"></span><br><span class="line">        # Update parameters. Inputs: &quot;parameters, learning_rate, v_corrected, s_corrected, epsilon&quot;. Output: &quot;parameters&quot;.</span><br><span class="line">        parameters[&apos;W&apos; + str(l + 1)] = parameters[&apos;W&apos; + str(l + 1)] - learning_rate * v_corrected[&apos;dW&apos; + str(l + 1)] / (np.sqrt(s_corrected[&apos;dW&apos; + str(l + 1)]) + epsilon)</span><br><span class="line">        parameters[&apos;b&apos; + str(l + 1)] = parameters[&apos;b&apos; + str(l + 1)] - learning_rate * v_corrected[&apos;db&apos; + str(l + 1)] / (np.sqrt(s_corrected[&apos;db&apos; + str(l + 1)]) + epsilon)</span><br><span class="line"></span><br><span class="line">    return parameters, v, s</span><br></pre></td></tr></table></figure></p><p>Adam 中，超参数 learning_rate 是需要调的，一般设置 beta1 = 0.9, beta2 = 0.999,  epsilon = 1e-8。</p><h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><p>最后，我们怎么在模型中使用这些优化算法呢？</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line">def model(X, Y, layers_dims, optimizer, learning_rate = 0.0007, mini_batch_size = 64, beta = 0.9,</span><br><span class="line">          beta1 = 0.9, beta2 = 0.999,  epsilon = 1e-8, num_epochs = 10000, print_cost = True):</span><br><span class="line"></span><br><span class="line">    L = len(layers_dims)             # number of layers in the neural networks</span><br><span class="line">    costs = []                       # to keep track of the cost</span><br><span class="line">    t = 0                            # initializing the counter required for Adam update</span><br><span class="line">    seed = 10                        # For grading purposes, so that your &quot;random&quot; minibatches are the same as ours</span><br><span class="line">    </span><br><span class="line">    # Initialize parameters</span><br><span class="line">    parameters = initialize_parameters(layers_dims)</span><br><span class="line"></span><br><span class="line">    # Initialize the optimizer</span><br><span class="line">    if optimizer == &quot;gd&quot;:</span><br><span class="line">        pass # no initialization required for gradient descent</span><br><span class="line">    elif optimizer == &quot;momentum&quot;:</span><br><span class="line">        v = initialize_velocity(parameters)</span><br><span class="line">    elif optimizer == &quot;adam&quot;:</span><br><span class="line">        v, s = initialize_adam(parameters)</span><br><span class="line">    </span><br><span class="line">    # Optimization loop</span><br><span class="line">    for i in range(num_epochs):</span><br><span class="line">        # Define the random minibatches. We increment the seed to reshuffle differently the dataset after each epoch</span><br><span class="line">        seed = seed + 1</span><br><span class="line">        minibatches = random_mini_batches(X, Y, mini_batch_size, seed)</span><br><span class="line">        for minibatch in minibatches:</span><br><span class="line">            # Select a minibatch</span><br><span class="line">            (minibatch_X, minibatch_Y) = minibatch</span><br><span class="line"></span><br><span class="line">            # Forward propagation</span><br><span class="line">            a3, caches = forward_propagation(minibatch_X, parameters)</span><br><span class="line"></span><br><span class="line">            # Compute cost</span><br><span class="line">            cost = compute_cost(a3, minibatch_Y)</span><br><span class="line"></span><br><span class="line">            # Backward propagation</span><br><span class="line">            grads = backward_propagation(minibatch_X, minibatch_Y, caches)</span><br><span class="line"></span><br><span class="line">            # Update parameters</span><br><span class="line">            if optimizer == &quot;gd&quot;:</span><br><span class="line">                parameters = update_parameters_with_gd(parameters, grads, learning_rate)</span><br><span class="line">            elif optimizer == &quot;momentum&quot;:</span><br><span class="line">                parameters, v = update_parameters_with_momentum(parameters, grads, v, beta, learning_rate)</span><br><span class="line">            elif optimizer == &quot;adam&quot;:</span><br><span class="line">                t = t + 1 # Adam counter</span><br><span class="line">                parameters, v, s = update_parameters_with_adam(parameters, grads, v, s,</span><br><span class="line"></span><br><span class="line">        # Print the cost every 1000 epoch</span><br><span class="line">        if print_cost and i % 1000 == 0:</span><br><span class="line">            print (&quot;Cost after epoch %i: %f&quot; %(i, cost))</span><br><span class="line">        if print_cost and i % 100 == 0:</span><br><span class="line">            costs.append(cost)</span><br><span class="line">                </span><br><span class="line">    # plot the cost</span><br><span class="line">    plt.plot(costs)</span><br><span class="line">    plt.ylabel(&apos;cost&apos;)</span><br><span class="line">    plt.xlabel(&apos;epochs (per 100)&apos;)</span><br><span class="line">    plt.title(&quot;Learning rate = &quot; + str(learning_rate))</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line">    return parameters</span><br></pre></td></tr></table></figure><p>调用模型。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">parameters = model(train_X, train_Y, layers_dims, optimizer = &quot;gd&quot;)</span><br></pre></td></tr></table></figure></p><p>Momentum 通常有所帮助，如果学习率较低且数据集过于简单的话，其影响几乎可以忽略不计。<br>另一方面，Adam 明显优于mini-batch 梯度下降和Momentum。<br>如果在数据集上运行更多时间，则所有这三种方法都将产生非常好的结果。 但是，Adam 收敛得更快。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;np.sqrt(x) 计算数组各元素的平方根&lt;br&gt;np.square(x) 计算数组各元素的平方&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;lin
      
    
    </summary>
    
      <category term="深度学习习题" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B9%A0%E9%A2%98/"/>
    
    
      <category term="DeepLearning" scheme="http://yoursite.com/tags/DeepLearning/"/>
    
      <category term="习题" scheme="http://yoursite.com/tags/%E4%B9%A0%E9%A2%98/"/>
    
  </entry>
  
  <entry>
    <title>数据挖掘引论</title>
    <link href="http://yoursite.com/2018/09/21/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E5%BC%95%E8%AE%BA/"/>
    <id>http://yoursite.com/2018/09/21/数据挖掘引论/</id>
    <published>2018-09-21T05:53:42.000Z</published>
    <updated>2018-09-28T02:50:55.576Z</updated>
    
    <content type="html"><![CDATA[<p>我们的数据是丰富的，但信息是贫乏的，数据中的有价值的知识不能被提取出来，数据再多也都变成了“数据坟墓”。可以这么说，我们并不是真正的生活在“信息时代”，而只是生活在数据时代。随着数据库和数据管理产业的不断发展：数据收集、数据库创建、数据管理和高级数据分析，如今，大量的数据被收集和存储，大量的数据库系统提供了查询和事务处理，高级数据分析自然成为信息技术发展的必然趋势。把数据挖掘类比为在砂石中淘金是很形象的，如何挖掘大量数据中有用的信息，提取出数据的利用价值，就是数据挖掘的研究目标。<br>数据挖掘可以分为两种任务：描述性和预测性。描述性挖掘任务刻画目标数据中数据的一般性质。预测性挖掘任务在当前数据上进行归纳，以便做出预测。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;我们的数据是丰富的，但信息是贫乏的，数据中的有价值的知识不能被提取出来，数据再多也都变成了“数据坟墓”。可以这么说，我们并不是真正的生活在“信息时代”，而只是生活在数据时代。随着数据库和数据管理产业的不断发展：数据收集、数据库创建、数据管理和高级数据分析，如今，大量的数据被
      
    
    </summary>
    
      <category term="数据挖掘" scheme="http://yoursite.com/categories/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/"/>
    
    
      <category term="数据挖掘" scheme="http://yoursite.com/tags/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/"/>
    
  </entry>
  
  <entry>
    <title>2-3 Gradient Checking</title>
    <link href="http://yoursite.com/2018/09/20/2-3-Gradient-Checking/"/>
    <id>http://yoursite.com/2018/09/20/2-3-Gradient-Checking/</id>
    <published>2018-09-20T07:58:13.000Z</published>
    <updated>2018-09-28T02:50:55.576Z</updated>
    
    <content type="html"><![CDATA[<p>np.linalg.norm(x, ord=None, axis=None, keepdims=False) 求向量（矩阵）范数。</p><table><thead><tr><th style="text-align:center">参数</th><th style="text-align:center">说明</th><th style="text-align:center">计算方法</th></tr></thead><tbody><tr><td style="text-align:center">默认</td><td style="text-align:center">二范数：$l_2$</td><td style="text-align:center">$\sqrt{x_{1}^{2}+x_{2}^{2}+\cdots+x_{n}^{2}}$</td></tr><tr><td style="text-align:center">ord = 2</td><td style="text-align:center">二范数：$l_2$</td><td style="text-align:center">$\sqrt{x_{1}^{2}+x_{2}^{2}+\cdots+x_{n}^{2}}$</td></tr><tr><td style="text-align:center">ord = 1</td><td style="text-align:center">一范数：$l_1$</td><td style="text-align:center">$\lvert x_{1} \rvert + \cdots + \lvert x_{n} \rvert $</td></tr><tr><td style="text-align:center">ord=np.inf</td><td style="text-align:center">无穷范数 $l_{\infty}$</td><td style="text-align:center">$\max({\lvert x_{i}) \rvert}$</td></tr></tbody></table><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; x = np.array([3, 4])</span><br><span class="line">&gt;&gt;&gt; np.linalg.norm(x)</span><br><span class="line">5.</span><br><span class="line">&gt;&gt;&gt; np.linalg.norm(x, ord=2)</span><br><span class="line">5.</span><br><span class="line">&gt;&gt;&gt; np.linalg.norm(x, ord=1)</span><br><span class="line">7.</span><br><span class="line">&gt;&gt;&gt; np.linalg.norm(x, ord=np.inf)</span><br><span class="line">4</span><br></pre></td></tr></table></figure><h4 id="算法步骤"><a href="#算法步骤" class="headerlink" title="算法步骤"></a>算法步骤</h4><p>梯度检查（Gradient Checking）用于检查实现的反向传播算法是否正确。利用前向传播函数，通过公式<br>$$ \frac{\partial J}{\partial \theta} = \lim_{\varepsilon \to 0} \frac{J(\theta + \varepsilon) - J(\theta - \varepsilon)}{2 \varepsilon} $$<br>来计算导数的近似值，与反向传播计算得到的导数进行对比，来判断反向传播是否正确。<br>具体步骤如下：</p><ul><li>首先计算导数的近似值<ol><li>$\theta^{+} = \theta + \varepsilon$</li><li>$\theta^{-} = \theta - \varepsilon$</li><li>$J^{+} = J(\theta^{+})$</li><li>$J^{-} = J(\theta^{-})$</li><li>$gradapprox = \frac{J^{+} - J^{-}}{2  \varepsilon}$</li></ol></li><li>然后计算反向传播算法得到的导数grad</li><li>最后通过下面公式来对比两个值的差别大小，若很小，如小于$10^{-7}$，便认为反向传播算法的实现是正确的。<br>$$ difference = \frac {\mid\mid grad - gradapprox \mid\mid_2}{\mid\mid grad \mid\mid_2 + \mid\mid gradapprox \mid\mid_2} $$</li></ul><p>当有很多个参数时，分别对每个进行梯度检查。算法步骤如下：<br>For each i in num_parameters:</p><ul><li>To compute <code>J_plus[i]</code>:<ol><li>Set $\theta^{+}$ to <code>np.copy(parameters_values)</code></li><li>Set $\theta^{+}_i$ to $\theta^{+}_i + \varepsilon$</li><li>Calculate $J^{+}_i$ using to <code>forward_propagation_n(x, y, vector_to_dictionary(</code>$\theta^{+}$ <code>))</code>.     </li></ol></li><li>To compute <code>J_minus[i]</code>: do the same thing with $\theta^{-}$</li><li>Compute $gradapprox[i] = \frac{J^{+}_i - J^{-}_i}{2 \varepsilon}$</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">def gradient_check_n(parameters, gradients, X, Y, epsilon = 1e-7):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Arguments:</span><br><span class="line">    parameters -- python dictionary containing your parameters &quot;W1&quot;, &quot;b1&quot;, &quot;W2&quot;, &quot;b2&quot;, &quot;W3&quot;, &quot;b3&quot;:</span><br><span class="line">    grad -- output of backward_propagation_n, contains gradients of the cost with respect to the parameters. </span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    </span><br><span class="line">    # Set-up variables</span><br><span class="line">    parameters_values, _ = dictionary_to_vector(parameters)</span><br><span class="line">    grad = gradients_to_vector(gradients)</span><br><span class="line">    num_parameters = parameters_values.shape[0]</span><br><span class="line">    J_plus = np.zeros((num_parameters, 1))</span><br><span class="line">    J_minus = np.zeros((num_parameters, 1))</span><br><span class="line">    gradapprox = np.zeros((num_parameters, 1))</span><br><span class="line">    </span><br><span class="line">    # Compute gradapprox</span><br><span class="line">    for i in range(num_parameters):</span><br><span class="line">        </span><br><span class="line">        # Compute J_plus[i]. Inputs: &quot;parameters_values, epsilon&quot;. Output = &quot;J_plus[i]&quot;.</span><br><span class="line">        # &quot;_&quot; is used because the function you have to outputs two parameters but we only care about the first one</span><br><span class="line">        theta_plus =  np.copy(parameters_values)                                     # Step 1</span><br><span class="line">        theta_plus[i] = theta_plus[i] +  epsilon                            # Step 2</span><br><span class="line">        J_plus[i], _ =  forward_propagation_n(X, Y, vector_to_dictionary(theta_plus))                                  # Step 3</span><br><span class="line">        </span><br><span class="line">        # Compute J_minus[i]. Inputs: &quot;parameters_values, epsilon&quot;. Output = &quot;J_minus[i]&quot;.</span><br><span class="line">        theta_minus =  np.copy(parameters_values)                                    # Step 1</span><br><span class="line">        theta_minus[i] = theta_minus[i] - epsilon                          # Step 2        </span><br><span class="line">        J_minus[i], _ =  forward_propagation_n(X, Y, vector_to_dictionary(theta_minus))                                   # Step 3</span><br><span class="line">        </span><br><span class="line">        # Compute gradapprox[i]</span><br><span class="line">        gradapprox[i] = (J_plus[i] - J_minus[i]) / (2 * epsilon)</span><br><span class="line">    </span><br><span class="line">    # Compare gradapprox to backward propagation gradients by computing difference.</span><br><span class="line">    numerator = np.linalg.norm(grad - gradapprox)                              # Step 1&apos;</span><br><span class="line">    denumerator = np.linalg.norm(grad) + np.linalg.norm(gradapprox)                          # Step 2&apos;</span><br><span class="line">    difference =   numerator / denumerator                                               # Step 3&apos;</span><br><span class="line"></span><br><span class="line">    if difference &gt; 1e-7:</span><br><span class="line">        print (&quot;\033[93m&quot; + &quot;There is a mistake in the backward propagation! difference = &quot; + str(difference) + &quot;\033[0m&quot;)</span><br><span class="line">    else:</span><br><span class="line">        print (&quot;\033[92m&quot; + &quot;Your backward propagation works perfectly fine! difference = &quot; + str(difference) + &quot;\033[0m&quot;)</span><br><span class="line">    </span><br><span class="line">    return difference</span><br></pre></td></tr></table></figure><p>注意，进行梯度检查是很耗时的，仅仅是用来检查反向传播的实现是否正确，所以我们在训练过程中不能加进去。而且梯度检查不能用在使用dropout的神经网络中，进行梯度检查时要关闭dropout。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;np.linalg.norm(x, ord=None, axis=None, keepdims=False) 求向量（矩阵）范数。&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&quot;text-align:center&quot;&gt;参数&lt;/th&gt;
&lt;th styl
      
    
    </summary>
    
      <category term="深度学习习题" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B9%A0%E9%A2%98/"/>
    
    
      <category term="DeepLearning" scheme="http://yoursite.com/tags/DeepLearning/"/>
    
      <category term="习题" scheme="http://yoursite.com/tags/%E4%B9%A0%E9%A2%98/"/>
    
  </entry>
  
  <entry>
    <title>2-2 Regularization</title>
    <link href="http://yoursite.com/2018/09/19/2-2%20Regularization/"/>
    <id>http://yoursite.com/2018/09/19/2-2 Regularization/</id>
    <published>2018-09-19T02:04:50.000Z</published>
    <updated>2018-09-28T02:50:55.576Z</updated>
    
    <content type="html"><![CDATA[<p>np.square(Wl) 计算各元素的平方<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; np.square([-1, 1])</span><br><span class="line">array([1, 1], dtype=int32)</span><br></pre></td></tr></table></figure></p><p>np.nansum() 求所有元素和的时候将非数字（NaN）当做 0。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; np.nansum([1, np.nan])</span><br><span class="line">1.0</span><br><span class="line">&gt;&gt;&gt; a = np.array([[1, 1], [1, np.nan]])</span><br><span class="line">&gt;&gt;&gt; np.nansum(a)</span><br><span class="line">3.0</span><br><span class="line">&gt;&gt;&gt; np.nansum(a, axis=0)</span><br><span class="line">array([ 2.,  1.])</span><br><span class="line">&gt;&gt;&gt; np.nansum([1, np.nan, np.inf])</span><br><span class="line">inf</span><br><span class="line">&gt;&gt;&gt; np.nansum([1, np.nan, np.NINF])</span><br><span class="line">-inf</span><br><span class="line">&gt;&gt;&gt; np.nansum([1, np.nan, np.inf, -np.inf]) # both +/- infinity present</span><br><span class="line">nan</span><br></pre></td></tr></table></figure></p><p>np.c_[xx.ravel(), yy.ravel()]<br>np.r_是按行连接两个矩阵，就是把两矩阵上下相加，要求行数相等。<br>np.c_是按列连接两个矩阵，就是把两矩阵左右相加，要求列数相等。<br>numpy.ravel((a, order=’C’)是将多维数组降为一维，等同于reshape(-1, order=order)。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; x = np.array([[1, 2, 3], [4, 5, 6]])</span><br><span class="line">&gt;&gt;&gt; print(np.ravel(x))</span><br><span class="line">[1 2 3 4 5 6]</span><br><span class="line">&gt;&gt;&gt; print(x.reshape(-1))</span><br><span class="line">[1 2 3 4 5 6]</span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line">&gt;&gt;&gt; print(np.ravel(x, order=&apos;F&apos;))</span><br><span class="line">[1 4 2 5 3 6]</span><br></pre></td></tr></table></figure></p><p>这里介绍解决过拟合的两个方法，一个是使用L2正则化，另一个是使用Dropout正则化。</p><h4 id="算法步骤"><a href="#算法步骤" class="headerlink" title="算法步骤"></a>算法步骤</h4><p>首先先介绍一个三层神经网络的模型，对此网络进行正则化。</p><h5 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h5><p>模型参数中，lamdb是L2正则化的参数，keep_prob是Dropout的参数。当lamdb &gt; 0 时，说明使用了L2正则化，这时，前向传播不变，要调用对应的损失函数compute_cost_with_regularization及对应的反向传播函数backward_propagation_with_regularization；当keep_prob != 1 时，说明使用了Dropout，这时，损失函数不变，要调用对应的前向传播函数forward_propagation_with_dropout及对应的反向传播函数backward_propagation_with_dropout。这些函数将会在接下来进行介绍。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line">def model(X, Y, learning_rate = 0.3, num_iterations = 30000, print_cost = True, lambd = 0, keep_prob = 1):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Implements a three-layer neural network: LINEAR-&gt;RELU-&gt;LINEAR-&gt;RELU-&gt;LINEAR-&gt;SIGMOID.</span><br><span class="line">    Arguments:</span><br><span class="line">    lambd -- regularization hyperparameter, scalar</span><br><span class="line">    keep_prob - probability of keeping a neuron active during drop-out, scalar.</span><br><span class="line">    Returns:</span><br><span class="line">    parameters -- parameters learned by the model. They can then be used to predict.</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">        </span><br><span class="line">    grads = &#123;&#125;</span><br><span class="line">    costs = []                            # to keep track of the cost</span><br><span class="line">    m = X.shape[1]                        # number of examples</span><br><span class="line">    layers_dims = [X.shape[0], 20, 3, 1]</span><br><span class="line">    </span><br><span class="line">    # Initialize parameters dictionary.</span><br><span class="line">    parameters = initialize_parameters(layers_dims)</span><br><span class="line"></span><br><span class="line">    # Loop (gradient descent)</span><br><span class="line">    for i in range(0, num_iterations):</span><br><span class="line"></span><br><span class="line">        # Forward propagation: LINEAR -&gt; RELU -&gt; LINEAR -&gt; RELU -&gt; LINEAR -&gt; SIGMOID.</span><br><span class="line">        if keep_prob == 1:</span><br><span class="line">            a3, cache = forward_propagation(X, parameters)</span><br><span class="line">        elif keep_prob &lt; 1:</span><br><span class="line">            a3, cache = forward_propagation_with_dropout(X, parameters, keep_prob)</span><br><span class="line">        </span><br><span class="line">        # Cost function</span><br><span class="line">        if lambd == 0:</span><br><span class="line">            cost = compute_cost(a3, Y)</span><br><span class="line">        else:</span><br><span class="line">            cost = compute_cost_with_regularization(a3, Y, parameters, lambd)</span><br><span class="line">            </span><br><span class="line">        # Backward propagation.</span><br><span class="line">        assert(lambd==0 or keep_prob==1)    # it is possible to use both L2 regularization and dropout, </span><br><span class="line">                                            # but this assignment will only explore one at a time</span><br><span class="line">        if lambd == 0 and keep_prob == 1:</span><br><span class="line">            grads = backward_propagation(X, Y, cache)</span><br><span class="line">        elif lambd != 0:</span><br><span class="line">            grads = backward_propagation_with_regularization(X, Y, cache, lambd)</span><br><span class="line">        elif keep_prob &lt; 1:</span><br><span class="line">            grads = backward_propagation_with_dropout(X, Y, cache, keep_prob)</span><br><span class="line">        </span><br><span class="line">        # Update parameters.</span><br><span class="line">        parameters = update_parameters(parameters, grads, learning_rate)</span><br><span class="line">        </span><br><span class="line">        # Print the loss every 10000 iterations</span><br><span class="line">        if print_cost and i % 10000 == 0:</span><br><span class="line">            print(&quot;Cost after iteration &#123;&#125;: &#123;&#125;&quot;.format(i, cost))</span><br><span class="line">        if print_cost and i % 1000 == 0:</span><br><span class="line">            costs.append(cost)</span><br><span class="line">    </span><br><span class="line">    # plot the cost</span><br><span class="line">    plt.plot(costs)</span><br><span class="line">    plt.ylabel(&apos;cost&apos;)</span><br><span class="line">    plt.xlabel(&apos;iterations (x1,000)&apos;)</span><br><span class="line">    plt.title(&quot;Learning rate =&quot; + str(learning_rate))</span><br><span class="line">    plt.show()</span><br><span class="line">    </span><br><span class="line">    return parameters</span><br></pre></td></tr></table></figure></p><h5 id="L2-正则化"><a href="#L2-正则化" class="headerlink" title="L2 正则化"></a>L2 正则化</h5><p>L2正则化依赖以下假设，即具有较小超参数的模型比较大的模型要更简单。<br>对于L2正则化，在原损失函数后面增加了对超参数W的惩罚项。<br>$$J_{regularized} = \small \underbrace{-\frac{1}{m} \sum\limits_{i = 1}^{m} \large{(}\small y^{[i]}\log\left(a^{[L][i]}\right) + (1-y^{[i]})\log\left(1- a^{[L][i]}\right) \large{)} }_\text{cross-entropy cost} + \underbrace{\frac{1}{m} \frac{\lambda}{2} \sum\limits_l\sum\limits_k\sum\limits_j W_{k,j}^{[l]2} }_\text{L2 regularization cost} $$<br>下面我们实现函数compute_cost_with_regularization()。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">def compute_cost_with_regularization(A3, Y, parameters, lambd):</span><br><span class="line">    m = Y.shape[1]</span><br><span class="line">    W1 = parameters[&quot;W1&quot;]</span><br><span class="line">    W2 = parameters[&quot;W2&quot;]</span><br><span class="line">    W3 = parameters[&quot;W3&quot;]</span><br><span class="line">    </span><br><span class="line">    cross_entropy_cost = compute_cost(A3, Y) # This gives you the cross-entropy part of the cost</span><br><span class="line">    </span><br><span class="line">    L2_regularization_cost = (np.sum(np.square(W1)) + np.sum(np.square(W2)) + np.sum(np.square(W3))) / m * lambd / 2</span><br><span class="line">    </span><br><span class="line">    cost = cross_entropy_cost + L2_regularization_cost</span><br><span class="line">    </span><br><span class="line">    return cost</span><br></pre></td></tr></table></figure></p><p>损失函数发生改变后，反向传播也随之变化，dW1， dW2 和 dW3 的计算公式是发生变化了的，即要添加一个正则项的导数$\frac{d}{dW} ( \frac{1}{2}\frac{\lambda}{m}  W^2) = \frac{\lambda}{m} W$。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">def backward_propagation_with_regularization(X, Y, cache, lambd):</span><br><span class="line">    m = X.shape[1]</span><br><span class="line">    (Z1, A1, W1, b1, Z2, A2, W2, b2, Z3, A3, W3, b3) = cache</span><br><span class="line">    </span><br><span class="line">    dZ3 = A3 - Y</span><br><span class="line">    </span><br><span class="line">    dW3 = 1./m * np.dot(dZ3, A2.T) + (lambd / m * W3)</span><br><span class="line">    db3 = 1./m * np.sum(dZ3, axis=1, keepdims = True)</span><br><span class="line">    </span><br><span class="line">    dA2 = np.dot(W3.T, dZ3)</span><br><span class="line">    dZ2 = np.multiply(dA2, np.int64(A2 &gt; 0))</span><br><span class="line">    dW2 = 1./m * np.dot(dZ2, A1.T) + (lambd / m * W2)</span><br><span class="line">    db2 = 1./m * np.sum(dZ2, axis=1, keepdims = True)</span><br><span class="line">    </span><br><span class="line">    dA1 = np.dot(W2.T, dZ2)</span><br><span class="line">    dZ1 = np.multiply(dA1, np.int64(A1 &gt; 0))</span><br><span class="line">    dW1 = 1./m * np.dot(dZ1, X.T) + (lambd / m * W1)</span><br><span class="line">    db1 = 1./m * np.sum(dZ1, axis=1, keepdims = True)</span><br><span class="line">    </span><br><span class="line">    gradients = &#123;&quot;dZ3&quot;: dZ3, &quot;dW3&quot;: dW3, &quot;db3&quot;: db3,&quot;dA2&quot;: dA2,</span><br><span class="line">                 &quot;dZ2&quot;: dZ2, &quot;dW2&quot;: dW2, &quot;db2&quot;: db2, &quot;dA1&quot;: dA1, </span><br><span class="line">                 &quot;dZ1&quot;: dZ1, &quot;dW1&quot;: dW1, &quot;db1&quot;: db1&#125;</span><br><span class="line">    </span><br><span class="line">    return gradients</span><br></pre></td></tr></table></figure><p>L2 正则化是我们的结果更加平滑，但是如果超参数 λ 选取的太大的话，就有可能 “oversmooth”, 从而导致模型具有较大的偏差（bias）。</p><h5 id="Dropout-正则化"><a href="#Dropout-正则化" class="headerlink" title="Dropout 正则化"></a>Dropout 正则化</h5><p>首先实现使用Dropout对应的前向传播函数forward_propagation_with_dropout。我们在第一个和第二个隐藏层使用Dropout，而在输入和输出层不使用。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">def forward_propagation_with_dropout(X, parameters, keep_prob = 0.5):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Implements the forward propagation: LINEAR -&gt; RELU + DROPOUT -&gt; LINEAR -&gt; RELU + DROPOUT -&gt; LINEAR -&gt; SIGMOID.</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    </span><br><span class="line">    np.random.seed(1)</span><br><span class="line">    </span><br><span class="line">    # retrieve parameters</span><br><span class="line">    W1 = parameters[&quot;W1&quot;]</span><br><span class="line">    b1 = parameters[&quot;b1&quot;]</span><br><span class="line">    W2 = parameters[&quot;W2&quot;]</span><br><span class="line">    b2 = parameters[&quot;b2&quot;]</span><br><span class="line">    W3 = parameters[&quot;W3&quot;]</span><br><span class="line">    b3 = parameters[&quot;b3&quot;]</span><br><span class="line">    </span><br><span class="line">    # LINEAR -&gt; RELU -&gt; LINEAR -&gt; RELU -&gt; LINEAR -&gt; SIGMOID</span><br><span class="line">    Z1 = np.dot(W1, X) + b1</span><br><span class="line">    A1 = relu(Z1)</span><br><span class="line"></span><br><span class="line">    D1 = np.random.rand(A1.shape[0], A1.shape[1])     # Step 1: initialize matrix D1 = np.random.rand(..., ...)</span><br><span class="line">    D1 = (D1 &lt; keep_prob)                             # Step 2: convert entries of D1 to 0 or 1 (using keep_prob as the threshold)</span><br><span class="line">    A1 = np.multiply(A1, D1)                          # Step 3: shut down some neurons of A1, you can think of  D[1] as a mask,</span><br><span class="line">    A1 /= keep_prob                                   # Step 4: scale the value of neurons that haven&apos;t been shut down</span><br><span class="line"></span><br><span class="line">    Z2 = np.dot(W2, A1) + b2</span><br><span class="line">    A2 = relu(Z2)</span><br><span class="line"> </span><br><span class="line">    D2 = np.random.rand(A2.shape[0], A2.shape[1])     # Step 1: initialize matrix D2 = np.random.rand(..., ...)</span><br><span class="line">    D2 = (D2 &lt; keep_prob)                             # Step 2: convert entries of D2 to 0 or 1 (using keep_prob as the threshold)</span><br><span class="line">    A2 = np.multiply(A2, D2)                          # Step 3: shut down some neurons of A2</span><br><span class="line">    A2 /= keep_prob                                   # Step 4: scale the value of neurons that haven&apos;t been shut down</span><br><span class="line"></span><br><span class="line">    Z3 = np.dot(W3, A2) + b3</span><br><span class="line">    A3 = sigmoid(Z3)</span><br><span class="line">    </span><br><span class="line">    cache = (Z1, D1, A1, W1, b1, Z2, D2, A2, W2, b2, Z3, A3, W3, b3)</span><br><span class="line">    </span><br><span class="line">    return A3, cache</span><br></pre></td></tr></table></figure></p><p>然后实现反向传播，在反向传播中，以dA1为例，要对dA1 也乘以同样的 D[1]，并除以keep_prob。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">def backward_propagation_with_dropout(X, Y, cache, keep_prob):</span><br><span class="line"></span><br><span class="line">    m = X.shape[1]</span><br><span class="line">    (Z1, D1, A1, W1, b1, Z2, D2, A2, W2, b2, Z3, A3, W3, b3) = cache</span><br><span class="line">    </span><br><span class="line">    dZ3 = A3 - Y</span><br><span class="line">    dW3 = 1./m * np.dot(dZ3, A2.T)</span><br><span class="line">    db3 = 1./m * np.sum(dZ3, axis=1, keepdims = True)</span><br><span class="line">    dA2 = np.dot(W3.T, dZ3)</span><br><span class="line"></span><br><span class="line">    dA2 = np.multiply(D2, dA2)             # Step 1: Apply mask D2 to shut down the same neurons as during the forward propagation</span><br><span class="line">    dA2 /= keep_prob                       # Step 2: Scale the value of neurons that haven&apos;t been shut down</span><br><span class="line"></span><br><span class="line">    dZ2 = np.multiply(dA2, np.int64(A2 &gt; 0))</span><br><span class="line">    dW2 = 1./m * np.dot(dZ2, A1.T)</span><br><span class="line">    db2 = 1./m * np.sum(dZ2, axis=1, keepdims = True)</span><br><span class="line">    </span><br><span class="line">    dA1 = np.dot(W2.T, dZ2)</span><br><span class="line"></span><br><span class="line">    dA1 = np.multiply(D1, dA1)             # Step 1: Apply mask D1 to shut down the same neurons as during the forward propagation</span><br><span class="line">    dA1 /= keep_prob                       # Step 2: Scale the value of neurons that haven&apos;t been shut down</span><br><span class="line"></span><br><span class="line">    dZ1 = np.multiply(dA1, np.int64(A1 &gt; 0))</span><br><span class="line">    dW1 = 1./m * np.dot(dZ1, X.T)</span><br><span class="line">    db1 = 1./m * np.sum(dZ1, axis=1, keepdims = True)</span><br><span class="line">    </span><br><span class="line">    gradients = &#123;&quot;dZ3&quot;: dZ3, &quot;dW3&quot;: dW3, &quot;db3&quot;: db3,&quot;dA2&quot;: dA2,</span><br><span class="line">                 &quot;dZ2&quot;: dZ2, &quot;dW2&quot;: dW2, &quot;db2&quot;: db2, &quot;dA1&quot;: dA1, </span><br><span class="line">                 &quot;dZ1&quot;: dZ1, &quot;dW1&quot;: dW1, &quot;db1&quot;: db1&#125;</span><br><span class="line">    </span><br><span class="line">    return gradients</span><br></pre></td></tr></table></figure><p>一个常见的误区就是同时在训练和测试中都使用Dropout，实际上，只能在训练过程中使用Dropout。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;np.square(Wl) 计算各元素的平方&lt;br&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;
      
    
    </summary>
    
      <category term="深度学习习题" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B9%A0%E9%A2%98/"/>
    
    
      <category term="DeepLearning" scheme="http://yoursite.com/tags/DeepLearning/"/>
    
      <category term="习题" scheme="http://yoursite.com/tags/%E4%B9%A0%E9%A2%98/"/>
    
  </entry>
  
  <entry>
    <title>2-1 Initialization</title>
    <link href="http://yoursite.com/2018/09/18/2-1-Initialization/"/>
    <id>http://yoursite.com/2018/09/18/2-1-Initialization/</id>
    <published>2018-09-18T06:41:03.000Z</published>
    <updated>2018-09-28T02:50:55.576Z</updated>
    
    <content type="html"><![CDATA[<h4 id="算法步骤"><a href="#算法步骤" class="headerlink" title="算法步骤"></a>算法步骤</h4><p>我们以一个三层神经网络为例，介绍三种不同的超参数初始化方法对模型的影响，分别是：</p><ul><li>Zeros initialization – setting initialization = “zeros” in the input argument.</li><li>Random initialization – setting initialization = “random” in the input argument. This initializes the weights to large random values.</li><li>He initialization – setting initialization = “he” in the input argument. This initializes the weights to random values scaled according to a paper by He et al., 2015.</li></ul><p>下面首先是我们的三层网络模型，通过传入不同的 initialization 值来调用不同的初始化方法。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">def model(X, Y, learning_rate = 0.01, num_iterations = 15000, print_cost = True, initialization = &quot;he&quot;):</span><br><span class="line"></span><br><span class="line">    grads = &#123;&#125;</span><br><span class="line">    costs = [] # to keep track of the loss</span><br><span class="line">    m = X.shape[1] # number of examples</span><br><span class="line">    layers_dims = [X.shape[0], 10, 5, 1]</span><br><span class="line">    </span><br><span class="line">    # Initialize parameters dictionary.</span><br><span class="line">    if initialization == &quot;zeros&quot;:</span><br><span class="line">        parameters = initialize_parameters_zeros(layers_dims)</span><br><span class="line">    elif initialization == &quot;random&quot;:</span><br><span class="line">        parameters = initialize_parameters_random(layers_dims)</span><br><span class="line">    elif initialization == &quot;he&quot;:</span><br><span class="line">        parameters = initialize_parameters_he(layers_dims)</span><br><span class="line"></span><br><span class="line">    # Loop (gradient descent)</span><br><span class="line">    for i in range(0, num_iterations):</span><br><span class="line"></span><br><span class="line">        # Forward propagation: LINEAR -&gt; RELU -&gt; LINEAR -&gt; RELU -&gt; LINEAR -&gt; SIGMOID.</span><br><span class="line">        a3, cache = forward_propagation(X, parameters)</span><br><span class="line">        </span><br><span class="line">        # Loss</span><br><span class="line">        cost = compute_loss(a3, Y)</span><br><span class="line"></span><br><span class="line">        # Backward propagation.</span><br><span class="line">        grads = backward_propagation(X, Y, cache)</span><br><span class="line">        </span><br><span class="line">        # Update parameters.</span><br><span class="line">        parameters = update_parameters(parameters, grads, learning_rate)</span><br><span class="line">        </span><br><span class="line">        # Print the loss every 1000 iterations</span><br><span class="line">        if print_cost and i % 1000 == 0:</span><br><span class="line">            print(&quot;Cost after iteration &#123;&#125;: &#123;&#125;&quot;.format(i, cost))</span><br><span class="line">            costs.append(cost)</span><br><span class="line">            </span><br><span class="line">    # plot the loss</span><br><span class="line">    plt.plot(costs)</span><br><span class="line">    plt.ylabel(&apos;cost&apos;)</span><br><span class="line">    plt.xlabel(&apos;iterations (per hundreds)&apos;)</span><br><span class="line">    plt.title(&quot;Learning rate =&quot; + str(learning_rate))</span><br><span class="line">    plt.show()</span><br><span class="line">    </span><br><span class="line">    return parameters</span><br></pre></td></tr></table></figure><p>下面对这三种初始化方法进行讨论。<br>对于 Zeros initialization ，表现非常糟糕，损失几乎在训练过程中不下降，表现不比随机猜测要好。这是为什么呢？因为将超参数初始化为0，由于对称性，导致在训练过程中每层的神经元都在做相同的计算，这样相当于每层只有一个神经元，从而神经网络的性能大大降低。因此W[l]需要通过初始化打破这种对称性（break symmetry）。<br>对于 Random initialization ，虽然进行了初始化，打破了上面所说的对称性，但是初始化的值都较大的话，会使得初始的损失较大，这是因为当超参数的值变大后，最后一层中 sigmoid 的输出对于一些样本会更加接近于 0 或 1 ，当样本预测错误后对于损失函数，例如  log(a[3])=log(0)log⁡(a[3])=log⁡(0) ，损失就变成了无穷大。这种情况减缓了神经网络的收敛。<br>对于 He initialization ， 对超参数随机初始化后，乘以一个缩放因子 sqrt(2./layers_dims[l-1]).) ，适合使用于带有 RELU 激活函数的层。其实现如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">def initialize_parameters_he(layers_dims):</span><br><span class="line">    </span><br><span class="line">    np.random.seed(3)</span><br><span class="line">    parameters = &#123;&#125;</span><br><span class="line">    L = len(layers_dims) - 1 # integer representing the number of layers</span><br><span class="line">     </span><br><span class="line">    for l in range(1, L + 1):</span><br><span class="line">        parameters[&quot;W&quot; + str(l)] = np.random.randn(layers_dims[l], layers_dims[l-1]) * np.sqrt(2.0 / layers_dims[l-1])</span><br><span class="line">        parameters[&quot;b&quot; + str(l)] = np.zeros((layers_dims[l], 1))</span><br><span class="line">        </span><br><span class="line">    return parameters</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h4 id=&quot;算法步骤&quot;&gt;&lt;a href=&quot;#算法步骤&quot; class=&quot;headerlink&quot; title=&quot;算法步骤&quot;&gt;&lt;/a&gt;算法步骤&lt;/h4&gt;&lt;p&gt;我们以一个三层神经网络为例，介绍三种不同的超参数初始化方法对模型的影响，分别是：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Zeros in
      
    
    </summary>
    
      <category term="深度学习习题" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B9%A0%E9%A2%98/"/>
    
    
      <category term="DeepLearning" scheme="http://yoursite.com/tags/DeepLearning/"/>
    
      <category term="习题" scheme="http://yoursite.com/tags/%E4%B9%A0%E9%A2%98/"/>
    
  </entry>
  
  <entry>
    <title>1-5 Deep Neural Network for Image Classification: Application</title>
    <link href="http://yoursite.com/2018/09/17/1-5%20Deep-Neural-Network-for-Image-Classification-Application/"/>
    <id>http://yoursite.com/2018/09/17/1-5 Deep-Neural-Network-for-Image-Classification-Application/</id>
    <published>2018-09-17T06:00:11.000Z</published>
    <updated>2018-09-28T02:50:55.576Z</updated>
    
    <content type="html"><![CDATA[<p>将4 Building your Deep Neural Network: Step by Step中实现的神经网络应用到图像识别中，这里的任务是识别一张图中是否有猫。</p><h5 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h5><p>数据的一些信息：<br>Number of training examples: 209<br>Number of testing examples: 50<br>Each image is of size: (64, 64, 3)<br>train_x_orig shape: (209, 64, 64, 3)<br>train_y shape: (1, 209)<br>test_x_orig shape: (50, 64, 64, 3)<br>test_y shape: (1, 50)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">train_x_orig, train_y, test_x_orig, test_y, classes = load_data()</span><br><span class="line"></span><br><span class="line">m_train = train_x_orig.shape[0] # 训练集中图片的数量</span><br><span class="line">num_px = train_x_orig.shape[1] # 图片的维度</span><br><span class="line">m_test = test_x_orig.shape[0] #测试集中图片的数量</span><br><span class="line"></span><br><span class="line"># Reshape the training and test examples </span><br><span class="line">train_x_flatten = train_x_orig.reshape(train_x_orig.shape[0], -1).T   # The &quot;-1&quot; makes reshape flatten the remaining dimensions</span><br><span class="line">test_x_flatten = test_x_orig.reshape(test_x_orig.shape[0], -1).T</span><br><span class="line"></span><br><span class="line"># Standardize data to have feature values between 0 and 1.</span><br><span class="line">train_x = train_x_flatten/255.</span><br><span class="line">test_x = test_x_flatten/255.</span><br></pre></td></tr></table></figure><p>预处理后：<br>train_x’s shape: (12288, 209)<br>test_x’s shape: (12288, 50)</p><h5 id="L层的神经网络"><a href="#L层的神经网络" class="headerlink" title="L层的神经网络"></a>L层的神经网络</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">layers_dims = [12288, 20, 7, 5, 1] #  5-layer model</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">def L_layer_model(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 3000, print_cost=False):#lr was 0.009</span><br><span class="line"></span><br><span class="line">    costs = []                         # keep track of cost</span><br><span class="line">    </span><br><span class="line">    # Parameters initialization.</span><br><span class="line">    parameters = initialize_parameters_deep(layers_dims)</span><br><span class="line"></span><br><span class="line">    # Loop (gradient descent)</span><br><span class="line">    for i in range(0, num_iterations):</span><br><span class="line">        # Forward propagation: [LINEAR -&gt; RELU]*(L-1) -&gt; LINEAR -&gt; SIGMOID.</span><br><span class="line">        AL, caches = L_model_forward(X, parameters)</span><br><span class="line">        </span><br><span class="line">        # Compute cost.</span><br><span class="line">        cost = compute_cost(AL, Y)</span><br><span class="line">    </span><br><span class="line">        # Backward propagation.</span><br><span class="line">        grads = L_model_backward(AL, Y, caches)</span><br><span class="line"> </span><br><span class="line">        # Update parameters.</span><br><span class="line">        parameters = update_parameters(parameters, grads, learning_rate)</span><br><span class="line">                </span><br><span class="line">        # Print the cost every 100 training example</span><br><span class="line">        if print_cost and i % 100 == 0:</span><br><span class="line">            print (&quot;Cost after iteration %i: %f&quot; %(i, cost))</span><br><span class="line">        if print_cost and i % 100 == 0:</span><br><span class="line">            costs.append(cost)</span><br><span class="line">            </span><br><span class="line">    # plot the cost</span><br><span class="line">    plt.plot(np.squeeze(costs))</span><br><span class="line">    plt.ylabel(&apos;cost&apos;)</span><br><span class="line">    plt.xlabel(&apos;iterations (per tens)&apos;)</span><br><span class="line">    plt.title(&quot;Learning rate =&quot; + str(learning_rate))</span><br><span class="line">    plt.show()</span><br><span class="line">    </span><br><span class="line">    return parameters</span><br></pre></td></tr></table></figure><p>如何进行调用上面的模型呢？<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">parameters = L_layer_model(train_x, train_y, layers_dims, num_iterations = 2500, print_cost = True)</span><br></pre></td></tr></table></figure></p><p>进行预测<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pred_test = predict(test_x, test_y, parameters)</span><br></pre></td></tr></table></figure></p><p>tips:如何找到误分类的图片呢？<br>将预测结果pred_test和test_y相加，如果等于1的位置的对应图片就是误分类的图片。</p><table><thead><tr><th style="text-align:center">pred_test</th><th style="text-align:center">test_y</th><th style="text-align:center">pred_test+test_y</th><th style="text-align:center">结果</th></tr></thead><tbody><tr><td style="text-align:center">0</td><td style="text-align:center">0</td><td style="text-align:center">0</td><td style="text-align:center">分类正确</td></tr><tr><td style="text-align:center">0</td><td style="text-align:center">1</td><td style="text-align:center">1</td><td style="text-align:center">误分类</td></tr><tr><td style="text-align:center">1</td><td style="text-align:center">0</td><td style="text-align:center">1</td><td style="text-align:center">误分类</td></tr><tr><td style="text-align:center">1</td><td style="text-align:center">1</td><td style="text-align:center">2</td><td style="text-align:center">分类正确</td></tr></tbody></table><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">a = pred_test + test_y</span><br><span class="line">mislabeled_indices = np.asarray(np.where(a == 1))</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;将4 Building your Deep Neural Network: Step by Step中实现的神经网络应用到图像识别中，这里的任务是识别一张图中是否有猫。&lt;/p&gt;
&lt;h5 id=&quot;数据预处理&quot;&gt;&lt;a href=&quot;#数据预处理&quot; class=&quot;headerlin
      
    
    </summary>
    
      <category term="深度学习习题" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B9%A0%E9%A2%98/"/>
    
    
      <category term="DeepLearning" scheme="http://yoursite.com/tags/DeepLearning/"/>
    
      <category term="习题" scheme="http://yoursite.com/tags/%E4%B9%A0%E9%A2%98/"/>
    
  </entry>
  
  <entry>
    <title>1-4 Building your Deep Neural Network: Step by Step</title>
    <link href="http://yoursite.com/2018/09/16/1-4%20Building-your-Deep-Neural-Network-Step-by-Step/"/>
    <id>http://yoursite.com/2018/09/16/1-4 Building-your-Deep-Neural-Network-Step-by-Step/</id>
    <published>2018-09-16T06:55:23.000Z</published>
    <updated>2018-09-28T02:50:55.576Z</updated>
    
    <content type="html"><![CDATA[<p>注意np.zeros(shape)中的shape是有括号的形式。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">W1 = np.random.randn(n_h,n_x)*0.01</span><br><span class="line">b1 = np.zeros((n_h, 1))</span><br></pre></td></tr></table></figure><p>python中，//为整除符号，取整数部分。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; 1/2</span><br><span class="line">0.5</span><br><span class="line">&gt;&gt;&gt; 1//2</span><br><span class="line">0</span><br></pre></td></tr></table></figure><h4 id="算法步骤"><a href="#算法步骤" class="headerlink" title="算法步骤"></a>算法步骤</h4><ol><li>初始化参数<br>我们使用layer_dims来存储每一层单元的数量。 如layer_dims = [2,4,1] : 第一个是输入层有两个单元，第二个是隐藏层有4个单元，第三个是输出层有1个单元，因此W1大小为 (4,2)， b1 为 (4,1)，W2 为 (1,4) ， b2 为 (1,1)。 </li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">def initialize_parameters_deep(layer_dims):</span><br><span class="line">    </span><br><span class="line">    np.random.seed(3)</span><br><span class="line">    parameters = &#123;&#125;</span><br><span class="line">    L = len(layer_dims)            # number of layers in the network，包括输入层，隐藏层和输出层总共L-1个</span><br><span class="line"></span><br><span class="line">    for l in range(1, L):</span><br><span class="line">        #Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])</span><br><span class="line">        parameters[&quot;W&quot; + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) * 0.01</span><br><span class="line">        #bl -- bias vector of shape (layer_dims[l], 1)</span><br><span class="line">        parameters[&quot;b&quot; + str(l)] = np.zeros((layer_dims[l], 1))</span><br><span class="line">        </span><br><span class="line">    return parameters</span><br></pre></td></tr></table></figure><ol start="2"><li>前向传播模块<br>首先实现线性前向传播</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">def linear_forward(A, W, b):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Arguments:</span><br><span class="line">    A -- activations from previous layer (or input data): (size of previous layer, number of examples)</span><br><span class="line">    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)</span><br><span class="line">    b -- bias vector, numpy array of shape (size of the current layer, 1)</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    </span><br><span class="line">    Z = np.dot(W, A) + b</span><br><span class="line"></span><br><span class="line">    cache = (A, W, b)</span><br><span class="line">    </span><br><span class="line">    return Z, cache</span><br></pre></td></tr></table></figure><p>加上激活函数</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">def linear_activation_forward(A_prev, W, b, activation):</span><br><span class="line"></span><br><span class="line">    if activation == &quot;sigmoid&quot;:</span><br><span class="line">        # Inputs: &quot;A_prev, W, b&quot;. Outputs: &quot;A, activation_cache&quot;.</span><br><span class="line">        Z, linear_cache = linear_forward(A_prev, W, b)</span><br><span class="line">        A, activation_cache = sigmoid(Z)</span><br><span class="line">    </span><br><span class="line">    elif activation == &quot;relu&quot;:</span><br><span class="line">        # Inputs: &quot;A_prev, W, b&quot;. Outputs: &quot;A, activation_cache&quot;.</span><br><span class="line">        Z, linear_cache = linear_forward(A_prev, W, b)</span><br><span class="line">        A, activation_cache = relu(Z)</span><br><span class="line">    </span><br><span class="line">    #linear_cache = (A_prev, W, b), activation_cache = Z</span><br><span class="line">    cache = (linear_cache, activation_cache)</span><br><span class="line"></span><br><span class="line">    return A, cache</span><br></pre></td></tr></table></figure><p>实现 L 层神经网络，堆叠使用 RELU 的 linear_activation_forward 函数 L−1 次, 最后堆叠一个使用 SIGMOID 的 linear_activation_forward 函数。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">def L_model_forward(X, parameters):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Arguments:</span><br><span class="line">    X -- data, numpy array of shape (input size, number of examples)</span><br><span class="line">    parameters -- output of initialize_parameters_deep()</span><br><span class="line">    </span><br><span class="line">    Returns:</span><br><span class="line">    AL -- last post-activation value</span><br><span class="line">    caches -- list of caches containing:</span><br><span class="line">                every cache of linear_relu_forward() (there are L-1 of them, indexed from 0 to L-2)</span><br><span class="line">                the cache of linear_sigmoid_forward() (there is one, indexed L-1)</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">    caches = []</span><br><span class="line">    A = X</span><br><span class="line">    L = len(parameters) // 2                  # number of layers in the neural network，不包括输入层，仅包括隐藏层和输出层。</span><br><span class="line"></span><br><span class="line">    # Implement [LINEAR -&gt; RELU]*(L-1). Add &quot;cache&quot; to the &quot;caches&quot; list.</span><br><span class="line">    # cache = (linear_cache, activation_cache) = ((A_prev, W, b), Z)</span><br><span class="line">    for l in range(1, L):</span><br><span class="line">        A_prev = A </span><br><span class="line">        A, cache = linear_activation_forward(A_prev, parameters[&quot;W&quot; + str(l)], parameters[&quot;b&quot; + str(l)], activation = &quot;relu&quot;)</span><br><span class="line">        caches.append(cache)</span><br><span class="line">    </span><br><span class="line">    # Implement LINEAR -&gt; SIGMOID. Add &quot;cache&quot; to the &quot;caches&quot; list.</span><br><span class="line">    AL, cache = linear_activation_forward(A, parameters[&quot;W&quot; + str(L)], parameters[&quot;b&quot; + str(L)], activation = &quot;sigmoid&quot;)</span><br><span class="line">    caches.append(cache)</span><br><span class="line">     </span><br><span class="line">    return AL, caches</span><br></pre></td></tr></table></figure><ol start="3"><li>计算损失函数</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">def compute_cost(AL, Y):</span><br><span class="line">   </span><br><span class="line">    m = Y.shape[1]</span><br><span class="line">    # Compute loss from aL and y.</span><br><span class="line">    cost = - np.sum(np.multiply(Y, np.log(AL)) + np.multiply(1 - Y, np.log(1 - AL))) / m </span><br><span class="line">    cost = np.squeeze(cost)      # To make sure your cost&apos;s shape is what we expect (e.g. this turns [[17]] into 17).</span><br><span class="line">    </span><br><span class="line">    return cost</span><br></pre></td></tr></table></figure><ol start="4"><li>反向传播模块<br>对于层$l$的线性部分 $Z^{[l]} = W^{[l]} A^{[l-1]} + b^{[l]}$。<br>假设已经知道 $dZ^{[l]} = \frac{\partial \mathcal{L} }{\partial Z^{[l]}}$，计算：<br>$$ dW^{[l]} = \frac{\partial \mathcal{L} }{\partial W^{[l]}} = \frac{1}{m} dZ^{[l]} A^{[l-1] T} $$</li></ol><p>$$ db^{[l]} = \frac{\partial \mathcal{L} }{\partial b^{[l]}} = \frac{1}{m} \sum_{i = 1}^{m} dZ^{[l][i]}$$</p><p>$$ dA^{[l-1]} = \frac{\partial \mathcal{L} }{\partial A^{[l-1]}} = W^{[l] T} dZ^{[l]} $$</p><p>其中，$dZ^{[l]}: (n^{[l]},m)$、$A^{[l-1]}: (n^{[l-1]},m)$、$W^{[l]}: (n^{[l]},n^{[l-1]})$。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">def linear_backward(dZ, cache):</span><br><span class="line"></span><br><span class="line">    A_prev, W, b = cache</span><br><span class="line">    m = A_prev.shape[1]</span><br><span class="line">    </span><br><span class="line">    dA_prev = np.dot(W.T, dZ)</span><br><span class="line">    dW = np.dot(dZ, A_prev.T) / m</span><br><span class="line">    db = np.sum(dZ, axis = 1, keepdims = True) / m</span><br><span class="line"></span><br><span class="line">    return dA_prev, dW, db</span><br></pre></td></tr></table></figure><p>下面求$dZ^{[l]}$，对于激活函数部分<br>$$dZ^{[l]} = dA^{[l]} * g’(Z^{[l]})$$</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">def linear_activation_backward(dA, cache, activation):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Arguments:</span><br><span class="line">    dA -- post-activation gradient for current layer l </span><br><span class="line"></span><br><span class="line">    Returns:</span><br><span class="line">    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    # cache = (linear_cache, activation_cache) = ((A_prev, W, b), Z)</span><br><span class="line">    linear_cache, activation_cache = cache</span><br><span class="line">    </span><br><span class="line">    if activation == &quot;relu&quot;:</span><br><span class="line">        dZ = relu_backward(dA, activation_cache)</span><br><span class="line">        dA_prev, dW, db = linear_backward(dZ, linear_cache)</span><br><span class="line">        </span><br><span class="line">    elif activation == &quot;sigmoid&quot;:</span><br><span class="line">        dZ = sigmoid_backward(dA, activation_cache)</span><br><span class="line">        dA_prev, dW, db = linear_backward(dZ, linear_cache)</span><br><span class="line">    </span><br><span class="line">    return dA_prev, dW, db</span><br></pre></td></tr></table></figure><p>堆叠L层反向传播。首先初始化反向传播，我们知道在第 L 层，$A^{[L]} = \sigma(Z^{[L]})$，要计算$Z^{[L]}$关于激活函数的导数，首先要计算出$A^{[l]}$关于损失函数的导数$ dA^{[L]} = \frac{\partial \mathcal{L}}{\partial A^{[L]}}$ ，以作为def linear_activation_backward(dA, cache, activation)的初值，之后的$ dA^{[L-1]} \dots dA^{[1]}$均可由此函数递推出来。<br>$ dA^{[L]} $计算方法如下:<br><code>dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">def L_model_backward(AL, Y, caches):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    caches -- list of caches containing:</span><br><span class="line">                every cache of linear_activation_forward() with &quot;relu&quot; (there are (L-1) or them, indexes from 0 to L-2)</span><br><span class="line">                the cache of linear_activation_forward() with &quot;sigmoid&quot; (there is one, index L-1)</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">    grads = &#123;&#125;</span><br><span class="line">    L = len(caches) # the number of layers</span><br><span class="line">    m = AL.shape[1]</span><br><span class="line">    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL</span><br><span class="line"></span><br><span class="line">    # Initializing the backpropagation</span><br><span class="line">    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))</span><br><span class="line">    </span><br><span class="line">    # Lth layer (SIGMOID -&gt; LINEAR) gradients. Inputs: &quot;AL, Y, caches&quot;. Outputs: &quot;grads[&quot;dAL&quot;], grads[&quot;dWL&quot;], grads[&quot;dbL&quot;]</span><br><span class="line">    grads[&quot;dA&quot; + str(L)], grads[&quot;dW&quot; + str(L)], grads[&quot;db&quot; + str(L)] = linear_activation_backward(dAL, caches[L-1], activation = &quot;sigmoid&quot;)</span><br><span class="line">    </span><br><span class="line">    for l in reversed(range(L - 1)):</span><br><span class="line">        # lth layer: (RELU -&gt; LINEAR) gradients.</span><br><span class="line">        # Inputs: &quot;grads[&quot;dA&quot; + str(l + 2)], caches&quot;. Outputs: &quot;grads[&quot;dA&quot; + str(l + 1)] , grads[&quot;dW&quot; + str(l + 1)] , grads[&quot;db&quot; + str(l + 1)] </span><br><span class="line">        grads[&quot;dA&quot; + str(l + 1)] , grads[&quot;dW&quot; + str(l + 1)] , grads[&quot;db&quot; + str(l + 1)] = linear_activation_backward(grads[&quot;dA&quot; + str(l + 2)], caches[l], activation = &quot;relu&quot;)</span><br><span class="line"></span><br><span class="line">    return grads</span><br></pre></td></tr></table></figure><ol start="5"><li>更新参数</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">def update_parameters(parameters, grads, learning_rate):</span><br><span class="line">   </span><br><span class="line">    L = len(parameters) // 2 # number of layers in the neural network</span><br><span class="line"></span><br><span class="line">    # Update rule for each parameter. Use a for loop.</span><br><span class="line">    for l in range(1, L+1):</span><br><span class="line">        parameters[&quot;W&quot; + str(l)] = parameters[&quot;W&quot; + str(l)] - learning_rate * grads[&quot;dW&quot; + str(l)]</span><br><span class="line">        parameters[&quot;b&quot; + str(l)] = parameters[&quot;b&quot; + str(l)] - learning_rate * grads[&quot;db&quot; + str(l)]</span><br><span class="line">        </span><br><span class="line">    return parameters</span><br></pre></td></tr></table></figure><ol start="6"><li>预测函数<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">def predict(X, y, parameters):</span><br><span class="line"></span><br><span class="line">    m = X.shape[1]</span><br><span class="line">    n = len(parameters) // 2 # number of layers in the neural network</span><br><span class="line">    p = np.zeros((1,m))</span><br><span class="line">    </span><br><span class="line">    # Forward propagation</span><br><span class="line">    probas, caches = L_model_forward(X, parameters)</span><br><span class="line"></span><br><span class="line">    # convert probas to 0/1 predictions</span><br><span class="line">    for i in range(0, probas.shape[1]):</span><br><span class="line">        if probas[0,i] &gt; 0.5:</span><br><span class="line">            p[0,i] = 1</span><br><span class="line">        else:</span><br><span class="line">            p[0,i] = 0</span><br><span class="line">            </span><br><span class="line">    print(&quot;Accuracy: &quot;  + str(np.sum((p == y)/m)))</span><br><span class="line">        </span><br><span class="line">    return p</span><br></pre></td></tr></table></figure></li></ol><h4 id="附录1：SIGMOID和RELU函数实现"><a href="#附录1：SIGMOID和RELU函数实现" class="headerlink" title="附录1：SIGMOID和RELU函数实现"></a>附录1：SIGMOID和RELU函数实现</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">def sigmoid(Z):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Implements the sigmoid activation in numpy</span><br><span class="line">    </span><br><span class="line">    Arguments:</span><br><span class="line">    Z -- numpy array of any shape</span><br><span class="line">    </span><br><span class="line">    Returns:</span><br><span class="line">    A -- output of sigmoid(z), same shape as Z</span><br><span class="line">    cache -- returns Z as well, useful during backpropagation</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    </span><br><span class="line">    A = 1/(1+np.exp(-Z))</span><br><span class="line">    cache = Z</span><br><span class="line">    </span><br><span class="line">    return A, cache</span><br><span class="line"></span><br><span class="line">def relu(Z):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Implement the RELU function.</span><br><span class="line"></span><br><span class="line">    Arguments:</span><br><span class="line">    Z -- Output of the linear layer, of any shape</span><br><span class="line"></span><br><span class="line">    Returns:</span><br><span class="line">    A -- Post-activation parameter, of the same shape as Z</span><br><span class="line">    cache -- a python dictionary containing &quot;A&quot; ; stored for computing the backward pass efficiently</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    </span><br><span class="line">    A = np.maximum(0,Z)</span><br><span class="line">    </span><br><span class="line">    assert(A.shape == Z.shape)</span><br><span class="line">    </span><br><span class="line">    cache = Z </span><br><span class="line">    return A, cache</span><br></pre></td></tr></table></figure><h4 id="附录2：SIGMOID和RELU反向传播函数实现"><a href="#附录2：SIGMOID和RELU反向传播函数实现" class="headerlink" title="附录2：SIGMOID和RELU反向传播函数实现"></a>附录2：SIGMOID和RELU反向传播函数实现</h4><p>relu_backward中dZ[Z &lt;= 0] = 0这一步不是很懂，不应该是大于等于0时导数为1吗？</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">def relu_backward(dA, cache):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Implement the backward propagation for a single RELU unit.</span><br><span class="line"></span><br><span class="line">    Arguments:</span><br><span class="line">    dA -- post-activation gradient, of any shape</span><br><span class="line">    cache -- &apos;Z&apos; where we store for computing backward propagation efficiently</span><br><span class="line"></span><br><span class="line">    Returns:</span><br><span class="line">    dZ -- Gradient of the cost with respect to Z</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    </span><br><span class="line">    Z = cache</span><br><span class="line">    dZ = np.array(dA, copy=True) # just converting dz to a correct object.</span><br><span class="line">    </span><br><span class="line">    # When z &lt;= 0, you should set dz to 0 as well. </span><br><span class="line">    dZ[Z &lt;= 0] = 0</span><br><span class="line">    </span><br><span class="line">    assert (dZ.shape == Z.shape)</span><br><span class="line">    </span><br><span class="line">    return dZ</span><br><span class="line"></span><br><span class="line">def sigmoid_backward(dA, cache):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Implement the backward propagation for a single SIGMOID unit.</span><br><span class="line"></span><br><span class="line">    Arguments:</span><br><span class="line">    dA -- post-activation gradient, of any shape</span><br><span class="line">    cache -- &apos;Z&apos; where we store for computing backward propagation efficiently</span><br><span class="line"></span><br><span class="line">    Returns:</span><br><span class="line">    dZ -- Gradient of the cost with respect to Z</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    </span><br><span class="line">    Z = cache</span><br><span class="line">    </span><br><span class="line">    s = 1/(1+np.exp(-Z))</span><br><span class="line">    dZ = dA * s * (1-s)</span><br><span class="line">    </span><br><span class="line">    assert (dZ.shape == Z.shape)</span><br><span class="line">    </span><br><span class="line">    return dZ</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;注意np.zeros(shape)中的shape是有括号的形式。&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;s
      
    
    </summary>
    
      <category term="深度学习习题" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B9%A0%E9%A2%98/"/>
    
    
      <category term="DeepLearning" scheme="http://yoursite.com/tags/DeepLearning/"/>
    
      <category term="习题" scheme="http://yoursite.com/tags/%E4%B9%A0%E9%A2%98/"/>
    
  </entry>
  
  <entry>
    <title>1-3 Planar data classification with one hidden layer</title>
    <link href="http://yoursite.com/2018/09/06/1-3%20Planar-data-classification-with-one-hidden-layer/"/>
    <id>http://yoursite.com/2018/09/06/1-3 Planar-data-classification-with-one-hidden-layer/</id>
    <published>2018-09-06T07:50:04.000Z</published>
    <updated>2018-09-28T02:50:55.561Z</updated>
    
    <content type="html"><![CDATA[<p>np.meshgrid()<br>从坐标向量返回坐标矩阵，生成网格数据。<br>例如：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">x = np.arange(-2,2)</span><br><span class="line">y = np.arange(0,3)#生成一位数组，其实也就是向量</span><br><span class="line"></span><br><span class="line">x</span><br><span class="line">Out[31]: array([-2, -1,  0,  1])</span><br><span class="line"></span><br><span class="line">y</span><br><span class="line">Out[32]: array([0, 1, 2])</span><br><span class="line"></span><br><span class="line">z,s = np.meshgrid(x,y)#将两个一维数组变为二维矩阵</span><br><span class="line"></span><br><span class="line">z</span><br><span class="line">Out[36]: </span><br><span class="line">array([[-2, -1,  0,  1],</span><br><span class="line">       [-2, -1,  0,  1],</span><br><span class="line">       [-2, -1,  0,  1]])</span><br><span class="line"></span><br><span class="line">s</span><br><span class="line">Out[37]: </span><br><span class="line">array([[0, 0, 0, 0],</span><br><span class="line">       [1, 1, 1, 1],</span><br><span class="line">       [2, 2, 2, 2]])</span><br></pre></td></tr></table></figure></p><p>z 和 s 就构成了一个坐标矩阵 (-2,0),(-1,0),(0,0),(1,0),(-2,1),(-1,1),(0,1) … (2,0),(2,1)。实际上就是一个网格。</p><p>用matplotlib画等高线图，contourf可以填充等高线之间的空隙颜色，呈现出区域的分划状，所以很多分类机器学习模型的可视化常会借助其展现。<br>核心函数是plt.contourf()，输入的参数是x,y对应的网格数据以及此网格对应的高度值。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">import pandas as pd</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line"> </span><br><span class="line"># 计算x,y坐标对应的高度值</span><br><span class="line">def f(x, y):</span><br><span class="line"> return (1-x/2+x**5+y**3) * np.exp(-x**2-y**2)</span><br><span class="line"> </span><br><span class="line"># 生成x,y的数据</span><br><span class="line">n = 256</span><br><span class="line">x = np.linspace(-3, 3, n)</span><br><span class="line">y = np.linspace(-3, 3, n)</span><br><span class="line"> </span><br><span class="line"># 把x,y数据生成mesh网格状的数据，因为等高线的显示是在网格的基础上添加上高度值</span><br><span class="line">X, Y = np.meshgrid(x, y)</span><br><span class="line"> </span><br><span class="line"># 填充等高线</span><br><span class="line">plt.contourf(X, Y, f(X, Y))</span><br><span class="line"># 显示图表</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></p><p>plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral)<br>cmap = plt.cm.Spectral 实现的功能是给label为1的点一种颜色，给label为0的点另一种颜色。</p><p>tips:<br>计算正确率的向量方法，np.dot()进行矩阵乘法，如果相同，则乘积和为1，反之为0。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">float((np.dot(Y,LR_predictions) + np.dot(1-Y,1-LR_predictions))/float(Y.size)*100)</span><br></pre></td></tr></table></figure><p>有时为保证能重现结果，设置相同的seed，每次生成的随机数相同。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">np.random.seed(2) # we set up a seed so that your output matches ours although the initialization is random.</span><br></pre></td></tr></table></figure><h4 id="公式"><a href="#公式" class="headerlink" title="公式"></a>公式</h4><p>对于 $x^{(i)}$:<br>$$z^{[1] (i)} =  W^{[1]} x^{(i)} + b^{[1] (i)}$$<br>$$a^{[1] (i)} = \tanh(z^{[1] (i)})$$<br>$$z^{[2] (i)} = W^{[2]} a^{[1] (i)} + b^{[2] (i)}$$<br>$$\hat{y}^{(i)} = a^{[2] (i)} = \sigma(z^{ [2] (i)})$$</p><p>$$y^{(i)}_{prediction} =<br>\begin{cases}<br>1&amp; \hat{y}^{(i)} &gt; 0.5 \\<br>0&amp; \text{otherwise}<br>\end{cases}$$</p><p>使用下面公式计算损失 $J$ :<br>$$J = - \frac{1}{m} \sum\limits_{i = 0}^{m} \large\left(\small y^{(i)}\log\left(a^{[2] (i)}\right) + (1-y^{(i)})\log\left(1- a^{[2] (i)}\right)  \large  \right) \small $$</p><h4 id="神经网络结构"><a href="#神经网络结构" class="headerlink" title="神经网络结构"></a>神经网络结构</h4><p><img src="/2018/09/06/1-3 Planar-data-classification-with-one-hidden-layer/classification_kiank.png" alt=""></p><h4 id="算法步骤"><a href="#算法步骤" class="headerlink" title="算法步骤"></a>算法步骤</h4><ol><li>定义神经网络结构：输入单元数量、隐藏单元数量、输出单元数量。</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">def layer_sizes(X, Y):</span><br><span class="line">    # X:(input size, number of examples) Y:(output size, number of examples)</span><br><span class="line">    n_x = X.shape[0] # the size of the input layer</span><br><span class="line">    n_h = 4 # the size of the hidden layer</span><br><span class="line">    n_y = Y.shape[0] #the size of the output layer</span><br><span class="line">    return (n_x, n_h, n_y)</span><br></pre></td></tr></table></figure><ol start="2"><li>初始化模型参数</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">def initialize_parameters(n_x, n_h, n_y):   </span><br><span class="line">    W1 = np.random.randn(n_h, n_x) * 0.01 # n_x 向量的数量 n_y 向量的维度 (n[l],n[l-1])</span><br><span class="line">    b1 = np.zeros((n_h,1))</span><br><span class="line">    W2 = np.random.randn(n_y, n_h) * 0.01</span><br><span class="line">    b2 = np.zeros((n_y,1))</span><br><span class="line">    parameters = &#123;&quot;W1&quot;: W1,</span><br><span class="line">                  &quot;b1&quot;: b1,</span><br><span class="line">                  &quot;W2&quot;: W2,</span><br><span class="line">                  &quot;b2&quot;: b2&#125;  </span><br><span class="line">    return parameters</span><br></pre></td></tr></table></figure><ol start="3"><li>实现前向传播</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">def forward_propagation(X, parameters):</span><br><span class="line">    # Retrieve each parameter from the dictionary &quot;parameters&quot;</span><br><span class="line">    W1 = parameters[&quot;W1&quot;]</span><br><span class="line">    b1 = parameters[&quot;b1&quot;]</span><br><span class="line">    W2 = parameters[&quot;W2&quot;]</span><br><span class="line">    b2 = parameters[&quot;b2&quot;]</span><br><span class="line">    </span><br><span class="line">    # Implement Forward Propagation to calculate A2 (probabilities)</span><br><span class="line">    Z1 = np.dot(W1,X) + b1</span><br><span class="line">    A1 = np.tanh(Z1)</span><br><span class="line">    Z2 = np.dot(W2,A1) + b2</span><br><span class="line">    A2 = sigmoid(Z2)</span><br><span class="line"></span><br><span class="line">    cache = &#123;&quot;Z1&quot;: Z1,</span><br><span class="line">             &quot;A1&quot;: A1,</span><br><span class="line">             &quot;Z2&quot;: Z2,</span><br><span class="line">             &quot;A2&quot;: A2&#125;</span><br><span class="line">    </span><br><span class="line">    return A2, cache</span><br></pre></td></tr></table></figure><ol start="4"><li>计算损失函数</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">def compute_cost(A2, Y, parameters):</span><br><span class="line"></span><br><span class="line">    m = Y.shape[1] # number of example</span><br><span class="line"></span><br><span class="line">    # Compute the cross-entropy cost</span><br><span class="line">    logprobs = np.multiply(np.log(A2),Y) + np.multiply(np.log(1-A2),1-Y)</span><br><span class="line">    cost = -np.sum(logprobs) / m</span><br><span class="line">    </span><br><span class="line">    cost = np.squeeze(cost)     # makes sure cost is the dimension we expect. </span><br><span class="line">                                # E.g., turns [[17]] into 17    </span><br><span class="line">    return cost</span><br></pre></td></tr></table></figure><ol start="5"><li>计算反向传播</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">def backward_propagation(parameters, cache, X, Y):</span><br><span class="line">  </span><br><span class="line">    m = X.shape[1]</span><br><span class="line">    </span><br><span class="line">    # First, retrieve W1 and W2 from the dictionary &quot;parameters&quot;.</span><br><span class="line">    W1 = parameters[&apos;W1&apos;]</span><br><span class="line">    W2 = parameters[&apos;W2&apos;]</span><br><span class="line">    </span><br><span class="line">    # Retrieve also A1 and A2 from dictionary &quot;cache&quot;.</span><br><span class="line">    A1 = cache[&apos;A1&apos;]</span><br><span class="line">    A2 = cache[&apos;A2&apos;]</span><br><span class="line">    </span><br><span class="line">    # Backward propagation: calculate dW1, db1, dW2, db2. </span><br><span class="line">    dZ2 = A2 - Y</span><br><span class="line">    dW2 = np.dot(dZ2, A1.T) / m</span><br><span class="line">    db2 = np.sum(dZ2,axis=1,keepdims=True) / m</span><br><span class="line">    dZ1 = np.dot(W2.T, dZ2) * (1 - np.power(A1, 2))</span><br><span class="line">    #dZ1 = np.dot(W2.T, dZ2) * np.maximum(0,A1)</span><br><span class="line">    dW1 = np.dot(dZ1, X.T) / m</span><br><span class="line">    db1 = np.sum(dZ1,axis=1,keepdims=True) / m</span><br><span class="line">    </span><br><span class="line">    grads = &#123;&quot;dW1&quot;: dW1,</span><br><span class="line">             &quot;db1&quot;: db1,</span><br><span class="line">             &quot;dW2&quot;: dW2,</span><br><span class="line">             &quot;db2&quot;: db2&#125;   </span><br><span class="line">    return grads</span><br></pre></td></tr></table></figure><ol start="6"><li>更新参数</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">def update_parameters(parameters, grads, learning_rate = 1.2):</span><br><span class="line">   </span><br><span class="line">    # Retrieve each parameter from the dictionary &quot;parameters&quot;</span><br><span class="line">    W1 = parameters[&apos;W1&apos;]</span><br><span class="line">    W2 = parameters[&apos;W2&apos;]</span><br><span class="line">    b1 = parameters[&apos;b1&apos;]</span><br><span class="line">    b2 = parameters[&apos;b2&apos;]</span><br><span class="line">    </span><br><span class="line">    # Retrieve each gradient from the dictionary &quot;grads&quot;</span><br><span class="line">    dW1 = grads[&apos;dW1&apos;]</span><br><span class="line">    dW2 = grads[&apos;dW2&apos;]</span><br><span class="line">    db1 = grads[&apos;db1&apos;]</span><br><span class="line">    db2 = grads[&apos;db2&apos;] </span><br><span class="line">    </span><br><span class="line">    # Update rule for each parameter</span><br><span class="line">    W1 = W1 - learning_rate*dW1</span><br><span class="line">    W2 = W2 - learning_rate*dW2</span><br><span class="line">    b1 = b1 - learning_rate*db1</span><br><span class="line">    b2 = b2 - learning_rate*db2</span><br><span class="line"> </span><br><span class="line">    parameters = &#123;&quot;W1&quot;: W1,</span><br><span class="line">                  &quot;b1&quot;: b1,</span><br><span class="line">                  &quot;W2&quot;: W2,</span><br><span class="line">                  &quot;b2&quot;: b2&#125;   </span><br><span class="line">    return parameters</span><br></pre></td></tr></table></figure><ol start="7"><li>将上述几步结合，实现最后的模型</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">def nn_model(X, Y, n_h, num_iterations = 10000, print_cost=False):</span><br><span class="line">    n_x = layer_sizes(X, Y)[0]</span><br><span class="line">    n_y = layer_sizes(X, Y)[2]  </span><br><span class="line">    parameters = initialize_parameters(n_x, n_h, n_y)</span><br><span class="line">    W1 = parameters[&apos;W1&apos;]</span><br><span class="line">    W2 = parameters[&apos;W2&apos;]</span><br><span class="line">    b1 = parameters[&apos;b1&apos;]</span><br><span class="line">    b2 = parameters[&apos;b2&apos;]   </span><br><span class="line">    # Loop (gradient descent)</span><br><span class="line">    for i in range(0, num_iterations):      </span><br><span class="line">        # Forward propagation. </span><br><span class="line">        A2, cache = forward_propagation(X, parameters)</span><br><span class="line">        # Cost function.</span><br><span class="line">        cost = compute_cost(A2, Y, parameters)</span><br><span class="line">        # Backpropagation. </span><br><span class="line">        grads = backward_propagation(parameters, cache, X, Y)</span><br><span class="line">        # Gradient descent parameter update.</span><br><span class="line">        parameters = update_parameters(parameters, grads)</span><br><span class="line">        </span><br><span class="line">        # Print the cost every 1000 iterations</span><br><span class="line">        if print_cost and i % 1000 == 0:</span><br><span class="line">            print (&quot;Cost after iteration %i: %f&quot; %(i, cost))</span><br><span class="line">    return parameters</span><br></pre></td></tr></table></figure><ol start="8"><li>实现预测函数</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">def predict(parameters, X):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Using the learned parameters, predicts a class for each example in X</span><br><span class="line">    Arguments:</span><br><span class="line">    parameters -- python dictionary containing your parameters </span><br><span class="line">    X -- input data of size (n_x, m)</span><br><span class="line">    Returns</span><br><span class="line">    predictions -- vector of predictions of our model (red: 0 / blue: 1)</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    # Computes probabilities using forward propagation, and classifies to 0/1 using 0.5 as the threshold.</span><br><span class="line">    y_predictions, _ = forward_propagation(X, parameters)</span><br><span class="line">    predictions = (y_predictions &gt; 0.5) </span><br><span class="line">    return predictions</span><br></pre></td></tr></table></figure><p>那么怎么调用我们实现的这个模型呢？</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">parameters = nn_model(X, Y, n_h, num_iterations = 5000)</span><br><span class="line">plot_decision_boundary(lambda x: predict(parameters, x.T), X, Y[0, :]) # lambda x: predict(parameters, x.T) 将函数作为参数传递，其中，x 为调用的参数。</span><br><span class="line">predictions = predict(parameters, X)</span><br><span class="line">accuracy = float((np.dot(Y,predictions.T) + np.dot(1-Y,1-predictions.T))/float(Y.size)*100)</span><br><span class="line">print (&quot;Accuracy for &#123;&#125; hidden units: &#123;&#125; %&quot;.format(n_h, accuracy))</span><br></pre></td></tr></table></figure><p>其中，plot_decision_boundary是实现可视化的函数，其实现如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">def plot_decision_boundary(model, X, y):</span><br><span class="line">    # Set min and max values and give it some padding</span><br><span class="line">    x_min, x_max = X[0, :].min() - 1, X[0, :].max() + 1</span><br><span class="line">    y_min, y_max = X[1, :].min() - 1, X[1, :].max() + 1</span><br><span class="line">    h = 0.01</span><br><span class="line">    # Generate a grid of points with distance h between them</span><br><span class="line">    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))</span><br><span class="line">    # Predict the function value for the whole grid</span><br><span class="line">    Z = model(np.c_[xx.ravel(), yy.ravel()]) </span><br><span class="line">    Z = Z.reshape(xx.shape)</span><br><span class="line">    # Plot the contour and training examples</span><br><span class="line">    plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral)</span><br><span class="line">    plt.ylabel(&apos;x2&apos;)</span><br><span class="line">    plt.xlabel(&apos;x1&apos;)</span><br><span class="line">    plt.scatter(X[0, :], X[1, :], c=y, cmap=plt.cm.Spectral)</span><br></pre></td></tr></table></figure><p>效果类似下图：</p><p><img src="/2018/09/06/1-3 Planar-data-classification-with-one-hidden-layer/visual.png" alt=""></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;np.meshgrid()&lt;br&gt;从坐标向量返回坐标矩阵，生成网格数据。&lt;br&gt;例如：&lt;br&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/s
      
    
    </summary>
    
      <category term="深度学习习题" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B9%A0%E9%A2%98/"/>
    
    
      <category term="DeepLearning" scheme="http://yoursite.com/tags/DeepLearning/"/>
    
      <category term="习题" scheme="http://yoursite.com/tags/%E4%B9%A0%E9%A2%98/"/>
    
  </entry>
  
  <entry>
    <title>1-2 Logistic Regression with a Neural Network mindset</title>
    <link href="http://yoursite.com/2018/09/04/1-2%20Logistic-Regression-with-a-Neural-Network-mindset/"/>
    <id>http://yoursite.com/2018/09/04/1-2 Logistic-Regression-with-a-Neural-Network-mindset/</id>
    <published>2018-09-04T13:00:54.000Z</published>
    <updated>2018-09-28T02:50:55.561Z</updated>
    
    <content type="html"><![CDATA[<h4 id="算法步骤"><a href="#算法步骤" class="headerlink" title="算法步骤"></a>算法步骤</h4><ol><li><p>首先实现函数initialize_with_zeros(dim)对参数w和b进行初始化</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">w = np.zeros((dim, 1))</span><br><span class="line">b = 0</span><br></pre></td></tr></table></figure></li><li><p>实现前向和后向传播算法 propagate(w, b, X, Y)，在此函数里计算损失cost，并求出w和b的梯度dw和db。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">A = sigmoid( np.dot(w.T, X) + b )           # compute activation</span><br><span class="line">cost = -  np.sum( Y * np.log(A) + (1-Y) * np.log(1-A))  / m   # compute cost</span><br><span class="line"></span><br><span class="line">dw = np.dot(X,(A-Y).T) / m</span><br><span class="line">db = np.sum(A-Y) / m</span><br></pre></td></tr></table></figure></li><li><p>实现优化函数optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False)进行梯度下降。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">costs = []</span><br><span class="line">for i in range(num_iterations):</span><br><span class="line">   # Cost and gradient calculation (≈ 1-4 lines of code)</span><br><span class="line">   grads, cost = propagate(w, b, X, Y)</span><br><span class="line"></span><br><span class="line">   # Retrieve derivatives from grads</span><br><span class="line">   dw = grads[&quot;dw&quot;]</span><br><span class="line">   db = grads[&quot;db&quot;]</span><br><span class="line">       </span><br><span class="line">   # update rule </span><br><span class="line">   w = w - learning_rate*dw</span><br><span class="line">   b = b - learning_rate*db</span><br><span class="line">   if i % 100 == 0:</span><br><span class="line">       costs.append(cost)</span><br></pre></td></tr></table></figure></li></ol><p>最后返回的是 params, grads 和 costs。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">params = &#123;&quot;w&quot;: w,</span><br><span class="line">            &quot;b&quot;: b&#125;</span><br><span class="line">    </span><br><span class="line">grads = &#123;&quot;dw&quot;: dw,</span><br><span class="line">            &quot;db&quot;: db&#125;</span><br></pre></td></tr></table></figure></p><ol start="4"><li><p>实现预测函数 predict(w, b, X)，返回预测结果 Y_prediction。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">A = sigmoid( np.dot(w.T, X) + b )</span><br><span class="line">for i in range(A.shape[1]):</span><br><span class="line">    if A[0][i] &lt;= 0.5:</span><br><span class="line">        Y_prediction[0][i] = 0</span><br><span class="line">    else:</span><br><span class="line">        Y_prediction[0][i] = 1</span><br></pre></td></tr></table></figure></li><li><p>将上述几步结合，实现最后的模型 model(X_train, Y_train, X_test, Y_test, num_iterations = 2000, learning_rate = 0.5, print_cost = False)。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">w, b = initialize_with_zeros(X_train.shape[0])</span><br><span class="line">params, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost = False)</span><br><span class="line">w = params[&apos;w&apos;]</span><br><span class="line">b = params[&apos;b&apos;]</span><br><span class="line">Y_prediction_test = predict(w, b, X_test)</span><br><span class="line">Y_prediction_train = predict(w, b, X_train)</span><br><span class="line">print(&quot;train accuracy: &#123;&#125; %&quot;.format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))</span><br><span class="line">    print(&quot;test accuracy: &#123;&#125; %&quot;.format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))</span><br></pre></td></tr></table></figure></li></ol><h4 id="numpy部分函数"><a href="#numpy部分函数" class="headerlink" title="numpy部分函数"></a>numpy部分函数</h4><p>np.squeeze() 从数组的形状中删除单维条目，即把shape中为1的维度去掉<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">x = np.array([[[0], [1], [2]]])</span><br><span class="line">print(x.shape)  # (1, 3, 1)</span><br><span class="line"></span><br><span class="line">x1 = np.squeeze(x)  # 从数组的形状中删除单维条目，即把shape中为1的维度去掉</span><br><span class="line">print(x1)  # [0 1 2]</span><br><span class="line">print(x1.shape)  # (3,)</span><br></pre></td></tr></table></figure></p><p>将矩阵 X : (a,b,c,d) 变换为矩阵 X_flatten ： (b ∗ c ∗ d, a) 时，使用<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X_flatten = X.reshape(X.shape[0], -1).T      # X.T is the transpose of X，参数为-1时，reshape函数会根据另一个参数的维度计算出数组的另外一个shape属性值。</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h4 id=&quot;算法步骤&quot;&gt;&lt;a href=&quot;#算法步骤&quot; class=&quot;headerlink&quot; title=&quot;算法步骤&quot;&gt;&lt;/a&gt;算法步骤&lt;/h4&gt;&lt;ol&gt;
&lt;li&gt;&lt;p&gt;首先实现函数initialize_with_zeros(dim)对参数w和b进行初始化&lt;/p&gt;
&lt;figu
      
    
    </summary>
    
      <category term="深度学习习题" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B9%A0%E9%A2%98/"/>
    
    
      <category term="DeepLearning" scheme="http://yoursite.com/tags/DeepLearning/"/>
    
      <category term="习题" scheme="http://yoursite.com/tags/%E4%B9%A0%E9%A2%98/"/>
    
  </entry>
  
  <entry>
    <title>1-1 Python Basics with Numpy</title>
    <link href="http://yoursite.com/2018/09/04/1-1%20Python%20Basics%20with%20Numpy/"/>
    <id>http://yoursite.com/2018/09/04/1-1 Python Basics with Numpy/</id>
    <published>2018-09-04T02:45:01.000Z</published>
    <updated>2018-09-28T02:50:55.561Z</updated>
    
    <content type="html"><![CDATA[<hr><p>在iPython Notebooks中按下”SHIFT”+”ENTER”来运行相应的代码块。</p><hr><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line"># example of np.exp</span><br><span class="line">x = np.array([1, 2, 3])</span><br><span class="line">print(np.exp(x)) # result is (exp(1), exp(2), exp(3))</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># example of vector operation</span><br><span class="line">x = np.array([1, 2, 3])</span><br><span class="line">print (x + 3)</span><br><span class="line">print (1.0 / x)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x_sum = np.sum(x_exp, axis = 1, keepdims = True)</span><br></pre></td></tr></table></figure><p> keepdims：是否保持矩阵的二维特性，例如使结果的形状是(4,1)而不是(4,)。</p><hr><p>np.shape 和 np.reshape().</p><p>X.shape 用来获得矩阵或者向量X的形状（维度）.<br>X.reshape(…) 改变X的维度.<br>例如， 将三维的图片向量  (length,height,depth=3) 更改为一维  (length∗height∗3,1)<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">v = image.reshape(image.shape[0]*image.shape[1]*image.shape[2],1)</span><br></pre></td></tr></table></figure></p><hr><p>求范数<br>x_norm=np.linalg.norm(x, ord=None, axis=None, keepdims=False)</p><ul><li>ord：范数类型<br>默认为第二范数，即算数平方根</li><li>axis：处理类型<br>axis=1表示按行向量处理，求多个行向量的范数<br>axis=0表示按列向量处理，求多个列向量的范数<br>axis=None表示矩阵范数。</li><li>keepdims：是否保持矩阵的二维特性</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># 正则化</span><br><span class="line"># Compute x_norm as the norm 2 of x. Use np.linalg.norm(..., ord = 2, axis = ..., keepdims = True)</span><br><span class="line">x_norm = np.linalg.norm(x,axis=1,keepdims=True)</span><br><span class="line"># Divide x by its norm.</span><br><span class="line">x = x / x_norm</span><br></pre></td></tr></table></figure><hr><p>向量化<br>np.dot()：进行矩阵与矩阵，矩阵与向量之间的乘法。对于秩为1的数组x（即一维数组），执行对应位置相乘，然后再相加，np.dot(x)效果等同于np.sum(np.multiply(x))；对于秩不为1的二维数组，执行矩阵乘法运算。<br>np.multiply() 和 <em> 进行的是元素相乘。<br>乘号</em>：对数组执行对应位置相乘，对矩阵执行矩阵乘法运算。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">x1 = [9, 2, 5, 0, 0, 7, 5, 0, 0, 0, 9, 2, 5, 0, 0]</span><br><span class="line">x2 = [9, 2, 2, 9, 0, 9, 2, 5, 0, 0, 9, 2, 5, 0, 0]</span><br><span class="line"></span><br><span class="line">### VECTORIZED DOT PRODUCT OF VECTORS ###</span><br><span class="line">dot = np.dot(x1,x2)</span><br><span class="line"></span><br><span class="line">### VECTORIZED OUTER PRODUCT ###</span><br><span class="line">outer = np.outer(x1,x2)</span><br><span class="line"></span><br><span class="line">### VECTORIZED ELEMENTWISE MULTIPLICATION ###</span><br><span class="line">mul = np.multiply(x1,x2)</span><br><span class="line"></span><br><span class="line">np.multiply(A,B)       #数组对应元素位置相乘，输出为array</span><br><span class="line">(np.mat(A))*(np.mat(B))  #执行矩阵运算，输出为matrix</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;hr&gt;
&lt;p&gt;在iPython Notebooks中按下”SHIFT”+”ENTER”来运行相应的代码块。&lt;/p&gt;
&lt;hr&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span cla
      
    
    </summary>
    
      <category term="深度学习习题" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B9%A0%E9%A2%98/"/>
    
    
      <category term="DeepLearning" scheme="http://yoursite.com/tags/DeepLearning/"/>
    
      <category term="习题" scheme="http://yoursite.com/tags/%E4%B9%A0%E9%A2%98/"/>
    
  </entry>
  
  <entry>
    <title>社会契约论</title>
    <link href="http://yoursite.com/2018/08/31/%E7%A4%BE%E4%BC%9A%E5%A5%91%E7%BA%A6%E8%AE%BA/"/>
    <id>http://yoursite.com/2018/08/31/社会契约论/</id>
    <published>2018-08-31T03:39:20.000Z</published>
    <updated>2018-09-28T02:50:55.576Z</updated>
    
    <content type="html"><![CDATA[<h3 id="第一卷"><a href="#第一卷" class="headerlink" title="第一卷"></a>第一卷</h3><p>社会的人在社会中得到权利的同时也必定意味着要失去一些东西，不能够贪婪的无限制的顺从自己的任何欲望，我们所拥有的自由是被公意所约束着的社会的自由。</p><blockquote><p>人类由于社会契约而丧失的，乃是他的天然的自由以及对于他所企图的和所能得到的一切东西的那种无限的权利；而他所获得的，乃是社会的自由以及对于他所享有的一切东西的所有权。<br>只有嗜欲的冲动便是奴隶状态，而唯有服从人们自己为自己所规定的法律，才是自由。<br>《论社会形态》</p></blockquote><p>最后一句感觉说得很有道理。</p><blockquote><p>基本公约并没有摧毁自然的平等，反而是以道德的与法律的平等来代替自然所造成的人与人之间的身体上的不平等；从而，人们尽可以在力量上和才智上不平等，但是由于约定并且根据权利，他们却是人人平等的。<br>在坏政府的下面，这种平等只是虚有其表；它只能保持穷人处于贫困，保持富人处于占有。事实上，法律总是利于享有财富的人，而有害于一无所有的人；由此可见，唯有当人人都有一些东西而又没有人能有过多的东西的时候，社会状态才会对人类有益。<br>《论财产权》</p></blockquote><h3 id="第二卷"><a href="#第二卷" class="headerlink" title="第二卷"></a>第二卷</h3><p>多数也不一定是正确的，真理也不一定掌握在所谓的多数人手中，只是因为可能所谓的多还不够多，对于全体人民来说，只是一个小集团。而我觉得，当集团无限大，以致包括了全体人民，而集团的意志也就成了公意。</p><blockquote><p>人们总是愿意自己幸福，但人们并不总是能看清楚幸福。人民是绝不会被腐蚀的，但人民却往往会受欺骗，而且唯有在这时候，人民才好像会愿意要不好的东西。<br>众意与公意之间经常总是有很大的差别；公意只着眼于公共的利益，而众意只是个别意志的总和。但是，除掉这些个别意志正负相抵消的部分，剩下的总和仍是公意。<br>如果当人民能够充分了解情况并进行讨论时，公民彼此之间又没有任何勾结，那么从大量的小分歧中总可以产生公意，而且讨论的结果总会是好的。但是当形成了派别的时候，形成了以牺牲大集体为代价的小集团的时候，每一个这种集团的意志对它的成员来说就成为公意，而对国家来说则成为个别意志；这时候我们可以说，投票者的数目已经不再和人数相等，而只与集团的数目相等了。分歧在数量上是减少了，而所得的记过却更缺乏公意。最后，当这些集团中有一个是如此之大，以至于超过了其他一切集团的时候，那么结果你就不再有许多小的分歧的总和，而只有一个唯一的分歧；这时，就不再有公意，而占优势的意见便只不过是一种个别的意见。<br>《公意是否可能错误》</p></blockquote><p>人人都要为代表公意的祖国而战斗，具有公意是不可缺少的前提条件，否则就是变质的且没有任何意义的。</p><blockquote><p>由于契约的结果，他们的处境确实比起他们以前的情况更加可取得多；他们所做的并不是一项割让而是一项有利的交易，也就是以一种更美好的、更稳定的生活方式代替了不可靠的、不安定的生活方式，以自由代替了天然的独立，以自身的安全代替了自己侵害别人的权力，以一种由社会的结合保障其不可战胜的权利，代替了自己有可能被别人所制服的强力。他们所献给国家的个人生命也不断地在受着国家的保护；并且当他们冒生命之险去捍卫国家的时候，这时他们所做的事不也就是把自己得之于国家的东西重新给予国家吗？他们现在所做的事，难道不就是他们在自然状态里，当生活于不可避免的搏斗之中必须冒着生命的危险以保卫自己的生存所需时，他们格外频繁地、格外危险地所必须要做的事情吗？诚然，在必要时，人人都要为祖国而战斗；然而这样也就再没有一个人为自己而战斗了。为了保障我们的安全，只需冒一旦丧失这种安全时我们自身所必须去冒的种种危险中的一部分，这难道还不是收益吗？<br>《论主权权力的界限》</p></blockquote><p>这就是为什么需要法律，为什么要维护公平正义。没有公平正义的存在，“好人有好报，坏人有坏报”也是不可能存在的，反而可能会恰好相反。</p><blockquote><p>然而从人世来考察事物，则缺少了自然的制裁，正义的法则在人间便是虚幻的；当正直的人对一切人都遵守正义的法则，却没有人对他也遵守时，正义的法则就只不过造成了坏人的幸福和正直的人的不幸罢了。因此，就需要有约定和法律来把权利和义务结合在一起，并使正义能符合于它的目的。<br>《论法律》</p></blockquote><p>这里透露出了卢梭权力分立制约的思想。</p><blockquote><p>因为号令人的人如果不应该号令法律的话，那么号令法律的人也就更不应该号令人；否则法律受到他的感情所支配，便只能经常地贯彻他自己的不公正，而他个人的意见之有害于他自己的事业的神圣性，也就只能是永远不可避免。<br>《论立法者》</p></blockquote><p>在《论立法者》中，卢梭认为立法是一项超乎人力之上的事业，执行却又是一种形同无物的权威，人们无法理解为什么要接受良好的法律，从而产生了宗教，借助于申明的权威来要求人们服从国家法，他认为，在各个国家初创时，宗教是用来作为政治的工具的。</p><blockquote><p>太概况的概念与太遥远的目标，都同样地是超乎人们的能力之外的；每一个个人所喜欢的政府计划，不外是与他自己的个别利益有关的计划，他们很难认识到自己可以从良好的法律要求他们所作的不断牺牲之中得到怎样的好处。为了使一个新生的民族能够爱好健全的政治准则并遵循国家利益的根本规律，便必须倒果为因，使本来应该是制度的产物的社会精神转而凌驾于制度本身之上，并且使人们在法律出现之前，便可以成为应该是由于法律才能形成的那种样子。这样，立法者便既不能使用强力，也不能使用说理；因此就有必要求之于另外一种不以暴力而能约束人，不以论证而能说服人的权威了（指宗教）。<br>这种超乎人们的能力之外的崇高的道理，也就是立法者所以要把自己的决定托之于神道设教的道理，为的是好让神圣的权威来约束那些为人类的深思熟虑所无法感动的人们。但是并不是人人都可以代神明立言，也不是当他自称是神明的代言人时，他便能为人们所相信。唯有立法者的伟大的灵魂，才是足以证明自己使命的真正奇迹。人人都可以刻石立碑，或者购买神谕，或者假托通灵，或者训练一只小鸟向人耳边口吐神言，或者寻求其他的卑鄙手段来欺骗人民。只会搞这一套的人，甚至于也偶尔能纠集一群愚民；但是他却绝不会建立起一个帝国，而他那种荒唐的把戏很快地也就会随他本人一起破灭的。虚假的威望只能形成一种过眼云烟的联系，唯有智慧才能使之经久不衰。</p></blockquote><p>应该成为一切立法体系最终的目的全体最大的幸福是自由和平等。</p><blockquote><p>至于平等，这个名词绝不是指权力和财富的程度应当绝对相等；而是说，就权力而言，则它应该不能成为任何暴力，并且只有凭职位与法律才能加以行使；就财富而言，则没有一个公民可以富足得足以购买另一人，也没有一个公民穷得不得不出卖自身。这就要求大人物这一方必须节制财富和权势，而小人物这一方必须节制贪得和婪求。<br>有人说，这种平等是实践中所绝不可能存在的一种思辩虚构。但是，如果滥用权力是不可避免的，是不是因此就应该一点也不去纠正它了呢？恰恰因为事物的力量总是倾向于摧毁平等的，所以立法的力量就应该总是倾向于维持平等。<br>《论各种不同的立法体系》</p></blockquote><h3 id="第三卷"><a href="#第三卷" class="headerlink" title="第三卷"></a>第三卷</h3><blockquote><p>什么是政府呢？政府就是在臣民与主权者之间所建立的一个中间体，以便两者得以互相适合，它负责执行法律并维护社会的以及政治的自由。<br>主权者/政府=政府/国家，即：政府<em>政府=主权者</em>国家，这里将主权者看做一个整体，随着国家的增大（例如，人口数目的增多），就要求政府的力量也要相应地加强。<br>主权者将权力委托给政府，政府只能是由于主权者而存在的。如果行政官居然具有了一种比主权者的意志更为活跃的个别意志，并且他竟然使自己所掌握的公共力量服从于这个个别意志，以至于可以说有了两个主权者，一个是权利上的，而另一个则是事实上的；这时，社会的结合便会立即消灭，而政治题也便会立即解体。<br>困难就在于以什么方式在整体中安排这个附属的整体，从而使它在确定自己的体制时，决不至于变更总的体制，从而使它始终能够区别以保存自身为目的的个别力量和以保存国家为目的的公共力量；从而，一言以蔽之，使它永远准备着为人民而牺牲政府，却不是为政府而牺牲人民。<br>《政府总论》</p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;第一卷&quot;&gt;&lt;a href=&quot;#第一卷&quot; class=&quot;headerlink&quot; title=&quot;第一卷&quot;&gt;&lt;/a&gt;第一卷&lt;/h3&gt;&lt;p&gt;社会的人在社会中得到权利的同时也必定意味着要失去一些东西，不能够贪婪的无限制的顺从自己的任何欲望，我们所拥有的自由是被公意所约束着的
      
    
    </summary>
    
      <category term="文学" scheme="http://yoursite.com/categories/%E6%96%87%E5%AD%A6/"/>
    
    
      <category term="摘抄" scheme="http://yoursite.com/tags/%E6%91%98%E6%8A%84/"/>
    
      <category term="文学" scheme="http://yoursite.com/tags/%E6%96%87%E5%AD%A6/"/>
    
  </entry>
  
  <entry>
    <title>论人类不平等的起源</title>
    <link href="http://yoursite.com/2018/08/28/%E8%AE%BA%E4%BA%BA%E7%B1%BB%E4%B8%8D%E5%B9%B3%E7%AD%89%E7%9A%84%E8%B5%B7%E6%BA%90/"/>
    <id>http://yoursite.com/2018/08/28/论人类不平等的起源/</id>
    <published>2018-08-28T05:05:28.000Z</published>
    <updated>2018-09-28T02:50:55.576Z</updated>
    
    <content type="html"><![CDATA[<p>《论人类不平等的起源》是卢梭的早期作品之一，英文原书名为“A Discourse On Inequality”。在此书中，卢梭揭示出私有制是人类不平等的起源和基础，他认为自然状态的野蛮人是独居的，彼此间没有任何联系，对同类既无所需要，也无加害意图，甚至从来不能辨认他同类中的任何人，因此，他们是平等的。但随着人类独具的自我能力被发挥，人类发明了冶金和农业，土地的耕种最终导致了土地的分配，即私有制的确立。私有制一旦被承认，由于人们才能的差异，逐渐产生了贫困和奴役。富人们为了保护他们的财富，便想借助于那些攻击他们的力量来保护自己，于是他们通过诱骗说服邻人出让权利，从而永久确立了保障私有财产和承认不平等的契约，即法律，建立起了国家政治体制，于是产生了人与人之间在政治上的不平等，完成了人类从自然上的不平等到政治上不平等的过度。<br>在阅读此书的过程中，对书中有些篇章段落比较有感触，遂摘抄如下。</p><h4 id="第一部分"><a href="#第一部分" class="headerlink" title="第一部分"></a>第一部分</h4><p>在社会中，我们正因为要与其他人不可避免的产生联系，才有了种种的束缚，这也许是一个社会人和自然人的根本不同吧。</p><blockquote><p>奴役的锁链仅仅是在人们相互依赖和彼此需要的时候才能够形成；彼此的需要使人们彼此结合起来。如果不首先使一个人陷入如果离开某一人就不能生存的状态，那么这个人就永远不可能奴役这个人。这种情形在自然状态下是不存在的。在自然状态中，每个人都不受束缚，即使最强大的人动用其力量也是徒然。</p></blockquote><p>人啊，往往不能节制自己的欲望。。。</p><blockquote><p>野兽根据本能决定取舍，而人类则通过自由意志进行取舍。这意味着野兽即使面对诱惑也不会去违背自然法则；但是人类却往往尽管面对损害，也会去违背这些规则。正是因为这样，生活放荡的人们沉溺于种种可能招致疾病与死亡的过度享乐中不能自拔，这是因为意志破坏了器官：当自然的需要已经得到满足的时候，意志还在不断地提出要求。</p></blockquote><p>书中指出了怜悯心是人最原始的情感，也许这也从一个侧面说明了人性本善，人类在变得理性的同时，为自己谋求利益的同时也可能变成全面的利己，而失去了最本真的自我。</p><blockquote><p>上帝赋予了人类看到同类受苦天生就会产生的一种反感。这种反感，在特定的情况下缓和了他的强烈的自尊心，或者在自尊心产生之前缓和了他自我保存的愿望，从而使他为自己谋利的热情受到限制。它是最普遍的美德，并且因为它在人类拥有理性之前就已经展现，因此对于人类是最为有用的。怜悯心是如此自然的一种美德，甚至野兽有的时候也会显露一些怜悯迹象。无需花费笔墨去描述母兽对待她们的幼崽是多么的温柔，也无需描述她们为了保护自己的孩子可以面对多么严峻的危险，我们每天都可以观察到马匹会避免践踏任何有生命的东西。一个动物从同类的尸体旁走过，总是不能遏制内心的悲伤，甚至有些动物会为已经死去的同类做某种形式的葬礼。我们很欣赏地看到《蜜蜂的寓言》的作者也不得不承认人是拥有怜悯心和情感的动物，在他描述的例子中，他抛弃了冷峻、世故的文笔，呈现给我们一个令人心碎的场景：一个被监禁的人透过他的牢笼，亲眼目睹了一头野兽从一位母亲的怀抱中夺走了她的孩子，用它致命的牙齿咬碎了孩子虚弱的身体，用它的利爪撕开了孩子鲜活的内脏。这个目击者虽然与这件事情没有个人利益上的关联，但是当他看到这个场景的那一瞬，他一定感受到了惊心动魄！目睹了这一切，却不能做任何事情去帮助昏厥的母亲和垂死的孩子，他一定是感到非常痛苦！这就是纯粹的自然感动，它先于一切的思考；这就是自然的怜悯心的力量，即使最堕落的道德也不能够破坏它。因为，我们每天在剧院中都会看到人们为一些不幸的人的悲惨遭遇动容，甚至伤心落泪；但如果他们站在暴君的位置上，只会增加对他的敌人的虐待。<br>事实上，除了对弱者、罪犯、人类整体的同情外，还有什么可以称得上是慷慨、仁慈和博爱？所谓的爱心，甚至友谊，如果我们正确地理解，就会发现它们仅仅是对于特定事物的恒久的怜悯心的产物；因为希望一个人不遭受痛苦，不就是希望他幸福吗？即使怜悯心仅仅是将我们置身于受难者的位置而产生的一种情感，这种说法除了增强我的论点的说服力外，没有任何意义。事实上，旁观的动物越是与受难者引起共鸣，怜悯心就越是强烈。现在，很显然，自然状态中的共鸣比理性状态中的共鸣深切不知几万倍。理性使人们产生自尊心，而增强自尊心的则是思考。理性使得人们追求自保；思考使得人们远离所有打扰或者妨碍他的事情。哲学使得人与世隔绝，促使人们在看到他人受苦时暗自说：“如果你要死就去死吧，只要我是安全的。” 除了社会整体的危险，没有什么可以打扰哲学家的清梦，把他从床上拖起来。人们可以肆无忌惮地在他的窗下杀死同类，因为这位哲人只需将手捂在耳朵上并且为自己争辩几句，就能够阻挡他内心澎湃着的对于受害者的怜悯。野蛮人完全不具备这令人钦佩的能力，由于缺少智慧和理智，他们总是不加思考地服从于人类的情感。<br>怜悯是一种自然的情感，它缓和了个体出于自利的行为，从而促进了人类整体的相互保存。是怜悯心使得我们看到他人受难时，不假思索地伸出援手；是怜悯心以其特有的优势，在自然状态中替代了法律、道德、美德的位置。它的优势就是没有一个人能够违抗它温柔的声音。它使得强壮的野蛮人，只要有从别的地方获取食物的可能就不会从幼小的孩童和孱弱的老人手中掠夺他们来之不易的东西。它使得合乎自然天性的格言“在为自己谋求利益的时候，尽可能少伤害他人”代替了富有理性正义的格言“己所不欲勿施于人”，从而激励所有的人；尽管前者远不如后者完善，但可能会更有用。总之，在我们探寻为什么任何人作恶时都会感到内疚的过程中，我们与其在那些微妙的争论中寻找，不如在这种自然的情感中寻找，即使作恶者对教育的格言一无所知。</p></blockquote><h4 id="第二部分"><a href="#第二部分" class="headerlink" title="第二部分"></a>第二部分</h4><p>有时候，享受变成了习惯，得到成了理所当然，却失去了其最初的快乐。</p><blockquote><p>人们可以享受大量的闲暇时光，他们用这些时光获得了许多为其祖先所不知道的享受。这是他们无意中为自己戴上的第一个枷锁，也是为自己的后代准备了第一个罪恶之源。因为这种享受不仅持续地侵蚀我们的身体与头脑，使他们虚弱下去，而且享受一旦成为习惯，就失去了快乐的力量，同时蜕化成生活的真正需要。于是，得不到它们的残酷比得到它们的幸福更大。当人们占有它们的时候，并不感到幸福，失去的时候却十分苦恼。</p></blockquote><p>这段阐述了政治社会的起源，也提醒了我们一种思考的误区：不要想当然地根据自己看到的事物，去判断我们从来没有见过的事情，也许极不相同。正如书中所说的：<strong>因此我们一定不能将被奴役的人们的堕落状态作为判断人类的自然天性是倾向于受奴役还是反抗奴役的基础，而是应当根据一切致力于抵抗压迫的自由民族取得的巨大成就来判断。</strong></p><blockquote><p>关于政治社会的起源，有人认为起源于强者的征服或弱者的联合。<br>在第一种情形下，征服权本身并不是一种真正的权利，因此也就不能成为其他权利的基础。除非被征服的民族完全恢复了自由，人们自愿选择征服者作为他们的首领，否则胜利者和被征服者之间将永远陷入相互的战争。在征服者由被征服的民族自愿推选为首领之前，不论签订了什么样的投降条约，由于这些条约都是以暴力作为唯一基础的，以此其本身就是无效的，这意味着，在这种假定的基础上是不可能产生任何真正的社会和政治组织的，除了强者的法律外，不可能有其他任何的法律。<br>在第二种情形中，强弱的含义是模糊不清的。在财产权或者先占权的确立和政治政府建立的间隔期间，这两个字的意义倒不如穷和富更加贴切。因为在法律制度产生以前，一个人要想驱使他人，除了袭击他的财产或者将自己的财产分给他们一些之外，是没有任何办法的。<br>因为穷人除了自由外没有什么可以失去的了，所以要他们毫无所获地自愿交出现今仅存的珍藏将会是最荒谬的事情。对于富人却恰恰相反，他们非常容易受到攻击，他们财产的每一个部分都可能成为攻击的目标，因此要伤害他们是比较容易的事。所以，富人需要为自身的保护采取更多的防护措施。因此，我们可以合理地断言，制度是应当由受益人而非受害者创造的。<br>社会起初不过是由少数几个公约组成的，每个人都必修遵守它们，并由共同体对个体承担保证人的责任。只有当经验表明这样的组织是如何脆弱的时候，违法者是多么轻易就能逃脱刑事案件的认定和惩罚的时候（因为只有公众才能对违法者的罪行作证和裁判），只有法律被人们想尽千百种方法规避的时候，只有不便和混乱持续增加的时候，人们才终于想到冒险将公共权力委托给特定的个人，才想到把执行人民的决议的任务委托给官员。然而，认为人们一开始就会无条件、义无反顾地投入一个专制首领的怀抱，认为狂妄尊大、未被驯服的人们为了维护公共安全所想到的第一个办法就是投身于奴隶制是不大合理的。事实上，如果不是为了保护自己免受压迫，不是为了保护构成他们生存要素的财产、自由、生命，他们为什么要给自己找到一个统治者呢？人与人关系中最坏的情况不外乎发现自己受到另一个人的支配。如果人们只是为了保护他们仅有的东西才会需要首领的帮助，那么他们一开始就将需要保存的东西交到首领的手中，不是违背常识的吗？<br>对于人们作出的如此巨大的权利让予，首领能给予他们什么样的均等利益作为回报呢？如果他胆敢假借保护民众的借口要求人们让予权利，人们很快就会用寓言中的一句话作为给他的答复：“敌人对我们也不过如此吧？” 因此，人们为自己寻找首领的原因，是为了保护他们的自由，而非找人奴役他们，这是所有的政治权力的基础原则。普林尼曾对图拉真说：“我们如果需要一位国王，那是为了他能够保护我们不被任何人奴役。”<br>政治家们关于爱好自由所做的那些诡辩与哲学家们关于自然状态所做地那些诡辩如出一辙。他们根据自己看到的事物，判断他们从来没有见过的极不相同的事情。他们因为看到奴隶忍受奴役时的耐心而断定人们就具有受奴役的天然倾向，却忘记了自由如同天真和美德，只有当一个人拥有它的时候才能够感受到它的价值，一旦人们失去了它，就失去了对它的喜爱。文明人会毫无怨言地戴着他们的枷锁，野蛮人则永远不会向枷锁低头，正如一匹未被驯服的野马，一旦有缰绳有些许靠近，它就会竖起鬃毛，用蹄子在地上刨土，激烈地暴跳，但是一匹被驯服的马就会耐心地忍受鞭子和马刺。因而，野蛮人是宁愿在暴风雨中享受自由，也不愿在安宁中受奴役的。因此我们一定不能将被奴役的人们的堕落状态作为判断人类的自然天性是倾向于受奴役还是反抗奴役的基础，而是应当根据一切致力于抵抗压迫的自由民族取得的巨大成就来判断。我知道被奴役的人们只会不断吹嘘他们在枷锁中享受到的和平与安宁，其实他们是把悲惨的奴役状态成为和平。但是，当我看到自由民族为了保护他们仅有的财产时，他们甘愿牺牲快乐、安宁、财富、权力，甚至生命本身的时候；当我看到一些生来自由的动物，因为憎恨被囚禁而撞向牢笼的栅栏，撞破了头的时候；当我看到成千上万赤裸裸的野蛮人鄙视欧洲人的骄奢淫逸，只为保持他们的独立而甘冒饥饿、战火、刀剑和死亡风险的时候，我感到关于“自由”的讨论是与奴隶无关的。</p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;《论人类不平等的起源》是卢梭的早期作品之一，英文原书名为“A Discourse On Inequality”。在此书中，卢梭揭示出私有制是人类不平等的起源和基础，他认为自然状态的野蛮人是独居的，彼此间没有任何联系，对同类既无所需要，也无加害意图，甚至从来不能辨认他同类中的
      
    
    </summary>
    
      <category term="文学" scheme="http://yoursite.com/categories/%E6%96%87%E5%AD%A6/"/>
    
    
      <category term="摘抄" scheme="http://yoursite.com/tags/%E6%91%98%E6%8A%84/"/>
    
      <category term="文学" scheme="http://yoursite.com/tags/%E6%96%87%E5%AD%A6/"/>
    
  </entry>
  
  <entry>
    <title>朴素贝叶斯法</title>
    <link href="http://yoursite.com/2018/08/11/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E6%B3%95/"/>
    <id>http://yoursite.com/2018/08/11/朴素贝叶斯法/</id>
    <published>2018-08-11T11:17:46.000Z</published>
    <updated>2018-09-28T02:50:55.576Z</updated>
    
    <content type="html"><![CDATA[<h3 id="理论"><a href="#理论" class="headerlink" title="理论"></a>理论</h3><h4 id="公式推导"><a href="#公式推导" class="headerlink" title="公式推导"></a>公式推导</h4><p>朴素贝叶斯法是基于贝叶斯定理和特征条件独立假设的分类方法，即对给定的输入 $x$，预测其类别 $y$。<br>此方法的思路是首先由训练数据计算 $P(Y)$ 和 $P(X|Y)$ 的估计,然后得到联合概率分布<br>$$ P(X,Y) = P(Y)P(X|Y) $$<br>之后利用贝叶斯定理及学到的联合概率分布计算 $X$ 属于类别 $Y$ 的概率<br>$$ P(Y|X) = \frac{P(X,Y)}{P(X)} = \frac{P(Y)P(X|Y)}{\mathop{\sum}_{Y}P(Y)P(X|Y)}$$<br>对于给定的输入 $x$，通过上式计算 $x$ 属于类别 $c_k$ 的概率 $ P(Y=c_k|X=x) $，即<br>$$ P(Y=c_k|X=x) =  \frac{P(Y=c_k)P(X=x|Y=c_k)}{\mathop{\sum}_{k}P(Y=c_k)P(X=x|Y=c_k)}$$<br>又由朴素贝叶斯法的特征条件独立性假设，有<br>$$\begin{equation}\begin{split}<br>P(X=x|Y=c_k) &amp;=P( X^{(1)}=x^{(1)},\cdots,X^{(n)}=x^{(n)} | Y=c_k )\\<br>&amp;= \prod_{j=1}^{n} P(X^{(j)}=x^{(j)}|Y=c_k)<br>\end{split}\end{equation}$$<br>其中，$x$ 为 $n$ 维向量，$x^{(j)}$ 为 $x$ 的第 $j$ 个特征。故<br>$$ P(Y=c_k|X=x) = \frac{P(Y=c_k)\prod_{j}P(X^{(j)}=x^{(j)}|Y=c_k)}{\mathop{\sum}_{k}P(Y=c_k)\prod_{j}P(X^{(j)}=x^{(j)}|Y=c_k)} , k=1,2,\dots,K$$<br>将 $x$ 分到后验概率最大的类中，朴素贝叶斯分类器可表示为<br>$$ y = f(x) = arg \max_{c_k} \frac{P(Y=c_k)\prod_{j}P(X^{(j)}=x^{(j)}|Y=c_k)}{\mathop{\sum}_{k}P(Y=c_k)\prod_{j}P(X^{(j)}=x^{(j)}|Y=c_k)}$$<br>又因为上式中分母对于所有 $c_k$ 都是相同的，故上式可以简化为<br>$$ y = arg \max_{c_k} P(Y=c_k)\prod_{j}P(X^{(j)}=x^{(j)}|Y=c_k) $$</p><p>由上式可知，只要由训练数据估计出每一个类别的概率 $P(Y=c_k)$ 和输入的每一个特征值在某一类别下的概率 $P(X^{(j)}=x^{(j)}|Y=c_k)$，便可进行预测。下面介绍进行估计的两种方法。</p><h4 id="参数估计"><a href="#参数估计" class="headerlink" title="参数估计"></a>参数估计</h4><h5 id="极大似然估计"><a href="#极大似然估计" class="headerlink" title="极大似然估计"></a>极大似然估计</h5><p>假设训练数据集为 $T = \{(x_1,y_1),\dots,(x_n,y_n)\}$。<br>先验概率 $P(Y=c_k)$ 的极大似然估计为<br>$$ P(Y=c_k) = \frac{\sum_{i=1}^{N}I(y_i=c_k)}{N} $$<br>设第 $j$ 个特征 $x^{(j)}$ 可能取值的集合为 $\{a_{j1},\dots,a_{jS_j}\}$，条件概率 $P(X^{(j)}=a_{jl}|Y=c_k)$ 的极大似然估计为<br>$$ P(X^{(j)}=a_{jl}|Y=c_k) = \frac{\sum_{i=1}^{N}I(x_{i}^{(j)}=a_{jl},y_i=c_k)}{\sum_{i=1}^{N}I(y_i=c_k)} $$<br>$$ j=1,2,\dots,n;l=1,2,\dots,S_j;k=1,2,\dots,K $$<br>其中，$x_{i}^{j}$ 是第 $i$ 个样本的第 $j$ 个特征；$a_{jl}$ 是第 $j$ 个特征可能取的第 $l$ 个值； $I$ 为指示函数，满足取 $1$，否则取 $0$。</p><h5 id="贝叶斯估计"><a href="#贝叶斯估计" class="headerlink" title="贝叶斯估计"></a>贝叶斯估计</h5><p>极大似然估计可能会出现所要估计的概率值为0的情况，在随机变量各个取值的频数上赋予一个正数 $\lambda \gt 0$，常取 $\lambda = 1$，称为拉普拉斯平滑。<br>$$ P_{\lambda}(Y=c_k) = \frac{\sum_{i=1}^{N}I(y_i=c_k)+{\lambda}}{N+K\lambda} $$<br>$$ P_{\lambda}(X^{(j)}=a_{jl}|Y=c_k) = \frac{\sum_{i=1}^{N}I(x_{i}^{(j)}=a_{jl},y_i=c_k)+\lambda}{\sum_{i=1}^{N}I(y_i=c_k)+S_j\lambda} $$</p><h3 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h3><p>训练一个朴素贝叶斯分类器并确定 $x=(2,S)^T$ 的类标记 $y$。表中 $X^{(1)}$，$X^{(2)}$ 为特征，取值集合分别为 $A_1 = \{1,2,3\}$， $A_2 = \{S,M,L\}$， $Y$ 为类标记，$Y \in C =\{1,-1\}$。</p><p>训练数据 train_data.csv<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">ID,X1,X2,Y</span><br><span class="line">1,1,S,-1</span><br><span class="line">2,1,M,-1</span><br><span class="line">3,1,M,1</span><br><span class="line">4,1,S,1</span><br><span class="line">5,1,S,-1</span><br><span class="line">6,2,S,-1</span><br><span class="line">7,2,M,-1</span><br><span class="line">8,2,M,1</span><br><span class="line">9,2,L,1</span><br><span class="line">10,2,L,1</span><br><span class="line">11,3,L,1</span><br><span class="line">12,3,M,1</span><br><span class="line">13,3,M,1</span><br><span class="line">14,3,L,1</span><br><span class="line">15,3,L,-1</span><br></pre></td></tr></table></figure></p><p>代码实现 naivebayes.py<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"> # -*- coding: utf-8 -*-</span><br><span class="line">import pandas as pd</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def add2dict(thedict, key_a, key_b, val):</span><br><span class="line">    if key_a in thedict.keys():</span><br><span class="line">        thedict[key_a].update(&#123;key_b: val&#125;)</span><br><span class="line">    else:</span><br><span class="line">        thedict.update(&#123;key_a:&#123;key_b: val&#125;&#125;)        </span><br><span class="line"></span><br><span class="line">def conditionalProbability(obj, attribute, clazz, lambd):</span><br><span class="line">    C = obj[clazz].value_counts()</span><br><span class="line">    label = C.index</span><br><span class="line">    counts = C.values</span><br><span class="line"></span><br><span class="line">    CP = dict()</span><br><span class="line">    for i in range(label.size):</span><br><span class="line">        for j in range(attribute.size):</span><br><span class="line">            temp = obj[obj[clazz] == label[i]][attribute[j]] </span><br><span class="line">            CC = temp.value_counts()</span><br><span class="line">            Sj = obj[attribute[j]].value_counts().index.size</span><br><span class="line">            P = ( CC + lambd) / ( counts[i] + Sj*lambd)</span><br><span class="line">            add2dict(CP,label[i],attribute[j],P) # Using dict to store probabilities</span><br><span class="line">    return CP</span><br><span class="line"></span><br><span class="line">def priorProbability(obj, clazz, lambd):</span><br><span class="line">    C = obj[clazz].value_counts()</span><br><span class="line">    N = float(obj.index.size)</span><br><span class="line">    K = float(C.index.size)</span><br><span class="line">    P = ( C + lambd ) / ( N + K*lambd)</span><br><span class="line">    return P</span><br><span class="line"></span><br><span class="line">def predicts(x, obj, attribute, clazz,lambd):</span><br><span class="line">    label = obj[clazz].value_counts().index # Types of class</span><br><span class="line">    P = priorProbability(obj,clazz, lambd) # Prior probability</span><br><span class="line">    CP = conditionalProbability(obj, attribute, clazz, lambd) # Conditional probability</span><br><span class="line">    max_p = 0 # Probability of the most likely class</span><br><span class="line">    max_c = &apos;&apos; # The most likely class</span><br><span class="line">    for i in range(label.size):</span><br><span class="line">        cur_max_p = 1</span><br><span class="line">        for j in range(attribute.size):</span><br><span class="line">            cur_max_p *= CP[label[i]][attribute[j]][x[j]]</span><br><span class="line">        cur_max_p *= P[label[i]]</span><br><span class="line">        if cur_max_p &gt; max_p:</span><br><span class="line">            max_c = str(label[i])</span><br><span class="line">            max_p = cur_max_p</span><br><span class="line">    return [max_c,max_p]</span><br><span class="line"></span><br><span class="line">df = pd.read_csv(&apos;train_data.csv&apos;, encoding=&apos;utf-8&apos;)</span><br><span class="line">[max_c,max_p] = predicts([2,&apos;S&apos;],df, df.columns.drop(&apos;Y&apos;).drop(&apos;ID&apos;), &apos;Y&apos;, 1)</span><br><span class="line">print(max_c,max_p)</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;理论&quot;&gt;&lt;a href=&quot;#理论&quot; class=&quot;headerlink&quot; title=&quot;理论&quot;&gt;&lt;/a&gt;理论&lt;/h3&gt;&lt;h4 id=&quot;公式推导&quot;&gt;&lt;a href=&quot;#公式推导&quot; class=&quot;headerlink&quot; title=&quot;公式推导&quot;&gt;&lt;/a&gt;公式推导&lt;/h
      
    
    </summary>
    
      <category term="统计机器学习" scheme="http://yoursite.com/categories/%E7%BB%9F%E8%AE%A1%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="朴素贝叶斯" scheme="http://yoursite.com/tags/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF/"/>
    
      <category term="统计学习" scheme="http://yoursite.com/tags/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>MathJax使用LaTeX语法编写数学公式教程</title>
    <link href="http://yoursite.com/2018/08/11/MathJax%E4%BD%BF%E7%94%A8LaTeX%E8%AF%AD%E6%B3%95%E7%BC%96%E5%86%99%E6%95%B0%E5%AD%A6%E5%85%AC%E5%BC%8F%E6%95%99%E7%A8%8B/"/>
    <id>http://yoursite.com/2018/08/11/MathJax使用LaTeX语法编写数学公式教程/</id>
    <published>2018-08-11T08:51:30.000Z</published>
    <updated>2018-09-28T02:50:55.576Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>转载于<a href="https://www.zybuluo.com/knight/note/96093" target="_blank" rel="noopener">@knight</a>博客，感谢博主的分享。</p></blockquote><h3 id="如何插入公式"><a href="#如何插入公式" class="headerlink" title="如何插入公式"></a>如何插入公式</h3><p>LaTeX的数学公式有两种：行中公式和独立公式。行中公式放在文中与其它文字混编，独立公式单独成行。</p><p>行中公式可以用如下方法表示：</p><p>\$数学公式\$</p><p>独立公式可以用如下方法表示：</p><p>\$\$ 数学公式 \$\$</p><p>例子：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$J_\alpha(x) = \sum_&#123;m=0&#125;^\infty \frac&#123;(-1)^m&#125;&#123;m! \Gamma (m + \alpha + 1)&#125; &#123;\left(&#123; \frac&#123;x&#125;&#123;2&#125; &#125;\right)&#125;^&#123;2m + \alpha&#125;$</span><br></pre></td></tr></table></figure></p><p>显示：</p><p>$J_\alpha(x) = \sum_{m=0}^\infty \frac{(-1)^m}{m! \Gamma (m + \alpha + 1)} {\left({ \frac{x}{2} }\right)}^{2m + \alpha}$</p><h3 id="如何插入公式大括号"><a href="#如何插入公式大括号" class="headerlink" title="如何插入公式大括号"></a>如何插入公式大括号</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$$f(x)=</span><br><span class="line">\begin&#123;cases&#125;</span><br><span class="line">0&amp; \text&#123;x=0&#125;\\\\</span><br><span class="line">1&amp; \text&#123;x!=0&#125;</span><br><span class="line">\end&#123;cases&#125;$$</span><br></pre></td></tr></table></figure><p>$$f(x)=<br>\begin{cases}<br>0&amp; \text{x=0}\\<br>1&amp; \text{x!=0}<br>\end{cases}$$</p><h3 id="矩阵"><a href="#矩阵" class="headerlink" title="矩阵"></a>矩阵</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$$\begin&#123;bmatrix&#125; </span><br><span class="line">1&amp;x&amp;x^2\\\\ </span><br><span class="line">1&amp;y&amp;y^2\\\\ </span><br><span class="line">1&amp;z&amp;z^2 </span><br><span class="line">\end&#123;bmatrix&#125;$$</span><br></pre></td></tr></table></figure><p>$$\begin{bmatrix}<br>1&amp;x&amp;x^2\\<br>1&amp;y&amp;y^2\\<br>1&amp;z&amp;z^2<br>\end{bmatrix}$$ </p><h3 id="如何输入上下标"><a href="#如何输入上下标" class="headerlink" title="如何输入上下标"></a>如何输入上下标</h3><p>^表示上标， _表示下标。如果上下标的内容多于一个字符，要用{}把这些内容括起来当成一个整体。上下标是可以嵌套的，也可以同时使用。</p><p>例子：<code>$x^{y^z}=(1+{\rm e}^x)^{-2xy^w}$</code></p><p>显示：$x^{y^z}=(1+{\rm e}^x)^{-2xy^w}$</p><p>另外，如果要在左右两边都有上下标，可以用<code>\sideset</code>命令。</p><p>例子：<code>$\sideset{^1_2}{^3_4}\bigotimes$</code></p><p>显示：$ \sideset{^1_2}{^3_4} \bigotimes $</p><p><code>$$\max_{k}$$</code><br>$$\max_{k}$$<br><code>$$\mathop{argmax}_{K}$$</code><br>$$\mathop{argmax}_{K}$$</p><h3 id="如何输入括号和分隔符"><a href="#如何输入括号和分隔符" class="headerlink" title="如何输入括号和分隔符"></a>如何输入括号和分隔符</h3><p>()、[]和|表示自己，{}表示{}。当要显示大号的括号或分隔符时，要用\left和\right命令。</p><p>例子：<code>$f(x,y,z) = 3y^2z \left( 3+\frac{7x+5}{1+y^2} \right)$</code></p><p>显示：$f(x,y,z) = 3y^2z \left( 3+\frac{7x+5}{1+y^2} \right)$</p><p>有时候要用<code>\left.</code>或<code>\right.</code>进行匹配而不显示本身。</p><p>例子：<code>$\left. \frac{du}{dx} \right| _{x=0}$</code></p><p>显示：$\left. \frac{du}{dx} \right| _{x=0}$</p><h3 id="如何输入分数"><a href="#如何输入分数" class="headerlink" title="如何输入分数"></a>如何输入分数</h3><p>例子：<code>$\frac{1}{3}$</code>　或　<code>$1 \over 3$</code></p><p>显示：$\frac{1}{3}$　或　$1 \over 3$</p><h3 id="如何输入开方"><a href="#如何输入开方" class="headerlink" title="如何输入开方"></a>如何输入开方</h3><p>例子：<code>$\sqrt{2}$</code>　和　<code>$\sqrt[n]{3}$</code></p><p>显示：$\sqrt{2}$　和　$\sqrt[n]{3}$</p><h3 id="如何输入省略号"><a href="#如何输入省略号" class="headerlink" title="如何输入省略号"></a>如何输入省略号</h3><p>数学公式中常见的省略号有两种，\ldots表示与文本底线对齐的省略号，\cdots表示与文本中线对齐的省略号。</p><p>例子：<code>$f(x_1,x_2,\ldots,x_n) = x_1^2 + x_2^2 + \cdots + x_n^2$</code></p><p>显示：$f(x_1,x_2,\ldots,x_n) = x_1^2 + x_2^2 + \cdots + x_n^2$</p><h3 id="如何输入矢量"><a href="#如何输入矢量" class="headerlink" title="如何输入矢量"></a>如何输入矢量</h3><p>例子：<code>$\vec{a} \cdot \vec{b}=0$</code></p><p>显示：$\vec{a} \cdot \vec{b}=0$</p><h3 id="如何输入积分"><a href="#如何输入积分" class="headerlink" title="如何输入积分"></a>如何输入积分</h3><p>例子：<code>$\int_0^1 x^2 {\rm d}x$</code></p><p>显示：$\int_0^1 x^2 {\rm d}x$</p><h3 id="如何输入极限运算"><a href="#如何输入极限运算" class="headerlink" title="如何输入极限运算"></a>如何输入极限运算</h3><p>例子：<br><code>$\lim\limits_{n \rightarrow +\infty} \frac{1}{n(n+1)}$</code></p><p><code>$$\lim_{n \rightarrow +\infty} \frac{1}{n(n+1)}$$</code></p><p>显示：$\lim\limits_{n \rightarrow +\infty} \frac{1}{n(n+1)}$<br>$$\lim_{n \rightarrow +\infty} \frac{1}{n(n+1)}$$ </p><blockquote><p>注:在latex中输入极限，主要的一种形式是使用<code>\lim</code>，输出的就是极限的原样。<br>如果在<code>$*****$</code>环境中，使用上下标起不到作用，在<code>$$******$$</code>中使用下标，会使下标部分出现在limit之下。<br>在文章中间，使用这种形式的极限，可以选择使用这种形式<code>\lim\limits_{t \to \infty }{x(t)}</code>。<br>上下极限的输入textfriend里面直接就有。<br>另外一点需要注意的是，极限的下标如果有多行的话，使用断行，有几种方法：可以使用<code>array</code>或者<code>substack</code>命令，也可以使用<code>\stackrel{top}{bot}</code>或者<code>mathop</code>命令。</p></blockquote><h3 id="如何输入累加、累乘运算"><a href="#如何输入累加、累乘运算" class="headerlink" title="如何输入累加、累乘运算"></a>如何输入累加、累乘运算</h3><p>例子：<code>$\sum_{i=0}^n \frac{1}{i^2}$　和　$\prod_{i=0}^n \frac{1}{i^2}$</code></p><p>显示：$\sum_{i=0}^n \frac{1}{i^2}$　和　$\prod_{i=0}^n \frac{1}{i^2}$</p><h3 id="如何进行公式应用"><a href="#如何进行公式应用" class="headerlink" title="如何进行公式应用"></a>如何进行公式应用</h3><p>例子：<code>$ r = r_F + \beta(r_M-r_F) - \epsilon $</code></p><p>显示： $ r = r_F + \beta(r_M-r_F) - \epsilon $</p><h3 id="如何输入希腊字母"><a href="#如何输入希腊字母" class="headerlink" title="如何输入希腊字母"></a>如何输入希腊字母</h3><p>例子：</p><p>\alpha　A　\beta　B　\gamma　\Gamma　\delta　\Delta　\epsilon　E<br>\varepsilon　　\zeta　Z　\eta　H　\theta　\Theta　\vartheta<br>\iota　I　\kappa　K　\lambda　\Lambda　\mu　M　\nu　N<br>\xi　\Xi　o　O　\pi　\Pi　\varpi　　\rho　P<br>\varrho　　\sigma　\Sigma　\varsigma　　\tau　T　\upsilon　\Upsilon<br>\phi　\Phi　\varphi　　\chi　X　\psi　\Psi　\omega　\Omega</p><p>显示： </p><p>$\alpha$　$A$　$\beta$　$B$　$\gamma$　$\Gamma$　$\delta$　$\Delta$　$\epsilon$　$E$<br>$\varepsilon$　　$\zeta$　$Z$　$\eta$　$H$　$\theta$　$\Theta$　$\vartheta$<br>$\iota$　$I$　$\kappa$　$K$　$\lambda$　$\Lambda$　$\mu$　$M$　$\nu$　$N$<br>$\xi$　$\Xi$　$o$　$O$　$\pi$　$\Pi$　$\varpi$　　$\rho$　$P$<br>$\varrho$　　$\sigma$　$\Sigma$　$\varsigma$　　$\tau$　$T$　$\upsilon$　$\Upsilon$<br>$\phi$　$\Phi$　$\varphi$　$\chi$　$X$　$\psi$　$\Psi$　$\omega$　$\Omega$</p><h3 id="如何输入其它特殊字符"><a href="#如何输入其它特殊字符" class="headerlink" title="如何输入其它特殊字符"></a>如何输入其它特殊字符</h3><h4 id="关系运算符："><a href="#关系运算符：" class="headerlink" title="关系运算符："></a>关系运算符：</h4><p>±：\pm<br>×：\times<br>÷：\div<br>∣：\mid<br>∤：\nmid<br>⋅：\cdot<br>∘：\circ<br>∗：\ast<br>⨀：\bigodot<br>⨂：\bigotimes<br>⨁：\bigoplus<br>≤：\leq<br>≥：\geq<br>≠：\neq<br>≈：\approx<br>≡：\equiv<br>∑：\sum<br>∏：\prod<br>∐：\coprod</p><h4 id="集合运算符："><a href="#集合运算符：" class="headerlink" title="集合运算符："></a>集合运算符：</h4><p>∅：\emptyset<br>∈：\in<br>∉：\notin<br>⊂：\subset<br>⊃：\supset<br>⊆：\subseteq<br>⊇：\supseteq<br>⋂：\bigcap<br>⋃：\bigcup<br>⋁：\bigvee<br>⋀：\bigwedge<br>⨄：\biguplus<br>⨆：\bigsqcup</p><h4 id="对数运算符："><a href="#对数运算符：" class="headerlink" title="对数运算符："></a>对数运算符：</h4><p>log：\log<br>lg：\lg<br>ln：\ln</p><h4 id="三角运算符："><a href="#三角运算符：" class="headerlink" title="三角运算符："></a>三角运算符：</h4><p>⊥：\bot<br>∠：\angle<br>30∘：30^\circ<br>sin：\sin<br>cos：\cos<br>tan：\tan<br>cot：\cot<br>sec：\sec<br>csc：\csc</p><h4 id="微积分运算符："><a href="#微积分运算符：" class="headerlink" title="微积分运算符："></a>微积分运算符：</h4><p>′：\prime<br>∫：\int<br>∬：\iint<br>∭：\iiint<br>⨌：\iiiint<br>∮：\oint<br>lim：\lim<br>∞：\infty<br>∇：\nabla</p><h4 id="逻辑运算符："><a href="#逻辑运算符：" class="headerlink" title="逻辑运算符："></a>逻辑运算符：</h4><p>∵：\because<br>∴：\therefore<br>∀：\forall<br>∃：\exists<br>≠：\not=<br>≯：\not&gt;<br>⊄：\not\subset</p><h4 id="戴帽符号："><a href="#戴帽符号：" class="headerlink" title="戴帽符号："></a>戴帽符号：</h4><p>$\hat{y}$ ：\hat{y}<br>$\check{y} $：\check{y}<br>$\breve{y}$：\breve{y}</p><h4 id="连线符号："><a href="#连线符号：" class="headerlink" title="连线符号："></a>连线符号：</h4><p>$\overline{a+b+c+d}$ ：\overline{a+b+c+d} </p><p>$\underline{a+b+c+d} $ ：\underline{a+b+c+d} </p><h4 id="箭头符号："><a href="#箭头符号：" class="headerlink" title="箭头符号："></a>箭头符号：</h4><p>↑：\uparrow<br>↓：\downarrow<br>⇑：\Uparrow<br>⇓：\Downarrow<br>→：\rightarrow<br>←：\leftarrow<br>⇒：\Rightarrow<br>⇐：\Leftarrow<br>⟶：\longrightarrow<br>⟵：\longleftarrow<br>⟹：\Longrightarrow<br>⟸：\Longleftarrow</p><p>要输出字符　空格　#　$　%　&amp;　_　{　}　，用命令：　\空格　#　\$　\%　\&amp;　_　{　}</p><h3 id="如何进行字体转换"><a href="#如何进行字体转换" class="headerlink" title="如何进行字体转换"></a>如何进行字体转换</h3><p>要对公式的某一部分字符进行字体转换，可以用{\rm 需转换的部分字符}命令，其中\rm可以参照下表选择合适的字体。一般情况下，公式默认为意大利体。<br>\rm　　罗马体　　　　　　　\it　　意大利体<br>\bf　　黑体　　　　　　　　\cal 　花体<br>\sl　　倾斜体　　　　　　　\sf　　等线体<br>\mit 　数学斜体　　　　　　\tt　　打字机字体<br>\sc　　小体大写字母</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;转载于&lt;a href=&quot;https://www.zybuluo.com/knight/note/96093&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;@knight&lt;/a&gt;博客，感谢博主的分享。&lt;/p&gt;
&lt;/blockquot
      
    
    </summary>
    
      <category term="MathJax" scheme="http://yoursite.com/categories/MathJax/"/>
    
    
      <category term="LaTex" scheme="http://yoursite.com/tags/LaTex/"/>
    
      <category term="MathJax" scheme="http://yoursite.com/tags/MathJax/"/>
    
  </entry>
  
  <entry>
    <title>Hexo博客重建记</title>
    <link href="http://yoursite.com/2018/08/04/Hexo%E5%8D%9A%E5%AE%A2%E9%87%8D%E5%BB%BA%E8%AE%B0/"/>
    <id>http://yoursite.com/2018/08/04/Hexo博客重建记/</id>
    <published>2018-08-04T15:29:38.000Z</published>
    <updated>2018-09-28T02:50:55.576Z</updated>
    
    <content type="html"><![CDATA[<h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p>好记性是真的不如烂笔头！另外，自己也深刻觉得做事情多做10%十分重要，比如阅解决一个什么问题，不能懒，及时记录下来，下次遇到便能省不少事儿，而现实情况是每次遇到同一个问题还要重新折腾，很是难受；比如看书，及时整理记录下来，时而回顾，也能防止遗忘，节约不少功夫。<br>半年前兴趣突发用Hexo鼓捣了一个博客，结果之后就荒废了，再想用的时候发现什么都忘了，耽误很多时间，遂决定重建一下，并记录下来这个过程，一来以备自己随时查阅，二来可以给想建博客的人们一个参考。</p><h3 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h3><p>在安装 Hexo 之前，需要确保检查电脑中已安装下列软件：</p><ul><li>Node.js</li><li>Git</li></ul><p>有关Git和Node.js的安装可以参考廖雪峰的<a href="https://www.liaoxuefeng.com/wiki/0013739516305929606dd18361248578c67b8067c8c017b000/00137396287703354d8c6c01c904c7d9ff056ae23da865a000" target="_blank" rel="noopener">Git教程</a>和<a href="https://www.liaoxuefeng.com/wiki/001434446689867b27157e896e74d51a89c25cc8b43bdb3000/001434501245426ad4b91f2b880464ba876a8e3043fc8ef000" target="_blank" rel="noopener">JavaScript教程</a>。对于windows用户来说，推荐使用 Git Bash 进行操作，这是git for windows自带的一组程序。</p><h3 id="安装Hexo"><a href="#安装Hexo" class="headerlink" title="安装Hexo"></a>安装Hexo</h3><p>接下来只需要运行下面一句命令即可完成 Hexo 的安装。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ npm install -g hexo-cli</span><br></pre></td></tr></table></figure></p><p>安装 Hexo 完成后，新建一个网站，执行：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ hexo init Hexo_Blog  # Hexo_Blog是博客所在文件夹名，可自行替换为你的文件夹</span><br><span class="line">$ cd Hexo_Blog</span><br><span class="line">$ npm install</span><br></pre></td></tr></table></figure></p><h3 id="安装主题"><a href="#安装主题" class="headerlink" title="安装主题"></a>安装主题</h3><p>设置主题，这里我采用的是非常受欢迎的<a href="http://theme-next.iissnan.com/" target="_blank" rel="noopener">Next主题</a>，运行下面命令，直接克隆整个仓库。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ cd Hexo_Blog</span><br><span class="line">$ git clone https://github.com/iissnan/hexo-theme-next themes/next</span><br></pre></td></tr></table></figure></p><p>然后在<strong>站点配置文件</strong>中设置你的主题：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">theme: next</span><br></pre></td></tr></table></figure></p><p>接下来我们验证主题是否安装成功。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ hexo clean</span><br><span class="line">$ hexo s --debug</span><br></pre></td></tr></table></figure></p><p>此时使用浏览器访问 <a href="http://localhost:4000，检查是否成功。如果端口4000被占用，可使用" target="_blank" rel="noopener">http://localhost:4000，检查是否成功。如果端口4000被占用，可使用</a> -p 更换端口。</p><blockquote><p>在 Hexo 中有两份主要的配置文件，其名称都是 _config.yml。 其中，一份位于站点根目录下，主要包含 Hexo 本身的配置；另一份位于主题目录下，这份配置由主题作者提供，主要用于配置主题相关的选项。为了描述方便，在以下说明中，将前者称为 站点配置文件， 后者称为 主题配置文件。</p></blockquote><h3 id="配置主题"><a href="#配置主题" class="headerlink" title="配置主题"></a>配置主题</h3><h4 id="选择-Scheme"><a href="#选择-Scheme" class="headerlink" title="选择 Scheme"></a>选择 Scheme</h4><p>Scheme 是 NexT 提供的一种特性，不同Scheme有不同的外观。切换只需在<strong>主题配置文件</strong>中更改即可。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># Schemes</span><br><span class="line">scheme: Muse</span><br><span class="line">#scheme: Mist</span><br><span class="line">#scheme: Pisces</span><br><span class="line">#scheme: Gemini</span><br></pre></td></tr></table></figure></p><h4 id="设置站点名，语言，作者昵称，站点描述"><a href="#设置站点名，语言，作者昵称，站点描述" class="headerlink" title="设置站点名，语言，作者昵称，站点描述"></a>设置站点名，语言，作者昵称，站点描述</h4><p>在<strong>站点配置文件</strong>进行更改，如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># Site</span><br><span class="line">title: shuang&apos;s blog</span><br><span class="line">subtitle:</span><br><span class="line">description: 滴水穿石，非一日之功</span><br><span class="line">keywords:</span><br><span class="line">author: cccshuang</span><br><span class="line">language: zh-Hans</span><br><span class="line">timezone:</span><br></pre></td></tr></table></figure></p><h4 id="设置头像"><a href="#设置头像" class="headerlink" title="设置头像"></a>设置头像</h4><p>将头像放置主题目录下的 source/images/ 目录下，在<strong>主题配置文件</strong>中配置为：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">avatar: /images/avatar.gif</span><br></pre></td></tr></table></figure></p><h4 id="设置菜单"><a href="#设置菜单" class="headerlink" title="设置菜单"></a>设置菜单</h4><p>在<strong>主题配置文件</strong>中修改以下内容：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">menu:</span><br><span class="line">  home: / || home</span><br><span class="line">  #about: /about/ || user</span><br><span class="line">  tags: /tags/ || tags</span><br><span class="line">  categories: /categories/ || th</span><br><span class="line">  archives: /archives/ || archive</span><br></pre></td></tr></table></figure></p><p>其中分类、标签云、关于等页面需要自己添加，输入<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo new page &quot;categories&quot; #新建页面</span><br></pre></td></tr></table></figure></p><p>之后在站点目录下的source文件夹下，会新增一个categories的文件夹，里面有一个index.md文件，编辑如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">title: 分类</span><br><span class="line">date: 2018-08-04 15:44:40</span><br><span class="line">type: &quot;categories&quot;</span><br><span class="line">comments: false</span><br></pre></td></tr></table></figure></p><p>comments设置为false是打开分类页面，不显示评论插件。<br>tags, about页面的创建相似，如出现中文乱码，可尝试把创建的md文件更改为UTF-8编码。</p><h4 id="设置首页列表是否显示阅读更多"><a href="#设置首页列表是否显示阅读更多" class="headerlink" title="设置首页列表是否显示阅读更多"></a>设置首页列表是否显示阅读更多</h4><p>在<strong>主题配置文件</strong>中修改以下内容：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">auto_excerpt:</span><br><span class="line">  enable: true</span><br><span class="line">  length: 150</span><br></pre></td></tr></table></figure></p><h4 id="添加RSS"><a href="#添加RSS" class="headerlink" title="添加RSS"></a>添加RSS</h4><p>首先安装 hexo-generator-feed插件：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ npm install --save hexo-generator-feed</span><br></pre></td></tr></table></figure></p><p>修改站点配置文件:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">feed: # RSS</span><br><span class="line">  type: atom</span><br><span class="line">  path: atom.xml</span><br><span class="line">  limit: 0</span><br><span class="line"></span><br><span class="line">plugins: hexo-generate-feed</span><br></pre></td></tr></table></figure></p><p>修改主题配置文件:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rss: /atom.xml</span><br></pre></td></tr></table></figure></p><h4 id="添加侧边栏社交链接"><a href="#添加侧边栏社交链接" class="headerlink" title="添加侧边栏社交链接"></a>添加侧边栏社交链接</h4><p>修改主题配置文件:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">social:</span><br><span class="line">  GitHub: your github url</span><br><span class="line">  ZhiHu: your zhihu url</span><br></pre></td></tr></table></figure></p><h4 id="字数统计"><a href="#字数统计" class="headerlink" title="字数统计"></a>字数统计</h4><p>统计文章的字数以及大致分析出阅读时间。修改主题配置文件：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">post_wordcount:</span><br><span class="line">  item_text: true</span><br><span class="line">  wordcount: true</span><br><span class="line">  min2read: true</span><br><span class="line">  totalcount: true</span><br><span class="line">  separated_meta: ture</span><br></pre></td></tr></table></figure></p><p>并安装插件：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ npm install hexo-wordcount --save</span><br></pre></td></tr></table></figure></p><h4 id="背景动画"><a href="#背景动画" class="headerlink" title="背景动画"></a>背景动画</h4><p>将主题配置文件下面其中一项改为true即可。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># Canvas-nest</span><br><span class="line">canvas_nest: true</span><br><span class="line"># three_waves</span><br><span class="line">three_waves: false</span><br><span class="line"># canvas_lines</span><br><span class="line">canvas_lines: false</span><br><span class="line"># canvas_sphere</span><br><span class="line">canvas_sphere: false</span><br></pre></td></tr></table></figure></p><h4 id="添加自动打开编辑器脚本"><a href="#添加自动打开编辑器脚本" class="headerlink" title="添加自动打开编辑器脚本"></a>添加自动打开编辑器脚本</h4><p>在 博客根目录/scripts/ 下新建 AutoOpenEditor.js 文件（取其他名字也可以，不影响）（如果没有 scripts 目录则新建），并粘贴以下代码，保存。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">let spawn = require(&apos;hexo-util/lib/spawn&apos;);</span><br><span class="line"></span><br><span class="line">hexo.on(&apos;new&apos;, (data) =&gt; &#123;</span><br><span class="line">  spawn(&apos;code&apos;, [hexo.base_dir, data.path]);</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure></p><p>这样，在你每次 hexo new 的时候，脚本就会自动帮你打开 VS Code 并切换到博客根目录顺带打开新建的 .md 文件啦。<br>参考博客 <a href="https://leaferx.online/2018/03/17/hexo-auto-open-vscode/" target="_blank" rel="noopener">HEXO小技巧在 hexo new 的时候自动用 VS Code 打开新建文章</a></p><h3 id="第三方服务"><a href="#第三方服务" class="headerlink" title="第三方服务"></a>第三方服务</h3><h4 id="阅读次数统计（LeanCloud）"><a href="#阅读次数统计（LeanCloud）" class="headerlink" title="阅读次数统计（LeanCloud）"></a>阅读次数统计（LeanCloud）</h4><p>可参考这个博客 <a href="https://notes.wanghao.work/2015-10-21-%E4%B8%BANexT%E4%B8%BB%E9%A2%98%E6%B7%BB%E5%8A%A0%E6%96%87%E7%AB%A0%E9%98%85%E8%AF%BB%E9%87%8F%E7%BB%9F%E8%AE%A1%E5%8A%9F%E8%83%BD.html#%E9%85%8D%E7%BD%AELeanCloud" target="_blank" rel="noopener">为NexT主题添加文章阅读量统计功能</a></p><h4 id="分享文章功能"><a href="#分享文章功能" class="headerlink" title="分享文章功能"></a>分享文章功能</h4><p>使用<a href="https://www.addthis.com" target="_blank" rel="noopener">AddThis</a>，定义自己的样式，如可以通过微信，微博，qq等进行分享。然后在Profile Settings的General里复制ID，修改主题配置文件：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">add_this_id: &#123;your AddThis ID&#125;</span><br></pre></td></tr></table></figure></p><p>更多设置，可以参考博客<a href="https://www.jianshu.com/p/1f8107a8778c" target="_blank" rel="noopener">hexo搭建个人博客–NexT主题优化</a></p><h4 id="数学公式"><a href="#数学公式" class="headerlink" title="数学公式"></a>数学公式</h4><p>修改主题配置文件：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># MathJax Support</span><br><span class="line">mathjax:</span><br><span class="line">  enable: true</span><br><span class="line">  per_page: true</span><br><span class="line">  cdn: //cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML</span><br></pre></td></tr></table></figure></p><p>在写博客时，如果博文带有公式，头部增加一项mathjax，如：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">---</span><br><span class="line">title: math</span><br><span class="line">date: 2018-08-04 23:12:07</span><br><span class="line">tags:</span><br><span class="line">mathjax: true</span><br><span class="line">---</span><br></pre></td></tr></table></figure></p><h3 id="部署到github"><a href="#部署到github" class="headerlink" title="部署到github"></a>部署到github</h3><p>登录github，创建一个repo，名称为 “yourname.github.io”, 其中yourname是你的github名称。<br>在<strong>站点配置文件</strong>进行更改，如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">deploy:</span><br><span class="line">  type: git</span><br><span class="line">  repo: git@github.com:yourname/yourname.github.io.git</span><br><span class="line">  branch: master</span><br></pre></td></tr></table></figure></p><p>然后安装一个插件<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ npm install hexo-deployer-git --save</span><br></pre></td></tr></table></figure></p><p>执行命令:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ hexo clean</span><br><span class="line">$ hexo g</span><br><span class="line">$ hexo d</span><br></pre></td></tr></table></figure></p><p>或<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ hexo clean</span><br><span class="line">$ hexo d -g #生成并上传</span><br></pre></td></tr></table></figure></p><p>即可将你写好的文章部署到github服务器上，打开浏览器，输入<a href="http://yourgithubname.github.io" target="_blank" rel="noopener">http://yourgithubname.github.io</a> 检测是否成功。</p><p>如果出现如下类似的错误：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">error: failed to execute prompt script (exit code 1)</span><br><span class="line">fatal: could not read Username for &apos;https://github.com&apos;: Invalid argument</span><br><span class="line"></span><br><span class="line">    at ChildProcess.&lt;anonymous&gt; (H:\Hexo_Blog\node_modules\hexo-util\lib\spawn.js:37:17)</span><br><span class="line">    at emitTwo (events.js:126:13)</span><br><span class="line">    at ChildProcess.emit (events.js:214:7)</span><br><span class="line">    at ChildProcess.cp.emit (H:\Hexo_Blog\node_modules\cross-spawn\lib\enoent.js:40:29)</span><br><span class="line">    at maybeClose (internal/child_process.js:925:16)</span><br><span class="line">    at Process.ChildProcess._handle.onexit (internal/child_process.js:209:5)</span><br></pre></td></tr></table></figure></p><p>可以尝试重新<a href="https://www.liaoxuefeng.com/wiki/0013739516305929606dd18361248578c67b8067c8c017b000/00137396287703354d8c6c01c904c7d9ff056ae23da865a000" target="_blank" rel="noopener">配置github账户信息</a>和<a href="https://www.liaoxuefeng.com/wiki/0013739516305929606dd18361248578c67b8067c8c017b000/001374385852170d9c7adf13c30429b9660d0eb689dd43a000" target="_blank" rel="noopener">配置SSH</a>。</p><h3 id="写博客"><a href="#写博客" class="headerlink" title="写博客"></a>写博客</h3><p>hexo根目录，执行命令：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new &apos;first blog&apos;</span><br></pre></td></tr></table></figure></p><p>hexo会在\source_posts下生成相关md文件，打开便可开始写博客了。博客格式如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">---</span><br><span class="line">title: MongoDB学习笔记</span><br><span class="line">date: 2018-01-17 19:40:37</span><br><span class="line">categories:</span><br><span class="line">- Database</span><br><span class="line">tags:</span><br><span class="line">- MongoDB</span><br><span class="line">- NoSQL</span><br><span class="line">---</span><br><span class="line">正文blabla</span><br></pre></td></tr></table></figure></p><h3 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h3><p>卸载hexo<br><code>$ npm uninstall hexo-cli -g</code></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h3&gt;&lt;p&gt;好记性是真的不如烂笔头！另外，自己也深刻觉得做事情多做10%十分重要，比如阅解决一个什么问题，不能懒，及时记录下来，下次遇到便能省不少事儿，
      
    
    </summary>
    
      <category term="Hexo" scheme="http://yoursite.com/categories/Hexo/"/>
    
    
      <category term="Hexo" scheme="http://yoursite.com/tags/Hexo/"/>
    
  </entry>
  
  <entry>
    <title>测试公式</title>
    <link href="http://yoursite.com/2018/08/04/%E6%B5%8B%E8%AF%95%E5%85%AC%E5%BC%8F/"/>
    <id>http://yoursite.com/2018/08/04/测试公式/</id>
    <published>2018-08-04T15:12:07.000Z</published>
    <updated>2018-09-28T02:50:55.576Z</updated>
    
    <content type="html"><![CDATA[<p>$f(x_1) = sin(x_1)^2$</p><p>$$f(x)=<br>\begin{cases}<br>0&amp; \text{x=0}\\<br>1&amp; \text{x!=0}<br>\end{cases}$$</p><p>$$<br>\sum_{i=0}^N\int_{a}^{b}g(t,i)\text{d}t<br>$$</p><p>$$\begin{bmatrix}<br>{a_{11}}&amp;{a_{12}}&amp;{\cdots}&amp;{a_{1n}}\\<br>{a_{21}}&amp;{a_{22}}&amp;{\cdots}&amp;{a_{2n}}\\<br>{\vdots}&amp;{\vdots}&amp;{\ddots}&amp;{\vdots}\\<br>{a_{m1}}&amp;{a_{m2}}&amp;{\cdots}&amp;{a_{mn}}\\<br>\end{bmatrix}$$</p><p>$$\begin{cases}<br>a_1x+b_1y+c_1z=d_1\\<br>a_2x+b_2y+c_2z=d_2\\<br>a_3x+b_3y+c_3z=d_3\\<br>\end{cases}<br>$$</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;$f(x_1) = sin(x_1)^2$&lt;/p&gt;
&lt;p&gt;$$f(x)=&lt;br&gt;\begin{cases}&lt;br&gt;0&amp;amp; \text{x=0}\\&lt;br&gt;1&amp;amp; \text{x!=0}&lt;br&gt;\end{cases}$$&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;\sum_{i=
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>MongoDB学习笔记</title>
    <link href="http://yoursite.com/2018/01/17/2018-01-17-MongoDB%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    <id>http://yoursite.com/2018/01/17/2018-01-17-MongoDB学习笔记/</id>
    <published>2018-01-17T11:40:37.000Z</published>
    <updated>2018-09-28T02:50:55.576Z</updated>
    
    <content type="html"><![CDATA[<h3 id="MongoDB"><a href="#MongoDB" class="headerlink" title="MongoDB"></a>MongoDB</h3><h3 id="Windows上安装方法："><a href="#Windows上安装方法：" class="headerlink" title="Windows上安装方法："></a>Windows上安装方法：</h3><ol><li>下载，安装，可以从customer自定义安装路径后，例如我设置的安装路径为”F:\MongoDB\”，一直Next直到安装结束。</li><li>配置<ul><li>将安装路径下的bin目录添加到环境变量PATH中，例如我的是”F:\MongoDB\bin”；</li><li>配置MongoDB的存储路径，例如我配置的是”F:\MongoDB\data\db”，就是在”F:\MongoDB\”这个文件夹下新建”data\db”这种目录结构；</li><li>将MongoDB注册为服务，以方便日后的使用：<ul><li>打开cmd命令行，输入（将logpath 和dbpath 改成自己的）：<br><code>mongod --logpath &quot;F:\MongoDB\logs.log&quot; --dbpath &quot;F:\MongoDB\data\db&quot; --install</code><ul><li>注册完成，便可通过输入：<br><code>net start mongodb</code><br>便可启动服务。</li></ul></li></ul></li><li>在命令行输入<code>mongo</code>之后便可以打开shell对MongoDB进行操作啦~</li></ul></li></ol><h3 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h3><table><thead><tr><th>SQL概念</th><th>MongoDB概念</th><th>说明</th></tr></thead><tbody><tr><td>database</td><td>database</td><td>数据库</td></tr><tr><td>table</td><td>collection</td><td>数据库表/集合</td></tr><tr><td>row</td><td>document</td><td>数据行/文档</td></tr><tr><td>column</td><td>field</td><td>数据字段列/域</td></tr></tbody></table><p>文档：文档是有序的，大小写敏感的。键不能包含”\0”，这个字符用于标记键的结尾；”.”、”$”、”_”保留，建议不要使用在键中。文档的数据结构和JSON基本一样，所有存在集合中的数据都是BSON格式。</p><h3 id="基本操作"><a href="#基本操作" class="headerlink" title="基本操作"></a>基本操作</h3><ul><li>创建/删除数据库<br><code>show dbs</code>查看所有数据库<br><code>use DATABASE_NAME</code>创建数据库，若不存在则新建，否则进入此数据库<br><code>db.dropDatabase()</code>删除当前数据库</li><li>集合<br><code>show collections</code>查看所有集合<br><code>db.createCollection(name, options)</code> 创建集合。options参数是可选的，如autoIndexId 参数，如果为true，则在_id字段上自动创建索引，默认值为false。<br>例子：<code>db.createCollection(&quot;mytest&quot;, { autoIndexId : true})</code><br><code>db.COLLECTION_NAME.drop()</code> 删除集合</li><li><p>插入文档<br>insert()或save()方法<br><code>db.COLLECTION_NAME.insert(document)</code><br>例如：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"> db.mytest.insert(&#123;</span><br><span class="line">name: &apos;Bob&apos;,</span><br><span class="line">score: 100</span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure></li><li><p>查询文档<br><code>db.COLLECTION_NAME.find()</code> 相当于SQL中的select * from tabe<br><code>db.COLLECTION_NAME.find().pretty()</code>以易读方式展现<br>条件语句和操作符：</p></li></ul><table><thead><tr><th>操作</th><th>格式</th><th>示例</th><th>说明</th></tr></thead><tbody><tr><td>等于</td><td><code>{&lt;key&gt;:&lt;value&gt;}</code></td><td><code>db.mytest.find({&quot;score&quot;:100})</code></td><td>从mytest集合中找到score等于100的文档</td></tr><tr><td>小于</td><td><code>{&lt;key&gt;:{$lt:&lt;value&gt;}}</code></td><td><code>db.mytest.find({&quot;score&quot;:{$lt:100}})</code></td><td>从mytest集合中找到score小于100的文档</td></tr><tr><td>小于或等于</td><td><code>{&lt;key&gt;:{$lte:&lt;value&gt;}}</code></td><td><code>db.mytest.find({&quot;score&quot;:{$lte:100}})</code></td><td>从mytest集合中找到score小于或等于100的文档</td></tr><tr><td>大于</td><td><code>{&lt;key&gt;:{$gt:&lt;value&gt;}}</code></td><td><code>db.mytest.find({&quot;score&quot;:{$gt:100}})</code></td><td>从mytest集合中找到score大于100的文档</td></tr><tr><td>大于或等于</td><td><code>{&lt;key&gt;:{$gte:&lt;value&gt;}}</code></td><td><code>db.mytest.find({&quot;score&quot;:{$gte:100}})</code></td><td>从mytest集合中找到score大于或等于100的文档</td></tr><tr><td>不等于</td><td><code>{&lt;key&gt;:{$ne:&lt;value&gt;}}</code></td><td><code>db.mytest.find({&quot;score&quot;:{$ne:100}})</code></td><td>从mytest集合中找到score不等于100的文档</td></tr></tbody></table><p> 条件组合：<br>  AND：以逗号隔开<br><code>db.mytest.find({&quot;score&quot;:{$gte:100}, &quot;name&quot;:&quot;Bob&quot;})</code><br> OR:使用关键字”$or”<br><code>db.mytest.find({$or:[{&quot;score&quot;:{$gte:100}},{&quot;name&quot;: &quot;Tom&quot;}]}).pretty()</code></p><ul><li><p>更新文档</p><ul><li>update()方法<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"> db.COLLECTION_NAME.update(</span><br><span class="line">   &lt;query&gt;,  update的查询条件</span><br><span class="line">   &lt;update&gt;,  update的更新对象等，类似sql update查询内set后面的</span><br><span class="line">   &#123;</span><br><span class="line">     upsert: &lt;boolean&gt;,  可选，不存在update的记录，是否新插入，默认是false，不插入</span><br><span class="line">     multi: &lt;boolean&gt;, 可选，默认是false,只更新找到的第一条记录</span><br><span class="line">     writeConcern: &lt;document&gt; 可选，抛出异常的级别</span><br><span class="line">   &#125;</span><br><span class="line">)</span><br></pre></td></tr></table></figure></li></ul><p>例子：将名字为Bob的记录更改为Jack，更新多条<br><code>db.mytest.update({&#39;name&#39;:&#39;Bob&#39;},{$set:{&#39;name&#39;:&#39;Jack&#39;}},{multi:true})</code></p><ul><li>save()方法：通过传入的文档来替换已有文档<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">   db.COLLECTION_NAME.save(</span><br><span class="line">   &lt;document&gt;, 文档数据</span><br><span class="line">   &#123;</span><br><span class="line">     writeConcern: &lt;document&gt; 可选，抛出异常的级别</span><br><span class="line">   &#125;</span><br><span class="line">)</span><br></pre></td></tr></table></figure></li></ul></li></ul><p>例子：替换 _id 为 56064f89ade2f21f36b04236 的文档数据<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">db.mytest.save(&#123;</span><br><span class="line">    &quot;_id&quot; : ObjectId(&quot;5a5f103d1fa359a981d5ec90&quot;),</span><br><span class="line">    &quot;name&quot; : &quot;Marry&quot;,</span><br><span class="line">    &quot;score&quot; :  100</span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure></p><ul><li>删除文档<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">db.COLLECTION_NAME.remove(</span><br><span class="line">   &lt;query&gt;, 可选，删除的文档的条件。若无，则删除全部文档</span><br><span class="line">   &#123;</span><br><span class="line">     justOne: &lt;boolean&gt;, 可选，设为 true 或 1，则只删除一个文档</span><br><span class="line">     writeConcern: &lt;document&gt; 可选，抛出异常的级别</span><br><span class="line">   &#125;</span><br><span class="line">)</span><br></pre></td></tr></table></figure></li></ul><p>例子：删除名字为Tom的文档<br><code>db.mytest.remove({&quot;name&quot;:&quot;Marry&quot;})</code></p><p>练习：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">use mydb  #创建数据库</span><br><span class="line">show dbs  #查询所有数据库</span><br><span class="line">db.createCollection(&quot;mytest&quot;, &#123; autoIndexId : true&#125;) #创建集合mytest</span><br><span class="line">show collections  #查询所有集合</span><br><span class="line">#插入数据</span><br><span class="line">db.mytest.insert(&#123;</span><br><span class="line">      name: &apos;Bob&apos;,</span><br><span class="line">      score: 100</span><br><span class="line">      &#125;)</span><br><span class="line">db.mytest.insert(&#123;</span><br><span class="line">      name: &apos;Tom&apos;,</span><br><span class="line">      score: 96</span><br><span class="line">      &#125;)</span><br><span class="line">db.mytest.find(&#123;&quot;score&quot;:100&#125;) #查询mytest中score为100的记录</span><br><span class="line">db.mytest.find(&#123;&quot;score&quot;:&#123;$lt:100&#125;&#125;)</span><br><span class="line">db.mytest.find(&#123;$or:[&#123;&quot;score&quot;:&#123;$gte:100&#125;&#125;,&#123;&quot;name&quot;: &quot;Tom&quot;&#125;]&#125;).pretty()</span><br><span class="line">db.mytest.update(&#123;&apos;name&apos;:&apos;Bob&apos;&#125;,&#123;$set:&#123;&apos;name&apos;:&apos;Jack&apos;&#125;&#125;,&#123;multi:true&#125;)</span><br><span class="line">db.mytest.find() #查询mytest中所有记录</span><br><span class="line">#将_id为5a5f103d1fa359a981d5ec90的数据替换</span><br><span class="line">db.mytest.save(&#123;</span><br><span class="line">    &quot;_id&quot; : ObjectId(&quot;5a5f103d1fa359a981d5ec90&quot;),</span><br><span class="line">    &quot;name&quot; : &quot;Marry&quot;,</span><br><span class="line">    &quot;score&quot; :  100</span><br><span class="line">&#125;)</span><br><span class="line">db.mytest.remove(&#123;&quot;name&quot;:&quot;Marry&quot;&#125;)  #删除mytest中名字为Marry的记录</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;MongoDB&quot;&gt;&lt;a href=&quot;#MongoDB&quot; class=&quot;headerlink&quot; title=&quot;MongoDB&quot;&gt;&lt;/a&gt;MongoDB&lt;/h3&gt;&lt;h3 id=&quot;Windows上安装方法：&quot;&gt;&lt;a href=&quot;#Windows上安装方法：&quot; class
      
    
    </summary>
    
      <category term="Database" scheme="http://yoursite.com/categories/Database/"/>
    
    
      <category term="MongoDB" scheme="http://yoursite.com/tags/MongoDB/"/>
    
      <category term="NoSQL" scheme="http://yoursite.com/tags/NoSQL/"/>
    
  </entry>
  
  <entry>
    <title>对西安市计算机相关职位情况的分析</title>
    <link href="http://yoursite.com/2018/01/14/2018-01-14-%E5%AF%B9%E8%A5%BF%E5%AE%89%E5%B8%82%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%9B%B8%E5%85%B3%E8%81%8C%E4%BD%8D%E6%83%85%E5%86%B5%E7%9A%84%E5%88%86%E6%9E%90/"/>
    <id>http://yoursite.com/2018/01/14/2018-01-14-对西安市计算机相关职位情况的分析/</id>
    <published>2018-01-14T06:55:05.000Z</published>
    <updated>2018-09-28T02:50:55.576Z</updated>
    
    <content type="html"><![CDATA[<h1 id="对西安市计算机相关职位情况的分析"><a href="#对西安市计算机相关职位情况的分析" class="headerlink" title="对西安市计算机相关职位情况的分析"></a>对西安市计算机相关职位情况的分析</h1><h2 id="报告主题"><a href="#报告主题" class="headerlink" title="报告主题"></a>报告主题</h2><p>本报告主要有以下几个目的：</p><ol><li>分析西安市计算机相关职位的需求情况</li><li>分析西安市计算机相关职位的薪酬情况</li><li>分析西安市计算机相关职位的招聘要求</li><li>为广大意向从事计算机行业人群的就业提供借鉴和指导</li></ol><h2 id="数据来源"><a href="#数据来源" class="headerlink" title="数据来源"></a>数据来源</h2><p>本报告对<code>智联招聘</code>网站进行爬虫，主要采集以下几方面的数据,爬虫代码见作业中<code>代码</code>部分。</p><ul><li>Java相关职位</li><li>C++相关职位</li><li>Python相关职位</li><li>C#相关职位</li><li>Ruby相关职位</li><li>PHP相关职位</li><li>.NET相关职位</li><li>Swift相关职位</li><li>Go相关职位</li><li>Scala相关职位</li></ul><h2 id="分析思路"><a href="#分析思路" class="headerlink" title="分析思路"></a>分析思路</h2><p>分析思路主要包含以下几步。</p><ul><li>数据读取</li><li>数据整理</li><li>分析职位数量情况</li><li>分析职位月薪情况</li><li>分析招聘岗位要求</li></ul><h2 id="计算过程"><a href="#计算过程" class="headerlink" title="计算过程"></a>计算过程</h2><p>以下是数据分析和数据挖掘过程</p><h3 id="数据读取"><a href="#数据读取" class="headerlink" title="数据读取"></a>数据读取</h3><p>将通过爬虫爬取的数据通过Python按行读取，去除不符合规则的数据，进而转换为字典，存在列表里面。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">def read_data():</span><br><span class="line">    lists=[]</span><br><span class="line">    try:</span><br><span class="line">        f = open(&apos;r.txt&apos;,&apos;r&apos;,encoding=&apos;utf-8&apos;)</span><br><span class="line">        tmp_list = f.readline()</span><br><span class="line">        lists.append(tmp_list)</span><br><span class="line">        while tmp_list:</span><br><span class="line">            tmp_list = f.readline()</span><br><span class="line">            lists.append(tmp_list)</span><br><span class="line">    except:</span><br><span class="line">        print(&apos;error:&apos;,tmp_list)</span><br><span class="line">    finally:</span><br><span class="line">        f.close()     </span><br><span class="line">    datas = []  </span><br><span class="line">    for item in lists:</span><br><span class="line">        if(not item.startswith(&apos;&#123;\&apos;&apos;)):</span><br><span class="line">            continue       </span><br><span class="line">        dic = eval(item)</span><br><span class="line">        datas.append(dic)</span><br><span class="line">    return datas</span><br></pre></td></tr></table></figure></p><h3 id="数据整理"><a href="#数据整理" class="headerlink" title="数据整理"></a>数据整理</h3><ol><li><p>使用pandas包，将数据组织成DataFrame形式。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df = pd.DataFrame(datas)</span><br></pre></td></tr></table></figure></li><li><p>将日期格式规范化，并对数据进行筛选，对月薪的数据进行筛选，选取格式为“XXXX-XXXX”的信息。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">df = pd.DataFrame(datas)</span><br><span class="line">df[&apos;save_date&apos;] = pd.to_datetime(df[&apos;save_date&apos;])</span><br><span class="line">df_clean = df[[&apos;position_name&apos;,</span><br><span class="line">       &apos;feedback&apos;,</span><br><span class="line">       &apos;company_name&apos;,</span><br><span class="line">       &apos;salary&apos;,</span><br><span class="line">       &apos;place&apos;,</span><br><span class="line">       &apos;time&apos;,</span><br><span class="line">       &apos;brief&apos;,</span><br><span class="line">       &apos;link&apos;,</span><br><span class="line">       &apos;save_date&apos;,</span><br><span class="line">       &apos;keyword&apos;]]</span><br><span class="line">df_clean = df_clean[df_clean[&apos;salary&apos;].str.contains(&apos;\d+-\d+&apos;, regex=True)]</span><br></pre></td></tr></table></figure></li><li><p>根据链接作为标志判断爬取的数据是否有重复值，并进行去重工作。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df_clean_concat = df_clean_concat.drop_duplicates([&apos;link&apos;])</span><br></pre></td></tr></table></figure></li></ol><h3 id="分析职位数量情况"><a href="#分析职位数量情况" class="headerlink" title="分析职位数量情况"></a>分析职位数量情况</h3><p>对西安市计算机行业不同职位数量的分布情况进行分析。</p><ol><li><p>首先以不同职位作为关键词进行分析，并按职位数量进行排序。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">df_key_main = df_city[df_city[&apos;keyword&apos;].isin(KEYWORDS)]</span><br><span class="line">df_key_main_count = df_key_main.groupby(&apos;keyword&apos;)[&apos;position_name&apos;,&apos;company_name&apos;].count()</span><br><span class="line">df_key_main_count[&apos;company_name&apos;] = df_key_main_count[&apos;company_name&apos;]/(df_key_main_count[&apos;company_name&apos;].sum())</span><br><span class="line">df_key_main_count.columns = [&apos;number&apos;, &apos;percentage&apos;]</span><br><span class="line">df_key_main_count.sort_values(by=&apos;number&apos;, ascending=False, inplace=True)</span><br></pre></td></tr></table></figure></li><li><p>对西安市计算机行业不同职位数量的情况进行分析，以饼图的形式呈现。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">label = df_key_main_count[&apos;label&apos;]</span><br><span class="line">sizes = df_key_main_count[&apos;number&apos;]</span><br><span class="line"># 设置绘图区域大小</span><br><span class="line">fig, axes = plt.subplots(figsize=(10,6),ncols=2)</span><br><span class="line">ax1, ax2 = axes.ravel()</span><br><span class="line">colors  = [&quot;blue&quot;,&quot;red&quot;,&quot;coral&quot;,&quot;green&quot;,&quot;yellow&quot;,&quot;orange&quot;] </span><br><span class="line">patches, texts = ax1.pie(sizes,labels=None, shadow=False, startangle=0, colors=colors)</span><br><span class="line">ax1.axis(&apos;equal&apos;)  </span><br><span class="line">ax1.set_title(&apos;职位数量分布&apos;, loc=&apos;center&apos;)</span><br><span class="line"># ax2 只显示图例（legend）</span><br><span class="line">ax2.axis(&apos;off&apos;)</span><br><span class="line">ax2.legend(patches, label, loc=&apos;center left&apos;, fontsize=9)</span><br><span class="line">plt.savefig(&apos;job_pie.jpg&apos;)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></li></ol><p>所得饼图如下所示：<br><img src="/2018/01/14/2018-01-14-对西安市计算机相关职位情况的分析/images/job_pie.jpg" alt="Alt text"></p><p>观察图表，可以看到，传统的Java和C++开发职位占据所有职位数量的半壁江山，此外，.NET，PHP，Python也占据了一部分，而像Scala，Swift等新出现的语言相关的职位则几乎没有。所以，像Java，C++在西安的计算机相关职位招聘中还是属于主流。</p><h3 id="分析职位月薪情况"><a href="#分析职位月薪情况" class="headerlink" title="分析职位月薪情况"></a>分析职位月薪情况</h3><p>对西安市的职位月薪情况进行分析。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">df_salary_adjust = df_clean_concat[df_clean_concat[&apos;salary_min&apos;]&lt;=20000]</span><br><span class="line"> fig, (ax1, ax2) = plt.subplots(figsize=(10,8), nrows=2)</span><br><span class="line"> x_pos = list(range(df_salary_adjust.shape[0]))</span><br><span class="line"> y1 = df_salary_adjust[&apos;salary_min&apos;]</span><br><span class="line"> ax1.plot(x_pos, y1)</span><br><span class="line"> ax1.set_title(&apos;西安计算机相关职位月薪趋势图&apos;, size=14)</span><br><span class="line"> ax1.set_xticklabels(&apos;&apos;)</span><br><span class="line"> ax1.set_ylabel(&apos;最低月薪(RMB)&apos;)</span><br><span class="line"> bins = [3000,6000, 9000, 12000, 15000, 18000, 21000]</span><br><span class="line"> counts, bins, patches = ax2.hist(y1, bins, normed=1, histtype=&apos;bar&apos;, facecolor=&apos;g&apos;, rwidth=0.8)</span><br><span class="line"> ax2.set_title(&apos;西安计算机相关职位月薪直方图&apos;, size=14)</span><br><span class="line"> ax2.set_yticklabels(&apos;&apos;)</span><br><span class="line"> ax2.set_xticks(bins) </span><br><span class="line"> ax2.set_xticklabels(bins, rotation=-90)</span><br><span class="line"> # Label the raw counts and the percentages below the x-axis...</span><br><span class="line"> bin_centers = 0.5 * np.diff(bins) + bins[:-1]</span><br><span class="line"> for count, x in zip(counts, bin_centers):</span><br><span class="line">     percent = &apos;%0.0f%%&apos; % (100 * float(count) / counts.sum())</span><br><span class="line">     ax2.annotate(percent, xy=(x, 0), xycoords=(&apos;data&apos;, &apos;axes fraction&apos;),</span><br><span class="line">                 xytext=(0, -40), textcoords=&apos;offset points&apos;, va=&apos;top&apos;, ha=&apos;center&apos;, rotation=-90, color=&apos;b&apos;, size=14)</span><br><span class="line"> fig.savefig(&apos;salary_inXian.jpg&apos;)</span><br><span class="line"> fig.show()</span><br></pre></td></tr></table></figure></p><p>所得结果如下所示：<br><img src="/2018/01/14/2018-01-14-对西安市计算机相关职位情况的分析/images/salary_inXian.jpg" alt="Alt text"></p><p>观察图表，可以看到，西安市计算机相关职位月薪在3000~12000人民币占据了绝大部分，也有一小部分月薪达到了12000以上，但几乎没有超过20000的。这个月薪水平在西安可以基本达到小康水平。</p><h3 id="分析招聘岗位要求"><a href="#分析招聘岗位要求" class="headerlink" title="分析招聘岗位要求"></a>分析招聘岗位要求</h3><p>对西安市计算机相关职位招聘岗位要求描述进行词云图分析<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">brief_list = list(df_clean_concat[&apos;brief&apos;])</span><br><span class="line">brief_str = &apos;&apos;.join(brief_list)</span><br><span class="line"># 首先使用 jieba 中文分词工具进行分词</span><br><span class="line">wordlist = jieba.cut(brief_str, cut_all=False)      </span><br><span class="line"># cut_all, True为全模式，False为精确模式</span><br><span class="line">wordlist_space_split = &apos; &apos;.join(wordlist)</span><br><span class="line"></span><br><span class="line">my_wordcloud = WordCloud( max_words=100, font_path=&quot;simhei.ttf&quot;,background_color=&quot;white&quot;,</span><br><span class="line">                     max_font_size=300, random_state=42).generate(wordlist_space_split)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">plt.imshow(my_wordcloud)            # 以图片的形式显示词云</span><br><span class="line">plt.axis(&apos;off&apos;)                     # 关闭坐标轴</span><br><span class="line">plt.show()</span><br><span class="line">my_wordcloud.to_file(os.path.join( &apos;brief_cloud.png&apos;))</span><br></pre></td></tr></table></figure></p><p>所得结果如下所示：</p><p><img src="/2018/01/14/2018-01-14-对西安市计算机相关职位情况的分析/images/brief_cloud.png" alt="Alt text"></p><p>观察图表，可以看到企业在进行招聘时，看重具有相关的开发经验，比较青睐相关专业的人员，学历也占据了相对来说比较重要的地位，另外，也希望应聘人员具有团队合作的能力，具有较好的学习能力。</p><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>本分析报告结论如下：</p><ul><li>Java，C++等开发岗位在西安的计算机相关职位招聘中属于主流，需求较多。</li><li>西安市计算机相关职位月薪大多在3000~12000人民币，在西安可以基本达到小康水平。</li><li>企业希望应聘人员具有相关的开发经验，比较青睐相关专业的人员，学历也占据了相对来说比较重要的地位。</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;对西安市计算机相关职位情况的分析&quot;&gt;&lt;a href=&quot;#对西安市计算机相关职位情况的分析&quot; class=&quot;headerlink&quot; title=&quot;对西安市计算机相关职位情况的分析&quot;&gt;&lt;/a&gt;对西安市计算机相关职位情况的分析&lt;/h1&gt;&lt;h2 id=&quot;报告主题&quot;&gt;&lt;a 
      
    
    </summary>
    
      <category term="Data mining" scheme="http://yoursite.com/categories/Data-mining/"/>
    
    
      <category term="python" scheme="http://yoursite.com/tags/python/"/>
    
      <category term="data" scheme="http://yoursite.com/tags/data/"/>
    
  </entry>
  
</feed>
