---
title: 决策树
date: 2018-10-30 16:28:27
categories:
- 统计机器学习
tags:
- 决策树
- 统计学习
mathjax: true
---

### 理论
#### 什么是决策树？
决策树内部节点表示一个特征或属性，叶节点表示一个类，进行分类时，从根节点开始，对实例的某一特征进行测试，根据测试结果，将实例分配到其子节点，这时每一个子节点对应着该特征的一个取值，如此递归地对实例进行测试并分配，直至到达叶节点，最后将实例分到叶节点地类中。
可以将决策树看成一个if-then规则的集合，每一个实例都被一条路径或一条规则所覆盖，而且只被一条路径或一条规则所覆盖，互斥并且完备。同时，决策树还表示给定特征条件下类的条件概率分布，定义了特征空间的一个划分，分为互不相交的几个单元，决策树的一条路径就对应划分中的一个单元，决策树分类时将实例分到条件概率大的一类去。

#### 决策树学习
决策树学习通常包括3个步骤：特征选择、决策树的生成和决策树的剪枝。决策树学习的算法通常是一个递归地选择最优特征，并根据该特征对训练数据进行分割，使得对各个子数据集有一个最好的分类的过程，这一过程对应着特征空间的划分，也对应着决策树的构建。为了防止过拟合，我们需要对生成的决策树自下而上进行剪枝，去掉过于细分的叶节点，使其回退到父节点，甚至更高的节点，然后将父节点或更高的节点改为新的叶节点。

##### 前提知识
设 $X$ 是一个取有限值的离散随机变量，其概率分布为
$$ P(X=x_i) = p_i,  i=1,2,\cdots,n $$
随机变量$(X,Y)$，其联合概率分布为
$$ P(X=x_i,Y=y_j) = p_{ij},  i=1,2,\cdots,n; j=1,2,\cdots,m; $$

则随机变量 $X$ 的熵定义为
$$ H(X) = -\sum_{i=1}^{n} p_i log(p_i)$$ 

条件熵$H(Y|X)$表示在已知$X$的条件下随机变量$Y$的不确定性，定义为在$X$给定的条件下，$Y$的条件概率分布的熵对$X$的数学期望
$$ H(Y|X) = \sum_{i=1}^{n} p_iH(Y|X=x_i) $$
这里，$p_i =P(X=x_i)$
当熵和条件熵的概率由数据估计得到时，称为经验熵和经验条件熵，如果有$0$概率，则令 $0log0=0$ 

##### 特征选择
特征选择在于选取对训练数据具有分类能力的特征，准则是信息增益或信息增益比。

信息增益表示得知特征$X$的信息而使得类$Y$的信息的不确定性减少的程度，将熵$H(Y)$与条件熵$H(Y|X)$之差称为互信息，决策树学习中的信息增益等价于训练数据集中类与特征的互信息。

给定训练数据集$D$和特征$A$，经验熵$H(D)$表示对数据集$D$进行分类的不确定性，经验条件熵$H(D|A)$表示在特征$A$给定的条件下对数据集$D$进行分类的不确定性，它们的差，即信息增益，表示由于特征$A$而使得对数据集$D$的分类的不确定性减少的程度。在进行特征选择的时候。我们选择信息增益最大的特征，因为信息增益大的特征具有更强的分类能力。
信息增益定义为
$$ g(D,A) = H(D) - H(D|A) $$


